<?xml version="1.0" encoding="iso-8859-1"?>
<!--
     The FreeBSD Documentation Project

     $FreeBSD$
-->
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="virtualization">

  <info>
    <title>Virtualization</title>

    <authorgroup>
      <author>
	<personname>
	  <firstname>Murray</firstname>
	  <surname>Stokely</surname>
	</personname>
	<contrib>Contributed by </contrib>
      </author>
    </authorgroup>

    <authorgroup>
      <author>
	<personname>
	  <firstname>Allan</firstname>
	  <surname>Jude</surname>
	</personname>
	<contrib>bhyve section by </contrib>
      </author>
    </authorgroup>

    <authorgroup>
      <author>
	<personname>
	  <firstname>Benedict</firstname>
	  <surname>Reuschling</surname>
	</personname>
	<contrib>Xen section by </contrib>
      </author>
    </authorgroup>
  </info>

  <sect1 xml:id="virtualization-synopsis">
    <title>Synopsis</title>

    <para>Virtualization software allows multiple operating systems to
      run simultaneously on the same computer.  Such software systems
      for <acronym>PC</acronym>s often involve a host operating system
      which runs the virtualization software and supports any number
      of guest operating systems.</para>

    <para>After reading this chapter, you will know:</para>

    <itemizedlist>
      <listitem>
	<para>The difference between a host operating system and a
	  guest operating system.</para>
      </listitem>

      <listitem>
	<para>How to install &os; on an &intel;-based &apple;
	  &mac; computer.</para>
      </listitem>

      <listitem>
	<para>How to install &os; on &microsoft.windows; with
	  <application>Virtual PC</application>.</para>
      </listitem>

      <listitem>
	<para>How to install &os; as a guest in
	  <application>bhyve</application>.</para>
      </listitem>

      <listitem>
	<para>How to tune a &os; system for best performance under
	  virtualization.</para>
      </listitem>

    </itemizedlist>

    <para>Before reading this chapter, you should:</para>

    <itemizedlist>
      <listitem>
	<para>Understand the <link linkend="basics">basics of &unix;
	    and &os;</link>.</para>
      </listitem>

      <listitem>
	<para>Know how to <link linkend="bsdinstall">install
	    &os;</link>.</para>
      </listitem>

      <listitem>
	<para>Know how to <link linkend="advanced-networking">set up a
	    network connection</link>.</para>
      </listitem>

      <listitem>
	<para>Know how to <link linkend="ports">install additional
	    third-party software</link>.</para>
      </listitem>
    </itemizedlist>
  </sect1>

  <sect1 xml:id="virtualization-guest-parallels">
    <title>&os; as a Guest on <application>Parallels</application> for
      &macos; X</title>

    <para><application>Parallels Desktop</application> for &mac; is
      a commercial software product available for &intel; based
      &apple; &mac; computers running &macos; 10.4.6 or higher.  &os;
      is a fully supported guest operating system.  Once
      <application>Parallels</application> has been installed on
      &macos; X, the user must configure a virtual machine and then
      install the desired guest operating system.</para>

    <sect2 xml:id="virtualization-guest-parallels-install">
      <title>Installing &os; on Parallels/&macos; X</title>

      <para>The first step in installing &os; on
	<application>Parallels</application> is to create a new
	virtual machine for installing &os;.  Select
	<guimenuitem>&os;</guimenuitem> as the
	<guimenu>Guest OS Type</guimenu> when prompted:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd1"/>
	</imageobject>
      </mediaobject>

      <para>Choose a reasonable amount of disk and memory
	depending on the plans for this virtual &os; instance.
	4GB of disk space and 512MB of RAM work well for most uses
	of &os; under <application>Parallels</application>:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd2"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd3"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd4"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd5"/>
	</imageobject>
      </mediaobject>

      <para>Select the type of networking and a network
	interface:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd6"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd7"/>
	</imageobject>
      </mediaobject>

      <para>Save and finish the configuration:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd8"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd9"/>
	</imageobject>
      </mediaobject>

      <para>After the &os; virtual machine has been created, &os;
	can be installed on it.  This is best done with an official
	&os; <acronym>CD</acronym>/<acronym>DVD</acronym> or with an
	<acronym>ISO</acronym> image downloaded from an official
	<acronym>FTP</acronym> site.  Copy the appropriate
	<acronym>ISO</acronym> image to the local &mac; filesystem or
	insert a <acronym>CD</acronym>/<acronym>DVD</acronym> in the
	&mac;'s <acronym>CD-ROM</acronym> drive.  Click on the disc
	icon in the bottom right corner of the &os;
	<application>Parallels</application> window.  This will bring
	up a window that can be used to associate the
	<acronym>CD-ROM</acronym> drive in the virtual machine with
	the <acronym>ISO</acronym> file on disk or with the real
	<acronym>CD-ROM</acronym> drive.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd11"/>
	</imageobject>
      </mediaobject>

      <para>Once this association with the <acronym>CD-ROM</acronym>
	source has been made, reboot the &os; virtual machine by
	clicking the reboot icon.
	<application>Parallels</application> will reboot with a
	special <acronym>BIOS</acronym> that first checks if there is
	a <acronym>CD-ROM</acronym>.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd10"/>
	</imageobject>
      </mediaobject>

      <para>In this case it will find the &os; installation media and
	begin a normal &os; installation.  Perform the installation,
	but do not attempt to configure
	<application>&xorg;</application> at this time.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd12"/>
	</imageobject>
      </mediaobject>

      <para>When the installation is finished, reboot into the newly
	installed &os; virtual machine.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/parallels-freebsd13"/>
	</imageobject>
      </mediaobject>
    </sect2>

    <sect2 xml:id="virtualization-guest-parallels-configure">
      <title>Configuring &os; on
	<application>Parallels</application></title>

      <para>After &os; has been successfully installed on &macos; X
	with <application>Parallels</application>, there are a number
	of configuration steps that can be taken to optimize the
	system for virtualized operation.</para>

      <procedure>
	<step>
	  <title>Set Boot Loader Variables</title>

	  <para>The most important step is to reduce the
	    <option>kern.hz</option> tunable to reduce the CPU
	    utilization of &os; under the
	    <application>Parallels</application> environment.  This is
	    accomplished by adding the following line to
	    <filename>/boot/loader.conf</filename>:</para>

	  <programlisting>kern.hz=100</programlisting>

	  <para>Without this setting, an idle &os;
	    <application>Parallels</application> guest will use
	    roughly 15% of the CPU of a single processor &imac;.
	    After this change the usage will be closer to 5%.</para>
	</step>

	<step>
	  <title>Create a New Kernel Configuration File</title>

	  <para>All of the SCSI, FireWire, and USB device drivers
	    can be removed from a custom kernel configuration file.
	    <application>Parallels</application> provides a virtual
	    network adapter used by the &man.ed.4; driver, so all
	    network devices except for &man.ed.4; and &man.miibus.4;
	    can be removed from the kernel.</para>
	</step>

	<step>
	  <title>Configure Networking</title>

	  <para>The most basic networking setup uses DHCP to connect
	    the virtual machine to the same local area network as the
	    host &mac;.  This can be accomplished by adding
	    <literal>ifconfig_ed0="DHCP"</literal> to
	    <filename>/etc/rc.conf</filename>.  More advanced
	    networking setups are described in
	    <xref linkend="advanced-networking"/>.</para>
	</step>
      </procedure>
    </sect2>
  </sect1>

  <sect1 xml:id="virtualization-guest-virtualpc">
    <title>&os; as a Guest on <application>Virtual PC</application>
      for &windows;</title>

    <para><application>Virtual PC</application> for &windows; is a
      &microsoft; software product available for free download.  See
      this website for the <link
	xlink:href="http://www.microsoft.com/windows/downloads/virtualpc/sysreq.mspx">system
	requirements</link>.  Once
      <application>Virtual PC</application> has been installed on
      &microsoft.windows;, the user can configure a virtual machine
      and then install the desired guest operating system.</para>

    <sect2 xml:id="virtualization-guest-virtualpc-install">
      <title>Installing &os; on
	<application>Virtual PC</application></title>

      <para>The first step in installing &os; on
	<application>Virtual PC</application> is to create a new
	virtual machine for installing &os;.  Select
	<guimenuitem>Create a virtual machine</guimenuitem> when
	prompted:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd1"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd2"/>
	</imageobject>
      </mediaobject>

      <para>Select <guimenuitem>Other</guimenuitem> as the
	<guimenuitem>Operating system</guimenuitem> when
	prompted:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd3"/>
	</imageobject>
      </mediaobject>

      <para>Then, choose a reasonable amount of disk and memory
	depending on the plans for this virtual &os; instance.
	4GB of disk space and 512MB of RAM work well for most uses
	of &os; under <application>Virtual PC</application>:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd4"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd5"/>
	</imageobject>
      </mediaobject>

      <para>Save and finish the configuration:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd6"/>
	</imageobject>
      </mediaobject>

      <para>Select the &os; virtual machine and click
	<guimenu>Settings</guimenu>, then set the type of networking
	and a network interface:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd7"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd8"/>
	</imageobject>
      </mediaobject>

      <para>After the &os; virtual machine has been created, &os; can
	be installed on it.  This is best done with an official &os;
	<acronym>CD</acronym>/<acronym>DVD</acronym> or with an
	<acronym>ISO</acronym> image downloaded from an official
	<acronym>FTP</acronym> site.  Copy the appropriate
	<acronym>ISO</acronym> image to the local &windows; filesystem
	or insert a <acronym>CD</acronym>/<acronym>DVD</acronym> in
	the <acronym>CD</acronym> drive, then double click on the &os;
	virtual machine to boot.  Then, click <guimenu>CD</guimenu>
	and choose <guimenu>Capture ISO Image...</guimenu> on the
	<application>Virtual PC</application> window.  This will bring
	up a window where the <acronym>CD-ROM</acronym> drive in the
	virtual machine can be associated with an
	<acronym>ISO</acronym> file on disk or with the real
	<acronym>CD-ROM</acronym> drive.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd9"/>
	</imageobject>
      </mediaobject>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd10"/>
	</imageobject>
      </mediaobject>

      <para>Once this association with the <acronym>CD-ROM</acronym>
	source has been made, reboot the &os; virtual machine by
	clicking <guimenu>Action</guimenu> and
	<guimenu>Reset</guimenu>.
	<application>Virtual PC</application> will reboot with a
	special <acronym>BIOS</acronym> that first checks for a
	<acronym>CD-ROM</acronym>.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd11"/>
	</imageobject>
      </mediaobject>

      <para>In this case it will find the &os; installation media
	and begin a normal &os; installation.  Continue with the
	installation, but do not attempt to configure
	<application>&xorg;</application> at this time.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd12"/>
	</imageobject>
      </mediaobject>

      <para>When the installation is finished, remember to eject the
	<acronym>CD</acronym>/<acronym>DVD</acronym> or release the
	<acronym>ISO</acronym> image.  Finally, reboot into the newly
	installed &os; virtual machine.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/virtualpc-freebsd13"/>
	</imageobject>
      </mediaobject>
    </sect2>

    <sect2 xml:id="virtualization-guest-virtualpc-configure">
      <title>Configuring &os; on <application>Virtual
	  PC</application></title>

      <para>After &os; has been successfully installed on
	&microsoft.windows; with
	<application>Virtual PC</application>, there are a number of
	configuration steps that can be taken to optimize the system
	for virtualized operation.</para>

      <procedure>
	<step>
	  <title>Set Boot Loader Variables</title>

	  <para>The most important step is to reduce the
	    <option>kern.hz</option> tunable to reduce the CPU
	    utilization of &os; under the
	    <application>Virtual PC</application> environment.  This
	    is accomplished by adding the following line to
	    <filename>/boot/loader.conf</filename>:</para>

	  <programlisting>kern.hz=100</programlisting>

	  <para>Without this setting, an idle &os;
	    <application>Virtual PC</application> guest OS will
	    use roughly 40% of the CPU of a single processor
	    computer.  After this change, the usage will be
	    closer to 3%.</para>
	</step>

	<step>
	  <title>Create a New Kernel Configuration File</title>

	  <para>All of the SCSI, FireWire, and USB device drivers can
	    be removed from a custom kernel configuration file.
	    <application>Virtual PC</application> provides a virtual
	    network adapter used by the &man.de.4; driver, so all
	    network devices except for &man.de.4; and &man.miibus.4;
	    can be removed from the kernel.</para>
	</step>

	<step>
	  <title>Configure Networking</title>

	  <para>The most basic networking setup uses DHCP to connect
	    the virtual machine to the same local area network as the
	    &microsoft.windows; host.  This can be accomplished by
	    adding <literal>ifconfig_de0="DHCP"</literal> to
	    <filename>/etc/rc.conf</filename>.  More advanced
	    networking setups are described in
	    <xref linkend="advanced-networking"/>.</para>
	</step>
      </procedure>
    </sect2>
  </sect1>

  <sect1 xml:id="virtualization-guest-vmware">
    <title>&os; as a Guest on <application>VMware Fusion</application>
      for &macos;</title>

    <para><application>VMware Fusion</application> for &mac; is a
      commercial software product available for &intel; based &apple;
      &mac; computers running &macos; 10.4.9 or higher.  &os; is a
      fully supported guest operating system.  Once
      <application>VMware Fusion</application> has been installed on
      &macos; X, the user can configure a virtual machine and then
      install the desired guest operating system.</para>

    <sect2 xml:id="virtualization-guest-vmware-install">
      <title>Installing &os; on
	<application>VMware Fusion</application></title>

      <para>The first step is to start
	<application>VMware Fusion</application> which will load the
	Virtual Machine Library.  Click <guimenuitem>New</guimenuitem>
	to create the virtual machine:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd01"/>
	</imageobject>
      </mediaobject>

      <para>This will load the New Virtual Machine Assistant.  Click
	<guimenuitem>Continue</guimenuitem> to proceed:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd02"/>
	</imageobject>
      </mediaobject>

      <para>Select <guimenuitem>Other</guimenuitem> as the
	<guimenuitem>Operating System</guimenuitem> and either
	<guimenuitem>&os;</guimenuitem> or
	<guimenuitem>&os; 64-bit</guimenuitem>, as the
	<guimenu>Version</guimenu> when prompted:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd03"/>
	</imageobject>
      </mediaobject>

      <para>Choose the name of the virtual machine and the directory
	where it should be saved:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd04"/>
	</imageobject>
      </mediaobject>

      <para>Choose the size of the Virtual Hard Disk for the virtual
	machine:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd05"/>
	</imageobject>
      </mediaobject>

      <para>Choose the method to install the virtual machine, either
	from an <acronym>ISO</acronym> image or from a
	<acronym>CD</acronym>/<acronym>DVD</acronym>:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd06"/>
	</imageobject>
      </mediaobject>

      <para>Click <guimenuitem>Finish</guimenuitem> and the virtual
	machine will boot:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd07"/>
	</imageobject>
      </mediaobject>

      <para>Install &os; as usual:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd08"/>
	</imageobject>
      </mediaobject>

      <para>Once the install is complete, the settings of the virtual
	machine can be modified, such as memory usage:</para>

      <note>
	<para>The System Hardware settings of the virtual machine
	  cannot be modified while the virtual machine is
	  running.</para>
      </note>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd09"/>
	</imageobject>
      </mediaobject>

      <para>The number of CPUs the virtual machine will have access
	to:</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd10"/>
	</imageobject>
      </mediaobject>

      <para>The status of the <acronym>CD-ROM</acronym> device.
	Normally the
	<acronym>CD</acronym>/<acronym>DVD</acronym>/<acronym>ISO</acronym>
	is disconnected from the virtual machine when it is no longer
	needed.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd11"/>
	</imageobject>
      </mediaobject>

      <para>The last thing to change is how the virtual machine will
	connect to the network.  To allow connections to the virtual
	machine from other machines besides the host, choose
	<guimenuitem>Connect directly to the physical network
	  (Bridged)</guimenuitem>.  Otherwise,
	<guimenuitem>Share the host's internet connection
	  (NAT)</guimenuitem> is preferred so that the virtual machine
	can have access to the Internet, but the network cannot access
	the virtual machine.</para>

      <mediaobject>
	<imageobject>
	  <imagedata fileref="virtualization/vmware-freebsd12"/>
	</imageobject>
      </mediaobject>

      <para>After modifying the settings, boot the newly installed
	&os; virtual machine.</para>
    </sect2>

    <sect2 xml:id="virtualization-guest-vmware-configure">
      <title>Configuring &os; on <application>VMware
	  Fusion</application></title>

      <para>After &os; has been successfully installed on &macos; X
	with <application>VMware Fusion</application>, there are a
	number of configuration steps that can be taken to optimize
	the system for virtualized operation.</para>

      <procedure>
	<step>
	  <title>Set Boot Loader Variables</title>

	  <para>The most important step is to reduce the
	    <option>kern.hz</option> tunable to reduce the CPU
	    utilization of &os; under the
	    <application>VMware Fusion</application> environment.
	    This is accomplished by adding the following line to
	    <filename>/boot/loader.conf</filename>:</para>

	  <programlisting>kern.hz=100</programlisting>

	  <para>Without this setting, an idle &os;
	    <application>VMware Fusion</application> guest will use
	    roughly 15% of the CPU of a single processor &imac;.
	    After this change, the usage will be closer to 5%.</para>
	</step>

	<step>
	  <title>Create a New Kernel Configuration File</title>

	  <para>All of the FireWire, and USB device drivers can be
	    removed from a custom kernel configuration file.
	    <application>VMware Fusion</application> provides a
	    virtual network adapter used by the &man.em.4; driver, so
	    all network devices except for &man.em.4; can be removed
	    from the kernel.</para>
	</step>

	<step>
	  <title>Configure Networking</title>

	  <para>The most basic networking setup uses DHCP to connect
	    the virtual machine to the same local area network as the
	    host &mac;.  This can be accomplished by adding
	    <literal>ifconfig_em0="DHCP"</literal> to
	    <filename>/etc/rc.conf</filename>.  More advanced
	    networking setups are described in
	    <xref linkend="advanced-networking"/>.</para>
	</step>
      </procedure>
    </sect2>
  </sect1>

  <sect1 xml:id="virtualization-guest-virtualbox">
    <title>&os; as a Guest on &virtualbox;</title>

    <para>&os; works well as a guest in
      <application>&virtualbox;</application>.  The virtualization
      software is available for most common operating systems,
      including &os; itself.</para>

    <para>The <application>&virtualbox;</application> guest additions
      provide support for:</para>

    <itemizedlist>
      <listitem>
	<para>Clipboard sharing.</para>
      </listitem>

      <listitem>
	<para>Mouse pointer integration.</para>
      </listitem>

      <listitem>
	<para>Host time synchronization.</para>
      </listitem>

      <listitem>
	<para>Window scaling.</para>
      </listitem>

      <listitem>
	<para>Seamless mode.</para>
      </listitem>
    </itemizedlist>

    <note>
      <para>These commands are run in the &os; guest.</para>
    </note>

    <para>First, install the
      <package>emulators/virtualbox-ose-additions</package> package
      or port in the &os; guest.  This will install the port:</para>

    <screen>&prompt.root; <userinput>cd /usr/ports/emulators/virtualbox-ose-additions &amp;&amp; make install clean</userinput></screen>

    <para>Add these lines to <filename>/etc/rc.conf</filename>:</para>

    <programlisting>vboxguest_enable="YES"
vboxservice_enable="YES"</programlisting>

    <para>If &man.ntpd.8; or &man.ntpdate.8; is used, disable host
      time synchronization:</para>

    <programlisting>vboxservice_flags="--disable-timesync"</programlisting>

    <para><application>Xorg</application> will automatically recognize
      the <literal>vboxvideo</literal> driver.  It can also be
      manually entered in
      <filename>/etc/X11/xorg.conf</filename>:</para>

    <programlisting>Section "Device"
	Identifier "Card0"
	Driver "vboxvideo"
	VendorName "InnoTek Systemberatung GmbH"
	BoardName "VirtualBox Graphics Adapter"
EndSection</programlisting>

    <para>To use the <literal>vboxmouse</literal> driver, adjust the
      mouse section in <filename>/etc/X11/xorg.conf</filename>:</para>

    <programlisting>Section "InputDevice"
	Identifier "Mouse0"
	Driver "vboxmouse"
EndSection</programlisting>

    <para><acronym>HAL</acronym> users should create the following
      <filename>/usr/local/etc/hal/fdi/policy/90-vboxguest.fdi</filename>
      or copy it from
      <filename>/usr/local/share/hal/fdi/policy/10osvendor/90-vboxguest.fdi</filename>:</para>

    <programlisting>&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;!--
# Sun VirtualBox
# Hal driver description for the vboxmouse driver
# $Id: chapter.xml,v 1.33 2012-03-17 04:53:52 eadler Exp $

	Copyright (C) 2008-2009 Sun Microsystems, Inc.

	This file is part of VirtualBox Open Source Edition (OSE, as
	available from http://www.virtualbox.org. This file is free software;
	you can redistribute it and/or modify it under the terms of the GNU
	General Public License (GPL) as published by the Free Software
	Foundation, in version 2 as it comes in the "COPYING" file of the
	VirtualBox OSE distribution. VirtualBox OSE is distributed in the
	hope that it will be useful, but WITHOUT ANY WARRANTY of any kind.

	Please contact Sun Microsystems, Inc., 4150 Network Circle, Santa
	Clara, CA 95054 USA or visit http://www.sun.com if you need
	additional information or have any questions.
--&gt;
&lt;deviceinfo version="0.2"&gt;
  &lt;device&gt;
    &lt;match key="info.subsystem" string="pci"&gt;
      &lt;match key="info.product" string="VirtualBox guest Service"&gt;
        &lt;append key="info.capabilities" type="strlist"&gt;input&lt;/append&gt;
	&lt;append key="info.capabilities" type="strlist"&gt;input.mouse&lt;/append&gt;
        &lt;merge key="input.x11_driver" type="string"&gt;vboxmouse&lt;/merge&gt;
	&lt;merge key="input.device" type="string"&gt;/dev/vboxguest&lt;/merge&gt;
      &lt;/match&gt;
    &lt;/match&gt;
  &lt;/device&gt;
&lt;/deviceinfo&gt;</programlisting>

    <para>Shared folders for file transfers between host and VM are
      accessible by mounting them using
      <literal>mount_vboxvfs</literal>.  A shared folder can be created
      on the host using the VirtualBox GUI or via
      <command>vboxmanage</command>.  For example, to create a shared
      folder called <replaceable>myshare</replaceable> under
      <filename><replaceable>/mnt/bsdboxshare</replaceable></filename>
      for the VM named <replaceable>BSDBox</replaceable>, run:</para>

    <screen>&prompt.root; <userinput>vboxmanage sharedfolder add '<replaceable>BSDBox</replaceable>' --name <replaceable>myshare</replaceable> --hostpath <replaceable>/mnt/bsdboxshare</replaceable></userinput></screen>

    <para>Note that the shared folder name must not contain spaces.
      Mount the shared folder from within the guest system like
      this:</para>

    <screen>&prompt.root; <userinput>mount_vboxvfs -w <replaceable>myshare</replaceable> <replaceable>/mnt</replaceable></userinput></screen>
  </sect1>

  <sect1 xml:id="virtualization-host-virtualbox">
    <title>&os; as a Host with &virtualbox;</title>

    <para><application>&virtualbox;</application> is an actively
      developed, complete virtualization package, that is available
      for most operating systems including &windows;, &macos;, &linux;
      and &os;.  It is equally capable of running &windows; or
      &unix;-like guests.  It is released as open source software, but
      with closed-source components available in a separate extension
      pack.  These components include support for USB 2.0 devices.
      More information may be found on the <link
	xlink:href="http://www.virtualbox.org/wiki/Downloads"><quote>Downloads</quote>
	page of the <application>&virtualbox;</application>
	wiki</link>.  Currently, these extensions are not available
      for &os;.</para>

    <sect2 xml:id="virtualization-virtualbox-install">
      <title>Installing &virtualbox;</title>

      <para><application>&virtualbox;</application> is available as a
	&os; package or port in
	<package>emulators/virtualbox-ose</package>.  The port can be
	installed using these commands:</para>

      <screen>&prompt.root; <userinput>cd /usr/ports/emulators/virtualbox-ose</userinput>
&prompt.root; <userinput>make install clean</userinput></screen>

      <para>One useful option in the port's configuration menu is the
	<literal>GuestAdditions</literal> suite of programs.  These
	provide a number of useful features in guest operating
	systems, like mouse pointer integration (allowing the mouse to
	be shared between host and guest without the need to press a
	special keyboard shortcut to switch) and faster video
	rendering, especially in &windows; guests.  The guest
	additions are available in the <guimenu>Devices</guimenu>
	menu, after the installation of the guest is finished.</para>

      <para>A few configuration changes are needed before
	<application>&virtualbox;</application> is started for the
	first time.  The port installs a kernel module in
	<filename>/boot/modules</filename> which
	must be loaded into the running kernel:</para>

      <screen>&prompt.root; <userinput>kldload vboxdrv</userinput></screen>

      <para>To ensure the module is always loaded after a reboot,
	add this line to
	<filename>/boot/loader.conf</filename>:</para>

      <programlisting>vboxdrv_load="YES"</programlisting>

      <para>To use the kernel modules that allow bridged or host-only
	networking, add this line to
	<filename>/etc/rc.conf</filename> and reboot the
	computer:</para>

      <programlisting>vboxnet_enable="YES"</programlisting>

      <para>The <systemitem class="groupname">vboxusers</systemitem>
	group is created during installation of
	<application>&virtualbox;</application>.  All users that need
	access to <application>&virtualbox;</application> will have to
	be added as members of this group.  <command>pw</command> can
	be used to add new members:</para>

      <screen>&prompt.root; <userinput>pw groupmod vboxusers -m <replaceable>yourusername</replaceable></userinput></screen>

      <para>The default permissions for
	<filename>/dev/vboxnetctl</filename> are restrictive and need
	to be changed for bridged networking:</para>

      <screen>&prompt.root; <userinput>chown root:vboxusers /dev/vboxnetctl</userinput>
&prompt.root; <userinput>chmod 0660 /dev/vboxnetctl</userinput></screen>

      <para>To make this permissions change permanent, add these
	lines to <filename>/etc/devfs.conf</filename>:</para>

      <programlisting>own     vboxnetctl root:vboxusers
perm    vboxnetctl 0660</programlisting>

      <para>To launch <application>&virtualbox;</application>,
	type from a <application>&xorg;</application> session:</para>

      <screen>&prompt.user; <userinput>VirtualBox</userinput></screen>

      <para>For more information on configuring and using
	<application>&virtualbox;</application>, refer to the
	<link xlink:href="http://www.virtualbox.org">official
	  website</link>.  For &os;-specific information and
	troubleshooting instructions, refer to the <link
	  xlink:href="http://wiki.FreeBSD.org/VirtualBox">relevant
	  page in the &os; wiki</link>.</para>
    </sect2>

    <sect2 xml:id="virtualization-virtualbox-usb-support">
      <title>&virtualbox; USB Support</title>

      <para><application>&virtualbox;</application> can be configured
	to pass <acronym>USB</acronym> devices through to the guest
	operating system.  The host controller of the OSE version is
	limited to emulating <acronym>USB</acronym> 1.1 devices until
	the extension pack supporting <acronym>USB</acronym> 2.0 and
	3.0 devices becomes available on &os;.</para>

      <para>For <application>&virtualbox;</application> to be aware of
	<acronym>USB</acronym> devices attached to the machine, the
	user needs to be a member of the <systemitem
	  class="groupname">operator</systemitem> group.</para>

      <screen>&prompt.root; <userinput>pw groupmod operator -m <replaceable>yourusername</replaceable></userinput></screen>

      <para>Then, add the following to
       <filename>/etc/devfs.rules</filename>, or create this file if
       it does not exist yet:</para>

      <programlisting>[system=10]
add path 'usb/*' mode 0660 group operator</programlisting>

      <para>To load these new rules, add the following to
       <filename>/etc/rc.conf</filename>:</para>

      <programlisting>devfs_system_ruleset="system"</programlisting>

      <para>Then, restart devfs:</para>

      <screen>&prompt.root; <userinput>service devfs restart</userinput></screen>

      <para>Restart the login session and
	<application>&virtualbox;</application> for these changes to
	take effect, and create <acronym>USB</acronym> filters as
	necessary.</para>
    </sect2>

    <sect2 xml:id="virtualization-virtualbox-host-dvd-cd-access">
      <title>&virtualbox; Host
	<acronym>DVD</acronym>/<acronym>CD</acronym> Access</title>

      <para>Access to the host
	<acronym>DVD</acronym>/<acronym>CD</acronym> drives from
	guests is achieved through the sharing of the physical drives.
	Within &virtualbox;, this is set up from the Storage window in
	the Settings of the virtual machine.  If needed, create an
	empty <acronym>IDE</acronym>
	<acronym>CD</acronym>/<acronym>DVD</acronym> device first.
	Then choose the Host Drive from the popup menu for the virtual
	<acronym>CD</acronym>/<acronym>DVD</acronym> drive selection.
	A checkbox labeled <literal>Passthrough</literal> will appear.
	This allows the virtual machine to use the hardware directly.
	For example, audio <acronym>CD</acronym>s or the burner will
	only function if this option is selected.</para>

      <para><acronym>HAL</acronym> needs to run for
	<application>&virtualbox;</application>
	<acronym>DVD</acronym>/<acronym>CD</acronym> functions to
	work, so enable it in <filename>/etc/rc.conf</filename> and
	start it if it is not already running:</para>

      <programlisting>hald_enable="YES"</programlisting>

      <screen>&prompt.root; <userinput>service hald start</userinput></screen>

      <para>In order for users to be able to use
	<application>&virtualbox;</application>
	<acronym>DVD</acronym>/<acronym>CD</acronym> functions, they
	need access to <filename>/dev/xpt0</filename>,
	<filename>/dev/cd<replaceable>N</replaceable></filename>, and
	<filename>/dev/pass<replaceable>N</replaceable></filename>.
	This is usually achieved by making the user a member of
	<systemitem class="groupname">operator</systemitem>.
	Permissions to these devices have to be corrected by adding
	these lines to <filename>/etc/devfs.conf</filename>:</para>

      <programlisting>perm cd* 0660
perm xpt0 0660
perm pass* 0660</programlisting>

      <screen>&prompt.root; <userinput>service devfs restart</userinput></screen>
    </sect2>
  </sect1>

  <sect1 xml:id="virtualization-host-bhyve">
    <title>&os; as a Host with
      <application>bhyve</application></title>

    <para>The <application>bhyve</application>
      <acronym>BSD</acronym>-licensed hypervisor became part of the
      base system with &os; 10.0-RELEASE.  This hypervisor supports a
      number of guests, including &os;, OpenBSD, and many &linux;
      distributions.  By default, <application>bhyve</application>
      provides access to serial console and does not emulate a
      graphical console.  Virtualization offload features of newer
      <acronym>CPU</acronym>s are used to avoid the legacy methods of
      translating instructions and manually managing memory
      mappings.</para>

    <para>The <application>bhyve</application> design requires a
      processor that supports &intel; Extended Page Tables
      (<acronym>EPT</acronym>) or &amd; Rapid Virtualization Indexing
      (<acronym>RVI</acronym>) or Nested Page Tables
      (<acronym>NPT</acronym>).  Hosting &linux; guests or &os; guests
      with more than one <acronym>vCPU</acronym> requires
      <acronym>VMX</acronym> unrestricted mode support
      (<acronym>UG</acronym>).  Most newer processors, specifically
      the &intel;&nbsp;&core; i3/i5/i7 and &intel;&nbsp;&xeon;
      E3/E5/E7, support these features.  <acronym>UG</acronym> support
      was introduced with Intel's Westmere micro-architecture.  For a
      complete list of &intel; processors that support
      <acronym>EPT</acronym>, refer to <link
	xlink:href="https://ark.intel.com/content/www/us/en/ark/search/featurefilter.html?productType=873&amp;0_ExtendedPageTables=True"/>.
      <acronym>RVI</acronym> is found on the third generation and
      later of the &amd.opteron; (Barcelona) processors.  The easiest
      way to tell if a processor supports
      <application>bhyve</application> is to run
      <command>dmesg</command> or look in
      <filename>/var/run/dmesg.boot</filename> for the
      <literal>POPCNT</literal> processor feature flag on the
      <literal>Features2</literal> line for &amd; processors or
      <literal>EPT</literal> and <literal>UG</literal> on the
      <literal>VT-x</literal> line for &intel; processors.</para>

    <sect2 xml:id="virtualization-bhyve-prep">
      <title>Preparing the Host</title>

      <para>The first step to creating a virtual machine in
	<application>bhyve</application> is configuring the host
	system.  First, load the <application>bhyve</application>
	kernel module:</para>

      <screen>&prompt.root; <userinput>kldload vmm</userinput></screen>

      <para>Then, create a <filename>tap</filename> interface for the
	network device in the virtual machine to attach to.  In order
	for the network device to participate in the network, also
	create a bridge interface containing the
	<filename>tap</filename> interface and the physical interface
	as members.  In this example, the physical interface is
	<replaceable>igb0</replaceable>:</para>

      <screen>&prompt.root; <userinput>ifconfig <replaceable>tap0</replaceable> create</userinput>
&prompt.root; <userinput>sysctl net.link.tap.up_on_open=1</userinput>
net.link.tap.up_on_open: 0 -> 1
&prompt.root; <userinput>ifconfig <replaceable>bridge0</replaceable> create</userinput>
&prompt.root; <userinput>ifconfig <replaceable>bridge0</replaceable> addm <replaceable>igb0</replaceable> addm <replaceable>tap0</replaceable></userinput>
&prompt.root; <userinput>ifconfig <replaceable>bridge0</replaceable> up</userinput></screen>
    </sect2>

    <sect2 xml:id="virtualization-bhyve-freebsd">
      <title>Creating a FreeBSD Guest</title>

      <para>Create a file to use as the virtual disk for the guest
	machine.  Specify the size and name of the virtual
	disk:</para>

      <screen>&prompt.root; <userinput>truncate -s <replaceable>16G</replaceable> <replaceable>guest.img</replaceable></userinput></screen>

      <para>Download an installation image of &os; to install:</para>

      <screen>&prompt.root; <userinput>fetch <replaceable>ftp://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/10.3/FreeBSD-10.3-RELEASE-amd64-bootonly.iso</replaceable></userinput>
FreeBSD-10.3-RELEASE-amd64-bootonly.iso       100% of  230 MB  570 kBps 06m17s</screen>

      <para>&os; comes with an example script for running a virtual
	machine in <application>bhyve</application>.  The script will
	start the virtual machine and run it in a loop, so it will
	automatically restart if it crashes.  The script takes a
	number of options to control the configuration of the machine:
	<option>-c</option> controls the number of virtual CPUs,
	<option>-m</option> limits the amount of memory available to
	the guest, <option>-t</option> defines which
	<filename>tap</filename> device to use, <option>-d</option>
	indicates which disk image to use, <option>-i</option> tells
	<application>bhyve</application> to boot from the
	<acronym>CD</acronym> image instead of the disk, and
	<option>-I</option> defines which <acronym>CD</acronym> image
	to use.  The last parameter is the name of the virtual
	machine, used to track the running machines.  This example
	starts the virtual machine in installation mode:</para>

      <screen>&prompt.root; <userinput>sh /usr/share/examples/bhyve/vmrun.sh -c <replaceable>1</replaceable> -m <replaceable>1024M</replaceable> -t <replaceable>tap0</replaceable> -d <replaceable>guest.img</replaceable> -i -I <replaceable>FreeBSD-10.3-RELEASE-amd64-bootonly.iso</replaceable> <replaceable>guestname</replaceable></userinput></screen>

      <para>The virtual machine will boot and start the installer.
	After installing a system in the virtual machine, when the
	system asks about dropping in to a shell at the end of the
	installation, choose <guibutton>Yes</guibutton>.</para>

      <para>Reboot the virtual machine.  While rebooting the virtual
	machine causes <application>bhyve</application> to exit, the
	<filename>vmrun.sh</filename> script runs
	<command>bhyve</command> in a loop and will automatically
	restart it.  When this happens, choose the reboot option from
	the boot loader menu in order to escape the loop.  Now the
	guest can be started from the virtual disk:</para>

      <screen>&prompt.root; <userinput>sh /usr/share/examples/bhyve/vmrun.sh -c <replaceable>4</replaceable> -m <replaceable>1024M</replaceable> -t <replaceable>tap0</replaceable> -d <replaceable>guest.img</replaceable> <replaceable>guestname</replaceable></userinput></screen>
    </sect2>

    <sect2 xml:id="virtualization-bhyve-linux">
      <title>Creating a &linux; Guest</title>

      <para>In order to boot operating systems other than &os;, the
	<package>sysutils/grub2-bhyve</package> port must be first
	installed.</para>

      <para>Next, create a file to use as the virtual disk for the
	guest machine:</para>

      <screen>&prompt.root; <userinput>truncate -s <replaceable>16G</replaceable> <replaceable>linux.img</replaceable></userinput></screen>

      <para>Starting a virtual machine with
	<application>bhyve</application> is a two step process.  First
	a kernel must be loaded, then the guest can be started.  The
	&linux; kernel is loaded with
	<package>sysutils/grub2-bhyve</package>.  Create a
	<filename>device.map</filename> that
	<application>grub</application> will use to map the virtual
	devices to the files on the host system:</para>

      <programlisting>(hd0) ./linux.img
(cd0) ./somelinux.iso</programlisting>

      <para>Use <package>sysutils/grub2-bhyve</package> to load the
	&linux; kernel from the <acronym>ISO</acronym> image:</para>

      <screen>&prompt.root; <userinput>grub-bhyve -m device.map -r cd0 -M <replaceable>1024M</replaceable> <replaceable>linuxguest</replaceable></userinput></screen>

      <para>This will start grub.  If the installation
	<acronym>CD</acronym> contains a
	<filename>grub.cfg</filename>, a menu will be displayed.
	If not, the <literal>vmlinuz</literal> and
	<literal>initrd</literal> files must be located and loaded
	manually:</para>

      <screen>grub> <userinput>ls</userinput>
(hd0) (cd0) (cd0,msdos1) (host)
grub> <userinput>ls (cd0)/isolinux</userinput>
boot.cat boot.msg grub.conf initrd.img isolinux.bin isolinux.cfg memtest
splash.jpg TRANS.TBL vesamenu.c32 vmlinuz
grub> <userinput>linux (cd0)/isolinux/vmlinuz</userinput>
grub> <userinput>initrd (cd0)/isolinux/initrd.img</userinput>
grub> <userinput>boot</userinput></screen>

      <para>Now that the &linux; kernel is loaded, the guest can be
	started:</para>

      <screen>&prompt.root; <userinput>bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,<replaceable>tap0</replaceable> -s 3:0,virtio-blk,<replaceable>./linux.img</replaceable> \
    -s 4:0,ahci-cd,<replaceable>./somelinux.iso</replaceable> -l com1,stdio -c <replaceable>4</replaceable> -m <replaceable>1024M</replaceable> <replaceable>linuxguest</replaceable></userinput></screen>

      <para>The system will boot and start the installer.  After
	installing a system in the virtual machine, reboot the virtual
	machine.  This will cause <application>bhyve</application> to
	exit.  The instance of the virtual machine needs to be
	destroyed before it can be started again:</para>

      <screen>&prompt.root; <userinput>bhyvectl --destroy --vm=<replaceable>linuxguest</replaceable></userinput></screen>

      <para>Now the guest can be started directly from the virtual
	disk.  Load the kernel:</para>

      <screen>&prompt.root; <userinput>grub-bhyve -m device.map -r hd0,msdos1 -M <replaceable>1024M</replaceable> <replaceable>linuxguest</replaceable></userinput>
grub> <userinput>ls</userinput>
(hd0) (hd0,msdos2) (hd0,msdos1) (cd0) (cd0,msdos1) (host)
(lvm/VolGroup-lv_swap) (lvm/VolGroup-lv_root)
grub> <userinput>ls (hd0,msdos1)/</userinput>
lost+found/ grub/ efi/ System.map-2.6.32-431.el6.x86_64 config-2.6.32-431.el6.x
86_64 symvers-2.6.32-431.el6.x86_64.gz vmlinuz-2.6.32-431.el6.x86_64
initramfs-2.6.32-431.el6.x86_64.img
grub> <userinput>linux (hd0,msdos1)/vmlinuz-2.6.32-431.el6.x86_64 root=/dev/mapper/VolGroup-lv_root</userinput>
grub> <userinput>initrd (hd0,msdos1)/initramfs-2.6.32-431.el6.x86_64.img</userinput>
grub> <userinput>boot</userinput></screen>

      <para>Boot the virtual machine:</para>

      <screen>&prompt.root; <userinput>bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,<replaceable>tap0</replaceable> \
    -s 3:0,virtio-blk,<replaceable>./linux.img</replaceable> -l com1,stdio -c <replaceable>4</replaceable> -m <replaceable>1024M</replaceable> <replaceable>linuxguest</replaceable></userinput></screen>

      <para>&linux; will now boot in the virtual machine and
	eventually present you with the login prompt.  Login and use
	the virtual machine.  When you are finished, reboot the
	virtual machine to exit <application>bhyve</application>.
	Destroy the virtual machine instance:</para>

      <screen>&prompt.root; <userinput>bhyvectl --destroy --vm=<replaceable>linuxguest</replaceable></userinput></screen>
    </sect2>

    <sect2 xml:id="virtualization-bhyve-uefi">
      <title>Booting <application>bhyve</application> Virtual Machines
	with <acronym>UEFI</acronym> Firmware</title>

      <para>In addition to <application>bhyveload</application> and
	<application>grub-bhyve</application>, the
	<application>bhyve</application> hypervisor can also boot
	virtual machines using the <acronym>UEFI</acronym> userspace
	firmware.  This option may support guest operating systems
	that are not supported by the other loaders.</para>

      <para>In order to make use of the <acronym>UEFI</acronym>
	support in <application>bhyve</application>, first obtain the
	<acronym>UEFI</acronym> firmware images.  This can be done by
	installing <package>sysutils/bhyve-firmware</package> port or
	package.</para>

      <para>With the firmware in place, add the flags <option>-l
	  bootrom,<replaceable>/path/to/firmware</replaceable></option>
	to your <application>bhyve</application> command line.  The
	actual <application>bhyve</application> command may look like
	this:</para>

      <screen>&prompt.root; <userinput>bhyve -AHP -s 0:0,hostbridge -s 1:0,lpc \
-s 2:0,virtio-net,<replaceable>tap1</replaceable> -s 3:0,virtio-blk,<replaceable>./disk.img</replaceable> \
-s 4:0,ahci-cd,<replaceable>./install.iso</replaceable> -c <replaceable>4</replaceable> -m <replaceable>1024M</replaceable> \
-l bootrom,<replaceable>/usr/local/share/uefi-firmware/BHYVE_UEFI.fd</replaceable> \
<replaceable>guest</replaceable></userinput></screen>

      <para><package>sysutils/bhyve-firmware</package> also contains a
	<acronym>CSM</acronym>-enabled firmware, to boot guests with
	no <acronym>UEFI</acronym> support in legacy
	<acronym>BIOS</acronym> mode:</para>

      <screen>&prompt.root; <userinput>bhyve -AHP -s 0:0,hostbridge -s 1:0,lpc \
-s 2:0,virtio-net,<replaceable>tap1</replaceable> -s 3:0,virtio-blk,<replaceable>./disk.img</replaceable> \
-s 4:0,ahci-cd,<replaceable>./install.iso</replaceable> -c <replaceable>4</replaceable> -m <replaceable>1024M</replaceable> \
-l bootrom,<replaceable>/usr/local/share/uefi-firmware/BHYVE_UEFI_CSM.fd</replaceable> \
<replaceable>guest</replaceable></userinput></screen>
    </sect2>

    <sect2 xml:id="virtualization-bhyve-framebuffer">
      <title>Graphical <acronym>UEFI</acronym> Framebuffer for
	<application>bhyve</application> Guests</title>

      <para>The <acronym>UEFI</acronym> firmware support is
	particularly useful with predominantly graphical guest
	operating systems such as Microsoft &windows;.</para>

      <para>Support for the UEFI-GOP framebuffer may also be enabled
	with the <option>-s
	  29,fbuf,tcp=<replaceable>0.0.0.0:5900</replaceable></option>
	flags.  The framebuffer resolution may be configured with
	<option>w=<replaceable>800</replaceable></option> and
	<option>h=<replaceable>600</replaceable></option>, and
	<application>bhyve</application> can be instructed to wait for
	a <acronym>VNC</acronym> connection before booting the guest
	by adding <option>wait</option>.  The framebuffer may be
	accessed from the host or over the network via the
	<acronym>VNC</acronym> protocol.  Additionally, <option>-s
	  30,xhci,tablet</option> can be added to achieve precise mouse
	cursor synchronization with the host.</para>

      <para>The resulting <application>bhyve</application> command
	would look like this:</para>

      <screen>&prompt.root; <userinput>bhyve -AHP -s 0:0,hostbridge -s 31:0,lpc \
-s 2:0,virtio-net,<replaceable>tap1</replaceable> -s 3:0,virtio-blk,<replaceable>./disk.img</replaceable> \
-s 4:0,ahci-cd,<replaceable>./install.iso</replaceable> -c <replaceable>4</replaceable> -m <replaceable>1024M</replaceable> \
-s 29,fbuf,tcp=<replaceable>0.0.0.0:5900</replaceable>,w=<replaceable>800</replaceable>,h=<replaceable>600</replaceable>,wait \
-s 30,xhci,tablet \
-l bootrom,<replaceable>/usr/local/share/uefi-firmware/BHYVE_UEFI.fd</replaceable> \
<replaceable>guest</replaceable></userinput></screen>

      <para>Note, in BIOS emulation mode, the framebuffer will cease
	receiving updates once control is passed from firmware to
	guest operating system.</para>
    </sect2>

    <sect2 xml:id="virtualization-bhyve-zfs">
      <title>Using <acronym>ZFS</acronym> with
	<application>bhyve</application> Guests</title>

      <para>If <acronym>ZFS</acronym> is available on the host
	machine, using <acronym>ZFS</acronym> volumes
	instead of disk image files can provide significant
	performance benefits for the guest <acronym>VMs</acronym>.  A
	<acronym>ZFS</acronym> volume can be created by:</para>

      <screen>&prompt.root; <userinput>zfs create -V<replaceable>16G</replaceable> -o volmode=dev <replaceable>zroot/linuxdisk0</replaceable></userinput></screen>

      <para>When starting the <acronym>VM</acronym>, specify the
	<acronym>ZFS</acronym> volume as the disk drive:</para>

      <screen>&prompt.root; <userinput>bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,<replaceable>tap0</replaceable> -s3:0,virtio-blk,<replaceable>/dev/zvol/zroot/linuxdisk0</replaceable> \
    -l com1,<replaceable>stdio</replaceable> -c <replaceable>4</replaceable> -m <replaceable>1024M</replaceable> <replaceable>linuxguest</replaceable></userinput></screen>
    </sect2>

    <sect2 xml:id="virtualization-bhyve-nmdm">
      <title>Virtual Machine Consoles</title>

      <para>It is advantageous to wrap the
	<application>bhyve</application> console in a session
	management tool such as <package>sysutils/tmux</package> or
	<package>sysutils/screen</package> in order to detach and
	reattach to the console.  It is also possible to have the
	console of <application>bhyve</application> be a null modem
	device that can be accessed with <command>cu</command>.  To do
	this, load the <filename>nmdm</filename> kernel module and
	replace <option>-l com1,stdio</option> with
	<option>-l com1,/dev/nmdm0A</option>.  The
	<filename>/dev/nmdm</filename> devices are created
	automatically as needed, where each is a pair, corresponding
	to the two ends of the null modem cable
	(<filename>/dev/nmdm0A</filename> and
	<filename>/dev/nmdm0B</filename>).  See &man.nmdm.4; for more
	information.</para>

      <screen>&prompt.root; <userinput>kldload nmdm</userinput>
&prompt.root; <userinput>bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,<replaceable>tap0</replaceable> -s 3:0,virtio-blk,<replaceable>./linux.img</replaceable> \
    -l com1,<replaceable>/dev/nmdm0A</replaceable> -c <replaceable>4</replaceable> -m <replaceable>1024M</replaceable> <replaceable>linuxguest</replaceable></userinput>
&prompt.root; <userinput>cu -l <replaceable>/dev/nmdm0B</replaceable></userinput>
Connected

Ubuntu 13.10 handbook ttyS0

handbook login:</screen>
    </sect2>

    <sect2 xml:id="virtualization-bhyve-managing">
      <title>Managing Virtual Machines</title>

      <para>A device node is created in <filename
	  role="directory">/dev/vmm</filename> for each virtual
	machine.  This allows the administrator to easily see a list
	of the running virtual machines:</para>

      <screen>&prompt.root; <userinput>ls -al /dev/vmm</userinput>
total 1
dr-xr-xr-x   2 root  wheel    512 Mar 17 12:19 ./
dr-xr-xr-x  14 root  wheel    512 Mar 17 06:38 ../
crw-------   1 root  wheel  0x1a2 Mar 17 12:20 guestname
crw-------   1 root  wheel  0x19f Mar 17 12:19 linuxguest
crw-------   1 root  wheel  0x1a1 Mar 17 12:19 otherguest</screen>

      <para>A specified virtual machine can be destroyed using
	<command>bhyvectl</command>:</para>

      <screen>&prompt.root; <userinput>bhyvectl --destroy --vm=<replaceable>guestname</replaceable></userinput></screen>
    </sect2>

    <sect2 xml:id="virtualization-bhyve-onboot">
      <title>Persistent Configuration</title>

      <para>In order to configure the system to start
	<application>bhyve</application> guests at boot time, the
	following configurations must be made in the specified
	files:</para>

      <procedure>
	<step>
	  <title><filename>/etc/sysctl.conf</filename></title>

	  <programlisting>net.link.tap.up_on_open=1</programlisting>
	</step>

	<step>
	  <title><filename>/etc/rc.conf</filename></title>

	  <programlisting>cloned_interfaces="<replaceable>bridge0</replaceable> <replaceable>tap0</replaceable>"
ifconfig_bridge0="addm <replaceable>igb0</replaceable> addm <replaceable>tap0</replaceable>"
kld_list="nmdm vmm"</programlisting>
	</step>
      </procedure>
    </sect2>
  </sect1>

  <sect1 xml:id="virtualization-host-xen">
    <title>&os; as a &xen;-Host</title>

    <para><application>Xen</application> is a GPLv2-licensed <link
	xlink:href="https://en.wikipedia.org/wiki/Hypervisor#Classification">type
	1 hypervisor</link> for &intel; and &arm; architectures.  &os;
      has included &i386; and &amd;&nbsp;64-Bit <link
	xlink:href="https://wiki.xenproject.org/wiki/DomU">DomU</link>
      and <link
	xlink:href="https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud">Amazon
	EC2</link> unprivileged domain (virtual machine) support since
      &os;&nbsp;8.0 and includes Dom0 control domain (host) support in
      &os;&nbsp;11.0.  Support for para-virtualized (PV) domains has
      been removed from &os;&nbsp;11 in favor of hardware virtualized
      (HVM) domains, which provides better performance.</para>

    <para>&xen; is a bare-metal hypervisor, which means that it is the
      first program loaded after the BIOS. A special privileged guest
      called the Domain-0 (<literal>Dom0</literal> for short) is then
      started.  The Dom0 uses its special privileges to directly
      access the underlying physical hardware, making it a
      high-performance solution.  It is able to access the disk
      controllers and network adapters directly.  The &xen; management
      tools to manage and control the &xen; hypervisor are also used
      by the Dom0 to create, list, and destroy VMs.  Dom0 provides
      virtual disks and networking for unprivileged domains, often
      called <literal>DomU</literal>.  &xen; Dom0 can be compared to
      the service console of other hypervisor solutions, while the
      DomU is where individual guest VMs are run.</para>

<!-- Hidden until the mode in which FreeBSD uses Xen is supported.
    <para>Features of &xen; include GPU passthrough from the host
      running the Dom0 into a DomU guest machine.  This requires a
      CPU, chipset, and BIOS with VT-D support and might require extra
      patches or not work with all graphics cards.  A list of adapters
      can be found in the <link
	xlink:href="https://wiki.xenproject.org/wiki/Xen_VGA_Passthrough_Tested_Adapters">Xen
	Wiki</link>.  Note that not all GPUs listed there are
      supported on &os;.  The  &xen; hypervisor also supports PCI
      passthrough to give a DomU guest full, direct access to a PCI
      device like NIC, disk controller, or soundcard.</para>
-->
    <para>&xen; can migrate VMs between different &xen; servers.  When
      the two xen hosts share the same underlying storage, the
      migration can be done without having to shut the VM down first.
      Instead, the migration is performed live while the DomU is
      running and there is no need to restart it or plan a downtime.
      This is useful in maintenance scenarios or upgrade windows to
      ensure that the services provided by the DomU are still
      provided.  Many more features of &xen; are listed on the <link
	xlink:href="https://wiki.xenproject.org/wiki/Category:Overview">Xen
	Wiki Overview page</link>.  Note that not all features are
      supported on &os; yet.</para>

    <sect2 xml:id="virtualization-host-xen-requirements">
      <title>Hardware Requirements for &xen; Dom0</title>

      <para>To run the &xen; hypervisor on a host, certain hardware
	functionality is required.  Hardware virtualized domains
	require Extended Page Table (<link
	  xlink:href="http://en.wikipedia.org/wiki/Extended_Page_Table">EPT</link>)
	and Input/Output Memory Management Unit (<link
	  xlink:href="http://en.wikipedia.org/wiki/List_of_IOMMU-supporting_hardware">IOMMU</link>)
	support in the host processor.</para>

      <note>
	<para>In order to run a FreeBSD &xen; Dom0 the box must be
	  booted using legacy boot (BIOS).</para>
      </note>

    </sect2>

    <sect2 xml:id="virtualization-host-xen-dom0-setup">
      <title>&xen; Dom0 Control Domain Setup</title>

      <para>Users of &os;&nbsp;11 should install the
	<package>emulators/xen-kernel47</package> and
	<package>sysutils/xen-tools47</package> packages that are
	based on Xen version 4.7.  Systems running on &os;-12.0 or
	newer can use Xen 4.11 provided by
	<package>emulators/xen-kernel411</package> and
	<package>sysutils/xen-tools411</package>, respectively.</para>

      <para>Configuration files must be edited to prepare the host
	for the Dom0 integration after the Xen packages are installed.
	An entry to <filename>/etc/sysctl.conf</filename> disables the
	limit on how many pages of memory are allowed to be wired.
	Otherwise, DomU VMs with higher memory requirements will not
	run.</para>

      <screen>&prompt.root; <userinput>echo 'vm.max_wired=-1' >> /etc/sysctl.conf</userinput></screen>

      <para>Another memory-related setting involves changing
	<filename>/etc/login.conf</filename>, setting the
	<literal>memorylocked</literal> option to
	<literal>unlimited</literal>.  Otherwise, creating DomU
	domains may fail with <errorname>Cannot allocate
	  memory</errorname> errors.  After making the change to
	<filename>/etc/login.conf</filename>, run
	<command>cap_mkdb</command> to update the capability database.
	See <xref linkend="security-resourcelimits"/> for
	details.</para>

      <screen>&prompt.root; <userinput>sed -i '' -e 's/memorylocked=64K/memorylocked=unlimited/' /etc/login.conf</userinput>
&prompt.root; <userinput>cap_mkdb /etc/login.conf</userinput></screen>

      <para>Add an entry for the &xen; console to
	<filename>/etc/ttys</filename>:</para>

      <screen>&prompt.root; <userinput>echo 'xc0     "/usr/libexec/getty Pc"         xterm   onifconsole  secure' >> /etc/ttys</userinput></screen>

      <para>Selecting a &xen; kernel in
	<filename>/boot/loader.conf</filename> activates the Dom0.
	&xen; also requires resources like CPU and memory from the
	host machine for itself and other DomU domains.  How much CPU
	and memory depends on the individual requirements and hardware
	capabilities.  In this example, 8&nbsp;GB of memory and 4
	virtual CPUs are made available for the Dom0. The serial
	console is also activated and logging options are
	defined.</para>

      <para>The following command is used for Xen 4.7 packages:</para>

      <screen>&prompt.root; <userinput>sysrc -f /boot/loader.conf hw.pci.mcfg=0</userinput>
&prompt.root; <userinput>sysrc -f /boot/loader.conf if_tap_load="YES"</userinput>
&prompt.root; <userinput>sysrc -f /boot/loader.conf xen_kernel="/boot/xen"</userinput>
&prompt.root; <userinput>sysrc -f /boot/loader.conf xen_cmdline="dom0_mem=<replaceable>8192M</replaceable> dom0_max_vcpus=<replaceable>4</replaceable> dom0pvh=1 console=com1,vga com1=115200,8n1 guest_loglvl=all loglvl=all"</userinput></screen>

      <para>For Xen versions 4.11 and higher, the following command
	should be used instead:</para>

      <screen>&prompt.root; <userinput>sysrc -f /boot/loader.conf if_tap_load="YES"</userinput>
&prompt.root; <userinput>sysrc -f /boot/loader.conf xen_kernel="/boot/xen"</userinput>
&prompt.root; <userinput>sysrc -f /boot/loader.conf xen_cmdline="dom0_mem=<replaceable>8192M</replaceable> dom0_max_vcpus=<replaceable>4</replaceable> dom0=pvh console=com1,vga com1=115200,8n1 guest_loglvl=all loglvl=all"</userinput></screen>

	<tip>
	  <para>Log files that &xen; creates for the DomU VMs
	    are stored in <filename>/var/log/xen</filename>.  Please
	    be sure to check the contents of that directory if
	    experiencing issues.</para>
	</tip>

	<para>Activate the xencommons service during system
	  startup:</para>

	<screen>&prompt.root; <userinput>sysrc xencommons_enable=yes</userinput></screen>

	<para>These settings are enough to start a Dom0-enabled
	  system.  However, it lacks network functionality for the
	  DomU machines.  To fix that, define a bridged interface with
	  the main NIC of the system which the DomU VMs can use to
	  connect to the network.  Replace
	  <replaceable>em0</replaceable> with the host network
	  interface name.</para>

	<screen>&prompt.root; <userinput>sysrc cloned_interfaces="bridge0"</userinput>
&prompt.root; <userinput>sysrc ifconfig_bridge0="addm <replaceable>em0</replaceable> SYNCDHCP"</userinput>
&prompt.root; <userinput>sysrc ifconfig_<replaceable>em0</replaceable>="up"</userinput></screen>

	<para>Restart the host to load the &xen; kernel and start the
	  Dom0.</para>

	<screen>&prompt.root; <userinput>reboot</userinput></screen>

	<para>After successfully booting the &xen; kernel and logging
	  into the system again, the &xen; management tool
	  <command>xl</command> is used to show information about the
	  domains.</para>

	<screen>&prompt.root; <userinput>xl list</userinput>
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  8192     4     r-----     962.0</screen>

	<para>The output confirms that the Dom0 (called
	  <literal>Domain-0</literal>) has the ID <literal>0</literal>
	  and is running.  It also has the memory and virtual CPUs
	  that were defined in <filename>/boot/loader.conf</filename>
	  earlier.  More information can be found in the <link
	    xlink:href="https://www.xenproject.org/help/documentation.html">&xen;
	    Documentation</link>.  DomU guest VMs can now be
	  created.</para>
      </sect2>

      <sect2 xml:id="virtualization-host-xen-domu-setup">
	<title>&xen; DomU Guest VM Configuration</title>

	<para>Unprivileged domains consist of a configuration file and
	  virtual or physical hard disks.  Virtual disk storage for
	  the DomU can be files created by &man.truncate.1; or ZFS
	  volumes as described in <xref linkend="zfs-zfs-volume"/>.
	  In this example, a 20&nbsp;GB volume is used.  A VM is
	  created with the ZFS volume, a &os; ISO image, 1&nbsp;GB of
	  RAM and two virtual CPUs.  The ISO installation file is
	  retrieved with &man.fetch.1; and saved locally in a file
	  called <filename>freebsd.iso</filename>.</para>

	<screen>&prompt.root; <userinput>fetch <replaceable>ftp://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/<replaceable>12.0</replaceable>/FreeBSD-<replaceable>12.0</replaceable>-RELEASE-amd64-bootonly.iso</replaceable> -o <replaceable>freebsd.iso</replaceable></userinput></screen>

      <para>A ZFS volume of 20&nbsp;GB called
	<filename>xendisk0</filename> is created to serve as the disk
	space for the VM.</para>

      <screen>&prompt.root; <userinput>zfs create -V20G -o volmode=dev zroot/xendisk0</userinput></screen>

      <para>The new DomU guest VM is defined in a file.  Some specific
	definitions like name, keymap, and VNC connection details are
	also defined.  The following <filename>freebsd.cfg</filename>
	contains a minimum DomU configuration for this example:</para>

      <screen>&prompt.root; <userinput>cat freebsd.cfg</userinput>
builder = "hvm" <co xml:id="co-xen-builder"/>
name = "freebsd" <co xml:id="co-xen-name"/>
memory = 1024 <co xml:id="co-xen-memory"/>
vcpus = 2 <co xml:id="co-xen-vcpus"/>
vif = [ 'mac=00:16:3E:74:34:32,bridge=bridge0' ] <co xml:id="co-xen-vif"/>
disk = [
'/dev/zvol/tank/xendisk0,raw,hda,rw', <co xml:id="co-xen-disk"/>
'/root/freebsd.iso,raw,hdc:cdrom,r' <co xml:id="co-xen-cdrom"/>
  ]
vnc = 1 <co xml:id="co-xen-vnc"/>
vnclisten = "0.0.0.0"
serial = "pty"
usbdevice = "tablet"</screen>

      <para>These lines are explained in more detail:</para>

      <calloutlist>
	<callout arearefs="co-xen-builder">
	  <para>This defines what kind of virtualization to use.
	    <literal>hvm</literal> refers to hardware-assisted
	    virtualization or hardware virtual machine.  Guest
	    operating systems can run unmodified on CPUs with
	    virtualization extensions, providing nearly the same
	    performance as running on physical hardware.
	    <literal>generic</literal> is the default value and
	    creates a PV domain.</para>
	</callout>

	<callout arearefs="co-xen-name">
	  <para>Name of this virtual machine to distinguish it from
	    others running on the same Dom0.  Required.</para>
	</callout>

	<callout arearefs="co-xen-memory">
	  <para>Quantity of RAM in megabytes to make available to the
	    VM.  This amount is subtracted from the hypervisor's total
	    available memory, not the memory of the Dom0.</para>
	</callout>

	<callout arearefs="co-xen-vcpus">
	  <para>Number of virtual CPUs available to the guest VM.  For
	    best performance, do not create guests with more virtual
	    CPUs than the number of physical CPUs on the host.</para>
	</callout>

	<callout arearefs="co-xen-vif">
	  <para>Virtual network adapter.  This is the bridge connected
	    to the network interface of the host.  The
	    <literal>mac</literal> parameter is the MAC address set on
	    the virtual network interface.  This parameter is
	    optional, if no MAC is provided &xen; will generate a
	    random one.</para>
	</callout>

	<callout arearefs="co-xen-disk">
	  <para>Full path to the disk, file, or ZFS volume of the disk
	    storage for this VM.  Options and multiple disk
	    definitions are separated by commas.</para>
	</callout>

	<callout arearefs="co-xen-cdrom">
	  <para>Defines the Boot medium from which the initial
	    operating system is installed.  In this example, it is the
	    ISO imaged downloaded earlier.  Consult the &xen;
	    documentation for other kinds of devices and options to
	    set.</para>
	</callout>

	<callout arearefs="co-xen-vnc">
	  <para>Options controlling VNC connectivity to the serial
	    console of the DomU.  In order, these are: active VNC
	    support, define IP address on which to listen, device node
	    for the serial console, and the input method for precise
	    positioning of the mouse and other input methods.
	    <literal>keymap</literal> defines which keymap to use, and
	    is <literal>english</literal> by default.</para>
	</callout>
      </calloutlist>

      <para>After the file has been created with all the necessary
	options, the DomU is created by passing it to <command>xl
	  create</command> as a parameter.</para>

      <screen>&prompt.root; <userinput>xl create freebsd.cfg</userinput></screen>

      <note>
	<para>Each time the Dom0 is restarted, the configuration file
	  must be passed to <command>xl create</command> again to
	  re-create the DomU.  By default, only the Dom0 is created
	  after a reboot, not the individual VMs.  The VMs can
	  continue where they left off as they stored the operating
	  system on the virtual disk.  The virtual machine
	  configuration can change over time (for example, when adding
	  more memory).  The virtual machine configuration files must
	  be properly backed up and kept available to be able to
	  re-create the guest VM when needed.</para>
      </note>

      <para>The output of <command>xl list</command> confirms that the
	DomU has been created.</para>

      <screen>&prompt.root; <userinput>xl list</userinput>
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  8192     4     r-----  1653.4
freebsd                                      1  1024     1     -b----   663.9</screen>

      <para>To begin the installation of the base operating system,
	start the VNC client, directing it to the main network address
	of the host or to the IP address defined on the
	<literal>vnclisten</literal> line of
	<filename>freebsd.cfg</filename>.  After the operating system
	has been installed, shut down the DomU and disconnect the VNC
	viewer.  Edit <filename>freebsd.cfg</filename>, removing the
	line with the <literal>cdrom</literal> definition or
	commenting it out by inserting a <literal>#</literal>
	character at the beginning of the line.  To load this new
	configuration, it is necessary to remove the old DomU with
	<command>xl destroy</command>, passing either the name or the
	id as the parameter.  Afterwards, recreate it using the
	modified <filename>freebsd.cfg</filename>.</para>

      <screen>&prompt.root; <userinput>xl destroy freebsd</userinput>
&prompt.root; <userinput>xl create freebsd.cfg</userinput></screen>

      <para>The machine can then be accessed again using the VNC
	viewer.  This time, it will boot from the virtual disk where
	the operating system has been installed and can be used as a
	virtual machine.</para>
    </sect2>

    <sect2 xml:id="virtualization-host-xen-troubleshooting">
      <title>Troubleshooting</title>

      <para>This section contains basic information in order to help
	troubleshoot issues found when using &os; as a &xen; host or
	guest.</para>

      <sect3 xml:id="virtualization-host-xen-troubleshooting-host">
	<title>Host Boot Troubleshooting</title>

	<para>Please note that the following troubleshooting tips
	  are intended for &xen; 4.11 or newer.  If you are still
	  using &xen; 4.7 and having issues consider migrating to
	  a newer version of &xen;.</para>

	<para>In order to troubleshoot host boot issues you will
	  likely need a serial cable, or a debug USB cable.  Verbose
	  &xen; boot output can be obtained by adding options to the
	  <literal>xen_cmdline</literal> option found in
	  <filename>loader.conf</filename>.  A couple of relevant
	  debug options are:</para>

	<itemizedlist>
	  <listitem>
	    <para><literal>iommu=debug</literal>: can be used to print
	      additional diagnostic information about the
	      iommu.</para>
	  </listitem>
	  <listitem>
	    <para><literal>dom0=verbose</literal>: can be used to
	      print additional diagnostic information about the
	      dom0 build process.</para>
	  </listitem>
	  <listitem>
	    <para><literal>sync_console</literal>: flag to force
	      synchronous console output.  Useful for debugging to
	      avoid losing messages due to rate limiting.
	      Never use this option in production environments since
	      it can allow malicious guests to perform DoS attacks
	      against &xen; using the console.</para>
	  </listitem>
	</itemizedlist>

	<para>&os; should also be booted in verbose mode in order
	  to identify any issues.  To activate verbose booting, run
	  this command:</para>

	<screen>&prompt.root; <userinput>sysrc -f /boot/loader.conf boot_verbose="YES"</userinput></screen>

	<para>If none of these options help solving the problem,
	  please send the serial boot log to
	  <email>freebsd-xen@FreeBSD.org</email> and
	  <email>xen-devel@lists.xenproject.org</email>
	  for further analysis.</para>
      </sect3>

      <sect3 xml:id="virtualization-host-xen-troubleshooting-guest">
	<title>Guest Creation Troubleshooting</title>

	<para>Issues can also arise when creating guests, the
	  following attempts to provide some help for those trying
	  to diagnose guest creation issues.</para>

	<para>The most common cause of guest creation failures is the
	  <literal>xl</literal> command spitting some error and
	  exiting with a return code different than 0.  If the error
	  provided is not enough to help identify the issue, more
	  verbose output can also be obtained from
	  <literal>xl</literal> by using the <literal>v</literal>
	  option repeatedly.</para>

	<screen>&prompt.root; <userinput>xl -vvv create freebsd.cfg</userinput>
Parsing config from freebsd.cfg
libxl: debug: libxl_create.c:1693:do_domain_create: Domain 0:ao 0x800d750a0: create: how=0x0 callback=0x0 poller=0x800d6f0f0
libxl: debug: libxl_device.c:397:libxl__device_disk_set_backend: Disk vdev=xvda spec.backend=unknown
libxl: debug: libxl_device.c:432:libxl__device_disk_set_backend: Disk vdev=xvda, using backend phy
libxl: debug: libxl_create.c:1018:initiate_domain_create: Domain 1:running bootloader
libxl: debug: libxl_bootloader.c:328:libxl__bootloader_run: Domain 1:not a PV/PVH domain, skipping bootloader
libxl: debug: libxl_event.c:689:libxl__ev_xswatch_deregister: watch w=0x800d96b98: deregister unregistered
domainbuilder: detail: xc_dom_allocate: cmdline="", features=""
domainbuilder: detail: xc_dom_kernel_file: filename="/usr/local/lib/xen/boot/hvmloader"
domainbuilder: detail: xc_dom_malloc_filemap    : 326 kB
libxl: debug: libxl_dom.c:988:libxl__load_hvm_firmware_module: Loading BIOS: /usr/local/share/seabios/bios.bin
...</screen>

	<para>If the verbose output does not help diagnose the issue
	  there are also QEMU and &xen; toolstack logs in
	  <filename>/var/log/xen</filename>.  Note that the name of
	  the domain is appended to the log name, so if the domain
	  is named <literal>freebsd</literal> you should find a
	  <filename>/var/log/xen/xl-freebsd.log</filename> and likely
	  a <filename>/var/log/xen/qemu-dm-freebsd.log</filename>.
	  Both log files can contain useful information for debugging.
	  If none of this helps solve the issue, please send the
	  description of the issue you are facing and as much
	  information as possible to
	  <email>freebsd-xen@FreeBSD.org</email> and
	  <email>xen-devel@lists.xenproject.org</email> in order to
	  get help.</para>
      </sect3>
    </sect2>
  </sect1>
</chapter>
