---
description: '이 장에서는 UNIX 시스템의 네트워크 서비스중 자주 사용되는 몇 가지를 다룹니다'
next: books/handbook/firewalls
part: 'IV. 네트워크 통신'
path: /books/handbook/
prev: books/handbook/mail
showBookMenu: 'true'
tags: ["network", "servers", "inetd", "NFS", "NIS", "LDAP", "DHCP", "DNS", "Apache HTTP", "FTP", "Samba", "NTP", "iSCSI"]
title: '31장. 네트워크 서버'
weight: 36
---

[[network-servers]]
= 네트워크 서버
:doctype: book
:toc: macro
:toclevels: 1
:icons: font
:sectnums:
:sectnumlevels: 6
:sectnumoffset: 31
:partnums:
:source-highlighter: rouge
:experimental:
:images-path: books/handbook/network-servers/

ifdef::env-beastie[]
ifdef::backend-html5[]
:imagesdir: ../../../../images/{images-path}
endif::[]
ifndef::book[]
include::shared/authors.adoc[]
include::shared/mirrors.adoc[]
include::shared/releases.adoc[]
include::shared/attributes/attributes-{{% lang %}}.adoc[]
include::shared/{{% lang %}}/teams.adoc[]
include::shared/{{% lang %}}/mailing-lists.adoc[]
include::shared/{{% lang %}}/urls.adoc[]
toc::[]
endif::[]
ifdef::backend-pdf,backend-epub3[]
include::../../../../../shared/asciidoctor.adoc[]
endif::[]
endif::[]

ifndef::env-beastie[]
toc::[]
include::../../../../../shared/asciidoctor.adoc[]
endif::[]

[[network-servers-synopsis]]
== 요약

이 장에서는 UNIX(R) 시스템의 네트워크 서비스중 자주 사용되는 몇 가지를 다룹니다. 여기에는 다양한 유형의 네트워크 서비스 설치, 구성, 테스트 및 유지 관리가 포함됩니다. 이 장에는 참조를 위해 구성 파일 예제가 포함되어 있습니다.

이 장이 끝나면 여러분은 다음을 배우게 됩니다:

* inetd 데몬을 관리하는 법.
* 네트워크 파일 시스템(NFS)를 설정하는 법.
* 사용자 계정을 중앙 집중화하고 공유하기 위해 네트워크 정보 서버(Network Information Server, NIS)를 설정하는 법.
* FreeBSD를 LDAP 서버 또는 클라이언트로 작동하도록 설정하는 방법
* DHCP를 사용하여 자동 네트워크 설정을 하는 방법.
* 도메인 네임 서버(DNS)를 설정하는 법.
* 아파치 HTTP 서버를 설청하는 법.
* FTP 서버를 설정하는 법.
* Samba를 이용해 Windows(R)용 파일 서버와 프린트 서버를 설정하는 법.
* Network Time Protocol (NTP)를 이용해 시간과 날짜를 동기화하고, 타임 서버를 설정하는 법.
* iSCSI를 설정하는 법.

이 장은 다음의 내용을 알고 있어야 합니다:

* [.filename]#/etc/rc# 스크립트.
* 네트워크 용어.
* 추가적인 타사 소프트웨어를 crossref:ports[ports,Installing Applications: Packages and Ports]에서 설치하는 법.

[[network-inetd]]
== inetd Super-Server

man:inetd[8] 데몬은 많은 서비스에 대한 연결을 관리하기 때문에 슈퍼 서버(Super-Server)라고도 불립니다. 일반적으로 여러 애플리케이션을 시작하는 대신 inetd 서비스만 시작하면 됩니다. inetd으로 관리되는 서비스에 대해 연결이 요청되면, inetd는 대상 프로그램을 결정하고 해당 프로그램에 대한 프로세스를 생성한 다음 해당 프로그램에 소켓을 위임합니다. 사용 부하가 높지 않은 서비스에 inetd를 사용하면 각 데몬을 독립 실행형 모드에서 개별적으로 실행할 때보다 시스템 부하를 줄일 수 있습니다.

inetd는 일반적인 프로세스에 대해 보통 다른 데몬을 생성하지만 chargen, auth, time, echo, discard, daytime과 같이 사소한 프로토콜은 내부적으로 직접 처리합니다.

이 섹션에서는 inetd 구성의 기본 사항을 다룹니다.

[[network-inetd-conf]]
=== 구성 파일

inetd의 구성은 [.filename]#/etc/inetd.conf#를 편집하여 할 수 있습니다. 이 구성 파일의 각 줄은 inetd로 시작할 수 있는 애플리케이션을 나타냅니다. 기본적으로 모든 줄은 주석(`+#+`)으로 시작하는데, 이는 inetd가 어떤 애플리케이션도 수신 대기하고 있지 않음을 의미합니다. 애플리케이션의 연결을 수신 대기하도록 inetd를 구성하려면 해당 애플리케이션의 첫 줄에 있는 `+#+`를 제거하세요.

이 파일을 편집하고 저장한 후, 시스템 부팅시 inetd가 시작되도록 [.filename]#/etc/rc.conf#을 설정합니다:

[.programlisting]
....
inetd_enable="YES"
....

구성한 서비스를 수신 대기하도록 지금 inetd를 시작하려면 다음과 같이 입력합니다:

[source, shell]
....
# service inetd start
....

일단 inetd가 시작되면 [.filename]#/etc/inetd.conf#를 수정할 때마다 알림을 받게 됩니다:

[[network-inetd-reread]]
.inetd 구성 파일을 리로딩 하기
[example]
====

[source, shell]
....
# service inetd reload
....

====

일반적으로 애플리케이션의 기본 항목은 `+#+`을 제거하는 것 외에는 편집할 필요가 없습니다. 하지만 어떤 경우에는 기본 항목을 편집하는 것이 적절할 수 있습니다.

예를 들어, 다음은 IPv4를 사용하는 man:ftpd[8]의 기본 항목입니다:

[.programlisting]
....
ftp     stream  tcp     nowait  root    /usr/libexec/ftpd       ftpd -l
....

항목의 7개 열은 다음과 같습니다:

[.programlisting]
....
service-name
socket-type
protocol
{wait|nowait}[/max-child[/max-connections-per-ip-per-minute[/max-child-per-ip]]]
user[:group][/login-class]
server-program
server-program-arguments
....

where:

service-name::
시작할 데몬의 서비스 이름입니다. 이 이름은 [.filename]#/etc/services#에 나열된 서비스와 일치해야 합니다. 이 값에 따라 해당 서비스로 들어오는 연결을 수신 대기할 포트가 결정됩니다. 사용자 지정 서비스를 사용하는 경우 먼저 [.filename]#/etc/services#에 추가해야 합니다.

socket-type::
`stream`, `dgram`, `raw`, 이나 `seqpacket`중 하나입니다. UDP 서비스를 할 때에는 `dgram`을, TCP 연결에 대해서는 `stream`을 사용합니다.

protocol::
다음 프로토콜 이름중 하나를 사용하세요:
+
[.informaltable]
[cols="1,1", frame="none", options="header"]
|===
| Protocol Name
| Explanation


|tcp or tcp4
|TCP IPv4

|udp or udp4
|UDP IPv4

|tcp6
|TCP IPv6

|udp6
|UDP IPv6

|tcp46
|Both TCP IPv4 and IPv6

|udp46
|Both UDP IPv4 and IPv6
|===

{wait|nowait}[/max-child[/max-connections-per-ip-per-minute[/max-child-per-ip]]]:: 이 필드에는 `wait` 또는 `nowait`을 지정해야 합니다. `max-child`, `max-connections-per-ip-per-minute` 및 `max-child-per-ip` 는 선택 사항입니다.
+
`wait|nowait` 는 서비스가 자체 소켓을 처리할 수 있는지 여부를 나타냅니다. `dgram` 소켓 유형은 `wait`을 사용해야 하며, 일반적으로 멀티스레드인 `stream` 데몬은 `nowait`을 사용해야 합니다. `wait`는 일반적으로 여러 소켓을 단일 데몬에 넘겨주는 반면, `nowait`은 새 소켓마다 자식 데몬을 생성합니다.
+
inetd가 생성할 수 있는 자식 데몬의 최대 개수는 `max-child`로 설정합니다. 예를 들어, 데몬의 인스턴스를 10개로 제한하려면 `nowait` 뒤에 `/10`을 넣습니다. `/0`을 지정하면 자식 수를 무제한으로 허용합니다.
+
`max-connections-per-ip-per-minute`는 특정 IP 주소로부터의 분당 연결 수를 제한합니다. 이 제한에 도달하면 해당 IP 주소로부터의 추가 연결은 해당 시간(분)이 끝날 때까지 끊어집니다. 예를 들어 `/10` 값은 특정 IP 주소의 연결 시도를 분당 10회로 제한합니다. `max-child-per-ip`는 언제든 단일 IP 주소에서 시작할 수 있는 최대 자식 프로세스의 수를 제한합니다. 이러한 옵션은 과도한 리소스 소비를 제한하고 서비스 거부 공격을 방지하는 데 도움이 될 수 있습니다.
+
man:fingerd[8]의 기본 설정에서 예시를 확인할 수 있습니다:
+
[.programlisting]
....
finger stream  tcp     nowait/3/10 nobody /usr/libexec/fingerd fingerd -k -s
....

user::
데몬이 실행할 사용자 이름입니다. 데몬은 일반적으로 `root`, `daemon`, 또는 `nobody`로 실행됩니다.

server-program::
데몬의 전체 경로입니다. 데몬이 내부적으로 inetd에서 제공하는 서비스인 경우 `internal`을 사용합니다.

server-program-arguments::
호출 시 데몬에 전달할 명령 인수를 지정하는 데 사용됩니다. 데몬이 내부 서비스인 경우 `internal`을 사용합니다.

[[network-inetd-cmdline]]
=== 명령줄 옵션

대부분의 서버 데몬과 마찬가지로 inetd에는 동작을 조정하는 데 사용할 수 있는 여러 옵션이 있습니다. 기본적으로 inetd는 `-wW -C 60`으로 시작됩니다. 이 옵션은 내부 서비스를 포함한 모든 서비스에 대해 TCP 래퍼를 활성화하고 어떤 IP 주소도 분당 60회 이상 서비스를 요청하지 못하도록 합니다.

inetd에 전달되는 기본 옵션을 변경하려면 [.filename]#/etc/rc.conf#에 `inetd_flags` 항목을 추가합니다. inetd이 이미 실행 중이면 `service inetd restart`로 다시 시작합니다.

사용 가능한 제한 옵션은 다음과 같습니다:

-c maximum::
각 서비스의 기본 최대 동시 호출 수를 지정하며, 기본값은 무제한입니다. 서비스별로 [.filename]#/etc/inetd.conf#에서 `max-child`를 사용하여 재정의할 수 있습니다.

-C rate::
단일 IP 주소에서 분당 서비스를 호출할 수 있는 기본 최대 횟수를 지정합니다. [.filename]#/etc/inetd.conf#에서 `max-connections-per-ip-per-minute`를 사용하여 서비스별로 재정의할 수 있습니다.

-R rate::
1분 동안 서비스를 호출할 수 있는 최대 횟수를 지정하며, 기본값은 `256`입니다. `0`으로 설정하면 횟수에 제한이 없습니다.

-s maximum::
단일 IP 주소에서 한 번에 서비스를 호출할 수 있는 최대 횟수를 지정하며, 기본값은 무제한입니다. [.filename]#/etc/inetd.conf#에서 `max-child-per-ip`를 사용하여 서비스별로 재정의할 수 있습니다.

이외에도 다양한 추가 옵션을 사용할 수 있습니다. 전체 옵션 목록은 man:inetd[8]을 참조하세요.

[[network-inetd-security]]
=== 보안 고려사항

inetd로 관리할 수 있는 많은 데몬은 보안을 고려하지 않습니다. fingerd와 같은 일부 데몬은 공격자에게 유용할 수 있는 정보를 제공할 수 있습니다. 필요한 서비스만 활성화하고 과도한 연결 시도가 있는지 시스템을 모니터링하세요. `max-connections-per-ip-per-minute`, `max-child` 와 `max-child-per-ip`를 사용하여 이러한 공격을 제한할 수 있습니다.

기본적으로 TCP 래퍼는 활성화되어 있습니다. 다양한 inetd 호출 데몬에 TCP 제한을 설정하는 방법에 대한 자세한 내용은 man:hosts_access[5]를 참조하세요.

[[network-nfs]]
== 네트워크 파일 시스템 (NFS)

FreeBSD는 서버가 네트워크를 통해 클라이언트와 디렉터리 및 파일을 공유할 수 있는 네트워크 파일 시스템(NFS)을 지원합니다. NFS를 사용하면 사용자와 프로그램은 원격 시스템의 파일에 마치 로컬에 저장된 것처럼 액세스할 수 있습니다.

NFS는 실용적인 용도로 많이 사용됩니다. 몇 가지 일반적인 용도는 다음과 같습니다:

* 각 클라이언트에서 중복될 수 있는 데이터를 단일 위치에 보관하고 네트워크의 클라이언트가 액세스할 수 있습니다.
* 여러 클라이언트가 [.filename]#/usr/ports/distfiles# 디렉터리에 액세스해야 할 수 있습니다. 해당 디렉터리를 공유하면 각 클라이언트에 다운로드할 필요 없이 소스 파일에 빠르게 액세스할 수 있습니다.
* 대규모 네트워크에서는 모든 사용자의 홈 디렉터리가 저장되는 중앙 NFS 서버를 구성하는 것이 더 편리한 경우가 많습니다. 사용자는 네트워크의 어느 곳에서나 클라이언트에 로그인하여 자신의 홈 디렉터리에 액세스할 수 있습니다.
* NFS 내보내기 관리가 간소화됩니다. 예를 들어 보안 또는 백업 정책을 설정해야 하는 파일 시스템을 단 하나로 줄일 수 있습니다.
* 이동식 미디어 저장 장치는 네트워크의 다른 컴퓨터에서 사용할 수 있습니다. 이렇게 하면 네트워크 전체에서 디바이스 수가 줄어들고 보안을 관리할 수 있는 중앙 집중형 장소를 구성할 수 있습니다. 또한 중앙 집중식 설치 미디어에서 여러 머신에 소프트웨어를 설치하는 것이 더 편리한 경우가 많습니다.

NFS는 서버와 하나 이상의 클라이언트로 구성됩니다. 클라이언트는 서버 머신에 저장된 데이터에 원격으로 액세스합니다. 이것이 제대로 작동하려면 몇 가지 프로세스를 구성하고 실행해야 합니다.

다음의 데몬은 서버에서 반드시 실행 중이어야 합니다:

[.informaltable]
[cols="1,1", frame="none", options="header"]
|===
| Daemon
| Description


|nfsd
|The NFS daemon which services requests from NFS clients.

|mountd
|The NFS mount daemon which carries out requests received from nfsd.

|rpcbind
| This daemon allows NFS clients to discover which port the NFS server is using.
|===

클라이언트에서 man:nfsiod[8]를 실행하면 성능을 향상시킬 수 있지만 필수는 아닙니다.

[[network-configuring-nfs]]
=== 서버 구성하기

NFS 서버가 공유할 파일 시스템은 [.filename]#/etc/exports#에 지정됩니다. 이 파일의 각 줄은 내보낼 파일 시스템, 해당 파일 시스템에 액세스할 수 있는 클라이언트 및 모든 액세스 옵션을 지정합니다. 이 파일에 항목을 추가할 때는 내보낸 각 파일 시스템, 해당 속성 및 허용된 호스트가 한 줄에 포함되어야 합니다. 항목에 특정 클라이언트가 표시되어 있지 않으면 네트워크의 모든 클라이언트가 해당 파일 시스템을 마운트할 수 있습니다.

다음 [.filename]#/etc/exports# 항목은 파일 시스템을 내보내는 방법을 보여줍니다. 이 예제는 여러분 네트워크의 파일 시스템 및 클라이언트 이름과 일치하도록 수정해 사용할 수 있습니다. 이 파일에 사용할 수 있는 옵션은 많지만 여기서는 몇 가지 옵션만 언급합니다. 전체 옵션 목록은 man:exports[5]를 참조하세요.

이 예는 _alpha_, _bravo_, _charlie_라는 세 개의 호스트로 [.filename]#/cdrom#을 내보내는 방법을 보여 줍니다:

[.programlisting]
....
/cdrom -ro alpha bravo charlie
....

`-ro` 플래그를 사용하면 파일 시스템을 읽기 전용으로 설정하여 클라이언트가 내보낸 파일 시스템을 변경할 수 없게 합니다. 이 예에서는 호스트 이름이 DNS 또는 [.filename]#/etc/hosts#에 있다고 가정합니다. 네트워크에 DNS 서버가 없는 경우 man:hosts[5]를 참조하세요.

다음 예는 [.filename]#/home#을 IP 주소별로 세 명의 클라이언트로 내보냅니다. 이 방법은 DNS 또는 [.filename]#/etc/hosts# 항목이 없는 네트워크에 유용할 수 있습니다. `-alldirs` 플래그를 사용하면 하위 디렉터리가 마운트 지점이 될 수 있습니다. 즉, 하위 디렉터리를 자동으로 마운트하지는 않지만 클라이언트가 필요에 따라 필요한 디렉터리를 마운트할 수 있도록 허용합니다.

[.programlisting]
....
/usr/home  -alldirs  10.0.0.2 10.0.0.3 10.0.0.4
....

다음 예제는 서로 다른 도메인의 두 클라이언트가 해당 파일 시스템에 액세스할 수 있도록 [.filename]#/a#을 내보냅니다. `-maproot=root`를 사용하면 원격 시스템의 `root`가 내보낸 파일 시스템에 `root`로 데이터를 쓸 수 있습니다. `-maproot=root`를 지정하지 않으면 클라이언트의 `root` 사용자가 서버의 `nobody` 계정에 매핑되고 `nobody`에 정의된 액세스 제한이 적용됩니다.

[.programlisting]
....
/a  -maproot=root  host.example.com box.example.org
....

클라이언트는 파일 시스템당 한 번만 지정할 수 있습니다. 예를 들어 [.filename]#/usr#이 단일 파일 시스템인 경우 두 항목이 모두 동일한 호스트를 지정하므로 다음 항목은 유효하지 않습니다:

[.programlisting]
....
# Invalid when /usr is one file system
/usr/src   client
/usr/ports client
....

이 상황에 맞는 올바른 형식은 하나의 항목을 사용하는 것입니다:

[.programlisting]
....
/usr/src /usr/ports  client
....

다음은 유효한 내보내기 목록의 예입니다. 여기서 [.filename]#/usr# 및 [.filename]#/exports#는 로컬 파일 시스템입니다:

[.programlisting]
....
# Export src and ports to client01 and client02, but only
# client01 has root privileges on it
/usr/src /usr/ports -maproot=root    client01
/usr/src /usr/ports               client02
# The client machines have root and can mount anywhere
# on /exports. Anyone in the world can mount /exports/obj read-only
/exports -alldirs -maproot=root      client01 client02
/exports/obj -ro
....

부팅 시 NFS 서버에 필요한 프로세스를 활성화하려면 [.filename]#/etc/rc.conf#에 다음의 옵션을 추가합니다:

[.programlisting]
....
rpcbind_enable="YES"
nfs_server_enable="YES"
mountd_enable="YES"
....

이 명령을 실행하여 서버를 지금 시작할 수 있습니다:

[source, shell]
....
# service nfsd start
....

NFS 서버가 시작될 때마다 mountd도 자동으로 시작됩니다. 그러나 mountd는 시작될 때 [.filename]#/etc/exports#만 읽습니다. 이후의 [.filename]#/etc/exports# 편집 내용을 즉시 적용하려면 mountd가 해당 파일을 다시 읽도록 강제 실행합니다:

[source, shell]
....
# service mountd reload
....

NFS 버전 4 설정에 대한 설명은 man:nfsv4[4]를 참조하세요.

=== 클라이언트 구성하기

NFS 클라이언트를 사용하려면 각 클라이언트의 [.filename]#/etc/rc.conf#에서 다음 옵션을 설정하세요:

[.programlisting]
....
nfs_client_enable="YES"
....

그런 다음 각각의 NFS 클라이언트에서 다음 명령을 실행합니다:

[source, shell]
....
# service nfsclient start
....

이제 클라이언트는 원격 파일 시스템을 마운트하는 데 필요한 모든 것을 갖추었습니다. 이 예제에서 서버의 이름은 `server`이고 클라이언트의 이름은 `client`입니다. `server`의 [.filename]#/home#을 `client`의 [.filename]#/mnt# 마운트 지점에 마운트하려면 다음과 같이 하세요:

[source, shell]
....
# mount server:/home /mnt
....

이제 [.filename]#/home#에 있는 파일과 디렉터리는 `client`의 [.filename]#/mnt# 디렉터리에서 사용할 수 있습니다.

클라이언트가 부팅할 때마다 원격 파일 시스템을 마운트하려면 [.filename]#/etc/fstab#에 추가합니다:

[.programlisting]
....
server:/home	/mnt	nfs	rw	0	0
....

사용 가능한 모든 옵션에 대한 설명은 man:fstab[5]을 참조하세요.

=== 잠금

일부 애플리케이션이 제대로 작동하려면 파일 잠금이 필요합니다. 잠금을 사용하려면 클라이언트와 서버 모두에서 [.filename]#/etc/rc.conf#에 다음 줄을 추가하세요:

[.programlisting]
....
rpc_lockd_enable="YES"
rpc_statd_enable="YES"
....

그리고나서 애플리케이션을 시작하세요:

[source, shell]
....
# service lockd start
# service statd start
....

서버에서 잠금이 필요하지 않은 경우, 마운트를 실행할 때 `-L`을 포함하여 로컬에서 잠그도록 NFS 클라이언트를 구성할 수 있습니다. 자세한 내용은 man:mount_nfs[8]을 참고하세요.

[[network-autofs]]
=== man:autofs[5]로 마운트 자동화하기

[NOTE]
====
man:autofs[5] 오토마운트 기능은 FreeBSD 10.1-RELEASE부터 지원됩니다. 이전 버전의 FreeBSD에서 오토마운터 기능을 사용하려면, man:amd[8]를 사용하십시오. 이 장에서는 man:autofs[5] 오토마운터에 대해서만 설명합니다.
====

man:autofs[5] 기능은 파일 시스템 내의 파일이나 디렉터리에 액세스할 때마다 원격 및 로컬 파일 시스템을 자동으로 마운트할 수 있도록 하는 여러 구성 요소의 공통 이름입니다. 이것은 커널 구성 요소인 man:autofs[5]와 여러 사용자 공간 응용 프로그램인 man:automount[8], man:automountd[8] 및 man:autounmountd[8]로 구성되어 있습니다. 이는 이전 FreeBSD 릴리스의 man:amd[8]를 대체하는 역할을 합니다. man:amd[8]는 이전 버전과의 호환성을 위해 여전히 제공되지만, 둘은 서로 다른 맵 형식을 사용하며 autofs에서 사용하는 맵 형식은 Solaris, MacOS X 및 Linux와 같은 다른 SVR4 오토마운터와 동일합니다.

man:autofs[5] 가상 파일 시스템은 보통 부팅 중에 호출되는 man:automount[8]에 의해 지정된 마운팅 포인트에 마운트됩니다.

프로세스가 man:autofs[5] 마운팅 포인트 내의 파일에 액세스하려고 시도할 때마다 커널은 man:automountd[8] 데몬에게 알리고 트리거 프로세스를 일시 중지합니다. man:automountd[8] 데몬은 적절한 맵을 찾고 그에 따라 파일 시스템을 마운트하여 커널 요청을 처리한 다음 커널에 차단된 프로세스를 해제하라는 신호를 보냅니다. man:autounmountd[8] 데몬은 자동 마운트된 파일시스템이 아직 사용 중이 아닌 경우 일정 시간이 지나면 자동으로 마운트를 해제합니다.

man:autofs[5]의 기본 구성 파일은 [.filename]#/etc/auto_master#입니다. 이 파일은 개별 맵을 최상위 마운트에 할당합니다. [.filename]#auto_master# 및 맵 구문에 대한 설명은 man:auto_master[5]를 참조하세요.

[.filename]#/net#에 마운트된 특수 자동 마운트 맵이 있습니다. 이 디렉터리 내에서 파일에 액세스하면 man:autofs[5]가 해당 원격 마운트를 조회하여 자동으로 마운트합니다. 예를 들어, [.filename]#/net/foobar/usr# 내의 파일에 액세스하려는 시도는 man:automountd[8]에게 호스트 `foobar`에서 내보낸 [.filename]#/usr#를 마운트하도록 지시합니다.

.man:autofs[5]로 마운팅 내보내기
[example]
====
이 예제에서 `showmount -e`는 NFS 서버에 마운트 할 수 있는 파일 시스템을 내보내기를 보여줍니다:

[source, shell]
....
% showmount -e foobar
Exports list on foobar:
/usr                               10.10.10.0
/a                                 10.10.10.0
% cd /net/foobar/usr
....

====

`showmount`의 출력은 내보내기로 [.filename]#/usr#을 표시합니다. 디렉터리를 [.filename]#/host/foobar/usr#로 변경할 때, man:automountd[8]는 요청을 가로채서 호스트 이름 `foobar`를 확인하려고 시도합니다. 성공하면 man:automountd[8]가 자동으로 소스 내보내기를 마운트합니다.

부팅 시 man:autofs[5]를 활성화하려면 [.filename]#/etc/rc.conf#에 다음 줄을 추가하세요:

[.programlisting]
....
autofs_enable="YES"
....

그런 다음 man:autofs[5]를 실행하여 시작할 수 있습니다:

[source, shell]
....
# service automount start
# service automountd start
# service autounmountd start
....

man:autofs[5] 맵 형식은 다른 운영 체제에서와 동일합니다. 이 형식에 대한 정보는 http://web.archive.org/web/20160813071113/http://images.apple.com/business/docs/Autofs.pdf[Mac OS X document]와 같은 다른 출처의 정보가 유용할 수 있습니다.

자세한 내용은 man:automount[8], man:automountd[8], man:autounmountd[8], man:auto_master[5] 매뉴얼 페이지를 참조하세요.

[[network-nis]]
== 네트워크 정보 시스템 (NIS)

NIS(네트워크 정보 시스템)는 Solaris(TM), HP-UX, AIX(R), Linux, NetBSD, OpenBSD, FreeBSD 등 UNIX(R) 계열 시스템의 관리를 중앙 집중화하도록 설계되었습니다. NIS는 원래 Yellow Pages로 알려졌지만 상표권 문제로 인해 이름이 변경되었습니다. 이것이 NIS 명령이 `yp`로 시작하는 이유입니다.

NIS는 NIS 도메인 내의 컴퓨터 그룹이 공통 구성 파일 집합을 공유할 수 있도록 하는 RPC(원격 프로시저 호출) 기반 클라이언트/서버 시스템입니다. 이를 통해 시스템 관리자는 최소한의 구성 데이터만으로 NIS 클라이언트 시스템을 설정하고 단일 위치에서 구성 데이터를 추가, 제거 또는 수정할 수 있습니다.

FreeBSD는 NIS 프로토콜 버전 2를 사용합니다.

=== NIS 약관 및 프로세스

표 28.1은 NIS에서 사용하는 용어와 주요 프로세스를 요약한 것입니다:

.NIS 용어
[cols="1,1", frame="none", options="header"]
|===
| Term
| Description

|NIS domain name
|NIS servers and clients share an NIS domain name. Typically, this name does not have anything to do with DNS.

|man:rpcbind[8]
|This service enables RPC and must be running in order to run an NIS server or act as an NIS client.

|man:ypbind[8]
|This service binds an NIS client to its NIS server. It will take the NIS domain name and use RPC to connect to the server. It is the core of client/server communication in an NIS environment. If this service is not running on a client machine, it will not be able to access the NIS server.

|man:ypserv[8]
|This is the process for the NIS server. If this service stops running, the server will no longer be able to respond to NIS requests so hopefully, there is a slave server to take over. Some non-FreeBSD clients will not try to reconnect using a slave server and the ypbind process may need to be restarted on these clients.

|man:rpc.yppasswdd[8]
|This process only runs on NIS master servers. This daemon allows NIS clients to change their NIS passwords. If this daemon is not running, users will have to login to the NIS master server and change their passwords there.
|===

=== 장비 유형

NIS 환경에는 세 가지 유형의 호스트가 있습니다:

* NIS master server
+
이 서버는 호스트 구성 정보의 중앙 저장소 역할을 하며 모든 NIS 클라이언트에서 사용하는 권한에 대한 파일 사본을 유지 관리합니다. [.filename]#passwd#, [.filename]#group# 및 NIS 클라이언트에서 사용하는 기타 다양한 파일은 마스터 서버에 저장됩니다. 하나의 컴퓨터가 둘 이상의 NIS 도메인에 대한 NIS 마스터 서버가 될 수 있지만 이러한 유형의 구성은 비교적 작은 규모의 NIS 환경을 가정하므로 이 장에서는 다루지 않습니다.
* NIS slave servers
+
NIS 슬레이브 서버는 중복성을 제공하기 위해 NIS 마스터 데이터 파일의 복사본을 유지합니다. 또한 슬레이브 서버는 NIS 클라이언트가 항상 먼저 응답하는 NIS 서버에 연결하므로 마스터 서버의 부하를 분산하는 데 도움이 됩니다.
* NIS clients
+
NIS 클라이언트는 로그온하는 동안 NIS 서버에 대해 인증을 진행합니다.

많은 파일에 있는 정보는 NIS를 사용하여 공유할 수 있습니다. [.filename]#master.passwd#, [.filename]#group# 및 [.filename]#hosts# 파일은 일반적으로 NIS를 통해 공유됩니다. 클라이언트의 프로세스가 로컬에서 일반적으로 이러한 파일에 있는 정보가 필요할 때마다 대신 바인딩된 NIS 서버에 쿼리를 수행합니다.

=== 계획시 고려 사항

이 섹션에서는 중앙 집중식 관리 지점이 없는 15대의 FreeBSD 머신으로 구성된 샘플 NIS 환경에 대해 설명합니다. 각 머신에는 고유한 [.filename]#/etc/passwd# 및 [.filename]#/etc/master.passwd#가 있습니다. 이러한 파일은 수동 개입을 통해서만 서로 동기화 상태로 유지됩니다. 현재는 실습에 사용자를 추가할 때 15대의 모든 머신에서 이 과정을 반복해야 합니다.

실습의 구성은 다음과 같습니다:

[.informaltable]
[cols="1,1,1", frame="none", options="header"]
|===
| Machine name
| IP address
| Machine role


|`ellington`
|`10.0.0.2`
|NIS master

|`coltrane`
|`10.0.0.3`
|NIS slave

|`basie`
|`10.0.0.4`
|Faculty workstation

|`bird`
|`10.0.0.5`
|Client machine

|`cli[1-11]`
|`10.0.0.[6-17]`
|Other client machines
|===

NIS 체계를 처음 개발하는 경우, 사전에 철저히 계획해야 합니다. 네트워크 규모에 관계없이 계획 프로세스의 일부로 몇 가지 결정을 내려야 합니다.

==== NIS 도메인 이름 선택하기

클라이언트가 정보 요청을 브로드캐스트할 때 해당 클라이언트가 속한 NIS 도메인의 이름이 포함됩니다. 이를 통해 하나의 네트워크에 있는 여러 서버중 어떤 서버가 요청에 응답해야 하는지 알 수 있습니다. NIS 도메인 이름은 호스트 그룹의 이름이라고 생각하면 됩니다.

일부 조직에서는 인터넷 도메인 이름을 NIS 도메인 이름으로 사용하기도 합니다. 이는 네트워크 문제를 디버깅할 때 혼동을 일으킬 수 있으므로 권장하지 않습니다. NIS 도메인 이름은 네트워크 내에서 고유해야 하며, 해당 도메인 이름이 나타내는 컴퓨터 그룹을 설명하는 것이 도움이 됩니다. 예를 들어, Acme Inc.의 미술 부서는 “acme-art” NIS 도메인에 있는 것이 좋습니다. 이 예에서는 도메인 이름 ‘test-domain’을 사용합니다.

그러나 일부 비FreeBSD 운영 체제에서는 NIS 도메인 이름이 인터넷 도메인 이름과 동일해야 합니다. 네트워크에 있는 하나 이상의 컴퓨터에 이 제한이 있는 경우, 인터넷 도메인 이름을 NIS 도메인 이름으로 _필수_ 사용해야 합니다.

==== 물리서버 요구사항

NIS 서버로 사용할 컴퓨터를 선택할 때 염두에 두어야 할 몇 가지 사항이 있습니다. NIS 클라이언트는 서버의 가용성에 따라 달라지므로 자주 재부팅하지 않는 컴퓨터를 선택하세요. NIS 서버는 NIS 서버로만 사용되는 독립 실행형 컴퓨터가 이상적입니다. 네트워크 사용량이 많지 않은 경우 다른 서비스를 실행하는 컴퓨터에 NIS 서버를 설치해도 됩니다. 그러나 NIS 서버를 사용할 수 없게 되면 모든 NIS 클라이언트에 부정적인 영향을 미칩니다.

=== NIS 마스터 서버 구성하기

모든 NIS 파일의 정식 사본은 마스터 서버에 저장됩니다. 정보를 저장하는 데 사용되는 데이터베이스를 NIS 맵이라고 합니다. FreeBSD에서 이러한 맵은 [.filename]#/var/yp/[domainname]#에 저장되며, 여기서 [.filename]#[domainname]#은 NIS 도메인의 이름입니다. 여러 도메인이 지원되므로 각 도메인마다 하나씩 여러 개의 디렉터리를 가질 수 있습니다. 각 도메인에는 고유한 독립적인 맵 세트가 있습니다.

NIS 마스터 및 슬레이브 서버는 man:ypserv[8]를 통해 모든 NIS 요청을 처리합니다. 이 데몬은 NIS 클라이언트로부터 들어오는 요청을 수신하고, 요청된 도메인 및 맵 이름을 해당 데이터베이스 파일의 경로로 변환하고, 데이터베이스의 데이터를 클라이언트로 다시 전송하는 작업을 담당합니다.

마스터 NIS 서버를 설정하는 것은 환경에 따라 비교적 간단할 수 있습니다. FreeBSD는 기본적으로 NIS 지원을 제공하므로 [.filename]#/etc/rc.conf#에 다음 줄을 추가하여 활성화하기만 하면 됩니다:

[.programlisting]
....
nisdomainname="test-domain"	<.>
nis_server_enable="YES"		<.>
nis_yppasswdd_enable="YES"	<.>
....

<.> 이 줄은 NIS 도메인 이름을 `test-domain`으로 설정합니다.
<.> 이것은 시스템이 부팅될 때 NIS 서버 프로세스가 자동으로 시작되도록 합니다.
<.> 이것은 사용자가 클라이언트 컴퓨터에서 NIS 암호를 변경할 수 있도록 man:rpc.yppasswdd[8] 데몬을 활성화합니다.

서버 컴퓨터가 NIS 클라이언트이기도 한 다중 서버 도메인에서는 주의를 기울여야 합니다. 일반적으로 서버가 바인딩 요청을 브로드캐스트하여 서로 바인딩될 수 있도록 허용하는 것보다 서버가 스스로 바인딩하도록 하는 것이 좋습니다. 한 서버가 다운되고 다른 서버가 그 서버에 종속되어 있는 경우 알 수 없는 장애 모드가 발생할 수 있습니다. 결국 모든 클라이언트가 시간이 초과되어 다른 서버에 바인딩을 시도하지만 지연 시간이 상당할 수 있으며 서버가 다시 서로 바인딩할 수 있으므로 장애 모드가 계속 발생할 수 있습니다.

클라이언트이기도 한 서버는 [.filename]#/etc/rc.conf#에 이러한 추가 줄을 추가하여 특정 서버에 강제로 바인딩할 수 있습니다:

[.programlisting]
....
nis_client_enable="YES"				<.>
nis_client_flags="-S test-domain,server"	<.>
....

<.> 클라이언트 작업도 실행할 수 있게 합니다.
<.> NIS 도메인 이름을 `test-domain`으로 설정하고 이 도메인에 바인딩합니다.

편집 내용을 저장한 후 `/etc/netstart`를 입력하여 네트워크를 다시 시작하고 [.filename]#/etc/rc.conf#에 정의된 값을 적용합니다. NIS 맵을 초기화하기 전에 man:ypserv[8]를 시작합니다:

[source, shell]
....
# service ypserv start
....


==== NIS 맵 시작하기

NIS 맵은 한 가지 예외를 제외하고 NIS 마스터의 [.filename]#/etc#에 있는 구성 파일에서 생성됩니다: [.filename]#/etc/master.passwd#. 이는 NIS 도메인의 모든 서버에 비밀번호가 전파되는 것을 방지하기 위한 것입니다. 따라서 NIS 맵을 초기화하기 전에 기본 비밀번호 파일을 구성하세요:

[source, shell]
....
# cp /etc/master.passwd /var/yp/master.passwd
# cd /var/yp
# vi master.passwd
....

시스템 계정에 대한 모든 항목과 `root` 및 기타 관리 계정과 같이, NIS 클라이언트에 전파할 필요가 없는 사용자 계정에 대한 모든 항목은 제거하는 것이 좋습니다.

[NOTE]
====
권한을 `600`으로 설정하여 [.filename]#/var/yp/master.passwd#이 그룹 또는 월드 모두에서 읽을 수 없는지 확인합니다.
====

이 작업을 완료한 후, NIS 맵을 초기화합니다. FreeBSD에는 이 작업을 수행할 수 있는 man:ypinit[8] 스크립트가 포함되어 있습니다. 마스터 서버용 맵을 생성할 때 `-m`을 포함하고 NIS 도메인 이름을 지정합니다:

[source, shell]
....
ellington# ypinit -m test-domain
Server Type: MASTER Domain: test-domain
Creating an YP server will require that you answer a few questions.
Questions will all be asked at the beginning of the procedure.
Do you want this procedure to quit on non-fatal errors? [y/n: n] n
Ok, please remember to go back and redo manually whatever fails.
If not, something might not work.
At this point, we have to construct a list of this domains YP servers.
rod.darktech.org is already known as master server.
Please continue to add any slave servers, one per line. When you are
done with the list, type a <control D>.
master server   :  ellington
next host to add:  coltrane
next host to add:  ^D
The current list of NIS servers looks like this:
ellington
coltrane
Is this correct?  [y/n: y] y

[..output from map generation..]

NIS Map update completed.
ellington has been setup as an YP master server without any errors.
....

이렇게 하면 [.filename]#/var/yp/Makefile.dist#에서 [.filename]#/var/yp/Makefile#가 생성됩니다. 기본적으로 이 파일은 네트워크에 FreeBSD 클라이언트만 있는 단일 NIS 서버가 있다고 가정합니다. `test-domain`에는 슬레이브 서버가 있으므로, 이 줄이 주석(`+#+`)으로 시작되도록 [.filename]#/var/yp/Makefile#에 이 줄을 편집합니다:

[.programlisting]
....
NOPUSH = "True"
....


==== 신규 사용자 추가하기

새 사용자를 만들 때마다 사용자 계정을 마스터 NIS 서버에 추가하고 NIS 맵을 다시 빌드해야 합니다. 이 작업이 완료될 때까지 새 사용자는 NIS 마스터를 제외한 다른 곳에서는 로그인할 수 없습니다. 예를 들어, 새 사용자 `jsmith`를 `test-domain` 도메인에 추가하려면 마스터 서버에서 다음 명령을 실행합니다:

[source, shell]
....
# pw useradd jsmith
# cd /var/yp
# make test-domain
....

‘pw useradd smith’ 대신 ‘adduser jsmith’를 사용하여 사용자를 추가할 수도 있습니다.

=== NIS 슬레이브 서버 설정하기

NIS 슬레이브 서버를 설정하려면 슬레이브 서버에 로그온하여 마스터 서버와 마찬가지로 [.filename]#/etc/rc.conf#을 편집합니다. 마스터 서버에 이미 존재하는 NIS 맵은 생성하지 마세요. 슬레이브 서버에서 `ypinit`을 실행할 때는 `-m`(마스터용) 대신 `-s`(슬레이브용)를 사용합니다. 이 옵션을 사용하려면 다음 예에서와 같이 도메인 이름 외에 NIS 마스터의 이름이 필요합니다:

[source, shell]
....
coltrane# ypinit -s ellington test-domain

Server Type: SLAVE Domain: test-domain Master: ellington

Creating an YP server will require that you answer a few questions.
Questions will all be asked at the beginning of the procedure.

Do you want this procedure to quit on non-fatal errors? [y/n: n]  n

Ok, please remember to go back and redo manually whatever fails.
If not, something might not work.
There will be no further questions. The remainder of the procedure
should take a few minutes, to copy the databases from ellington.
Transferring netgroup...
ypxfr: Exiting: Map successfully transferred
Transferring netgroup.byuser...
ypxfr: Exiting: Map successfully transferred
Transferring netgroup.byhost...
ypxfr: Exiting: Map successfully transferred
Transferring master.passwd.byuid...
ypxfr: Exiting: Map successfully transferred
Transferring passwd.byuid...
ypxfr: Exiting: Map successfully transferred
Transferring passwd.byname...
ypxfr: Exiting: Map successfully transferred
Transferring group.bygid...
ypxfr: Exiting: Map successfully transferred
Transferring group.byname...
ypxfr: Exiting: Map successfully transferred
Transferring services.byname...
ypxfr: Exiting: Map successfully transferred
Transferring rpc.bynumber...
ypxfr: Exiting: Map successfully transferred
Transferring rpc.byname...
ypxfr: Exiting: Map successfully transferred
Transferring protocols.byname...
ypxfr: Exiting: Map successfully transferred
Transferring master.passwd.byname...
ypxfr: Exiting: Map successfully transferred
Transferring networks.byname...
ypxfr: Exiting: Map successfully transferred
Transferring networks.byaddr...
ypxfr: Exiting: Map successfully transferred
Transferring netid.byname...
ypxfr: Exiting: Map successfully transferred
Transferring hosts.byaddr...
ypxfr: Exiting: Map successfully transferred
Transferring protocols.bynumber...
ypxfr: Exiting: Map successfully transferred
Transferring ypservers...
ypxfr: Exiting: Map successfully transferred
Transferring hosts.byname...
ypxfr: Exiting: Map successfully transferred

coltrane has been setup as an YP slave server without any errors.
Remember to update map ypservers on ellington.
....

이렇게 하면 슬레이브 서버에 [.filename]#/var/yp/test-domain#이라는 디렉터리가 생성되며, 여기에는 NIS 마스터 서버의 맵 사본이 포함됩니다. 각 슬레이브 서버에 이 [.filename]#/etc/crontab# 항목을 추가하면 슬레이브가 마스터 서버의 맵과 맵을 동기화하게 됩니다:

[.programlisting]
....
20      *       *       *       *       root   /usr/libexec/ypxfr passwd.byname
21      *       *       *       *       root   /usr/libexec/ypxfr passwd.byuid
....

마스터 서버는 모든 맵 변경 사항을 슬레이브 서버에 자동으로 푸시하기 때문에 이러한 항목이 필수는 아닙니다. 하지만 클라이언트가 올바른 비밀번호 정보를 제공하기 위해 슬레이브 서버에 의존할 수 있으므로 비밀번호 맵을 자주 강제로 업데이트하는 것이 좋습니다. 이는 맵 업데이트가 항상 완료되지 않을 수 있는 바쁜 네트워크에서 특히 중요합니다.

구성을 완료하려면 슬레이브 서버에서 `/etc/netstart`를 실행하여 NIS 서비스를 시작합니다.

=== NIS 클라이언트 설정하기

NIS 클라이언트는 man:ypbind[8]를 사용하여 NIS 서버에 바인딩합니다. 이 데몬은 로컬 네트워크에서 RPC 요청을 브로드캐스트합니다. 이러한 요청은 클라이언트에 구성된 도메인 이름을 포함합니다. 동일한 도메인의 NIS 서버가 브로드캐스트 중 하나를 수신하면 서버의 주소를 기록하는 ypbind에 응답합니다. 사용 가능한 서버가 여러 대 있는 경우 클라이언트는 첫 번째 서버의 주소를 사용하여 응답하고 모든 NIS 요청을 해당 서버로 보냅니다. 클라이언트는 서버가 여전히 사용 가능한지 확인하기 위해 정기적으로 서버에 자동으로 핑을 보냅니다. 적절한 시간 내에 응답을 받지 못하면 ypbind는 도메인을 바인딩 해제된 것으로 표시하고 다른 서버를 찾기 위해 다시 브로드캐스팅을 시작합니다.

FreeBSD 머신에 NIS 클라이언트를 구성하려면:

[.procedure]
====
. [.filename]#/etc/rc.conf#을 편집하고 다음 줄을 추가하여 네트워크 시작 중에 NIS 도메인 이름을 설정과 man:ypbind[8]를 시작하세요:
+
[.programlisting]
....
nisdomainname="test-domain"
nis_client_enable="YES"
....

. NIS 서버에서 가능한 모든 비밀번호 항목을 가져오려면 `vipw`를 사용하여 [.filename]#/etc/master.passwd#를 제외한 모든 사용자 계정을 제거합니다. 계정을 제거할 때 로컬 계정이 하나 이상 남아 있어야 하며 이 계정은 `wheel`의 구성원이어야 한다는 점에 유의하세요. NIS에 문제가 있는 경우 이 로컬 계정을 사용하여 원격으로 로그인하고 수퍼유저가 되어 문제를 해결할 수 있습니다. 편집 내용을 저장하기 전에 파일 끝에 다음 줄을 추가합니다:
+
[.programlisting]
....
+:::::::::
....
+
이 줄은 NIS 서버의 비밀번호 매핑에 유효한 계정을 가진 모든 사용자에게 클라이언트의 계정을 제공하도록 클라이언트를 구성합니다. 이 줄을 수정하여 NIS 클라이언트를 구성하는 방법에는 여러 가지가 있습니다. 한 가지 방법은 <<network-netgroups>>에 설명되어 있습니다. 더 자세한 내용은 O’Reilly Media에서 발행한 ‘NFS 및 NIS 관리 (Managing NFS and NIS)’라는 책을 참조하십시오.
. NIS 서버에서 가능한 모든 그룹 항목을 가져오려면 [.filename]#/etc/group#에 다음 내용을 추가하세요:
+
[.programlisting]
....
+:*::
....
====

NIS 클라이언트를 즉시 시작하려면 수퍼유저로 다음 명령을 실행합니다:

[source, shell]
....
# /etc/netstart
# service ypbind start
....

이 단계를 완료한 후 클라이언트에서 `ypcat passwd`를 실행하면 서버의 [.filename]#passwd# 맵이 표시됩니다.

=== NIS 보안

RPC는 브로드캐스트 기반 서비스이기 때문에, 같은 도메인 내에서 ypbind를 실행하는 모든 시스템은 NIS 맵의 내용을 검색할 수 있습니다. 무단 트랜잭션을 방지하기 위해, man:ypserv[8]는 특정 호스트 집합에 대한 액세스를 제한하는 데 사용할 수 있는 “securenets”이라는 기능을 지원합니다. 기본적으로 이 정보는 man:ypserv[8]이 `-p`와 대체 경로로 시작하지 않는 한 [.filename]#/var/yp/securenets#에 저장됩니다. 이 파일에는 공백으로 구분된 네트워크 사양과 네트워크 마스크로 구성된 항목이 포함되어 있습니다. “+”#”+`로 시작하는 줄은 주석으로 간주됩니다. 샘플 [.filename]#securenets#은 다음과 같이 보일 수 있습니다:

[.programlisting]
....
# allow connections from local host -- mandatory
127.0.0.1     255.255.255.255
# allow connections from any host
# on the 192.168.128.0 network
192.168.128.0 255.255.255.0
# allow connections from any host
# between 10.0.0.0 to 10.0.15.255
# this includes the machines in the testlab
10.0.0.0      255.255.240.0
....

man:ypserv[8]는 이 규칙 중 하나와 일치하는 주소에서 요청을 받으면 요청을 정상적으로 처리합니다. 하지만 주소가 규칙과 일치하지 않으면 요청이 무시되고 경고 메시지가 기록됩니다. [.filename]#securenets#이 존재하지 않는 경우, `ypserv`는 모든 호스트의 연결을 허용합니다.

crossref:security[tcpwrappers,”TCP Wrapper”]는 [.filename]#securenets# 대신 접근 제어를 제공하기 위한 대체 메커니즘입니다. 두 가지 접근 제어 메커니즘 모두 보안을 강화할 수 있지만 “IP spoofing” 공격에 취약합니다. 모든 NIS 관련 트래픽은 방화벽에서 차단해야 합니다.

[.filename]#securenets#을 사용하는 서버는 구식 TCP/IP 구현을 사용하는 정상적인 NIS 클라이언트에 서비스를 제공하지 못할 수 있습니다. 이러한 장비 중 일부는 브로드캐스트 할 때 모든 호스트 비트를 0으로 설정하거나 브로드캐스트 주소를 계산할 때 서브넷 마스크를 확인하지 못할 수 있습니다. 이러한 문제가 발생했을때 일부는 클라이언트 구성을 변경하여 해결할 수 있지만, 이게 불가능할 경우 클라이언트 시스템을 폐기하거나 [.filename]#securenets#을 포기해야 할 수도 있습니다.

TCP 래퍼를 사용하면 NIS 서버의 지연 시간이 증가합니다. 추가 지연은 클라이언트 프로그램에서 시간 초과를 일으킬 만큼 충분히 길 수 있으며, 특히 느린 NIS 서버가 있는 바쁜 네트워크에서는 더욱 그렇습니다. 하나 이상의 클라이언트에 지연이 발생하면 해당 클라이언트를 NIS 슬레이브 서버로 변환하고 강제로 바인딩하세요.

==== 일부 사용자 차단하기

이 예에서 `basie` 시스템은 NIS 도메인 내의 교직원 워크스테이션입니다. 마스터 NIS 서버의 [.filename]#passwd# 맵에는 교직원과 학생의 계정이 모두 포함되어 있습니다. 이 섹션에서는 이 시스템에서 교직원 로그인을 허용하고 학생 로그인을 거부하는 방법을 설명합니다.

지정된 사용자가 NIS 데이터베이스에 있더라도 시스템에 로그온하지 못하도록 하려면 `vipw`를 사용하여 클라이언트에서 [.filename]#/etc/master.passwd# 끝에 정확한 콜론 수로 `-_username_`을 추가하세요. 여기서 _username_은 로그인을 금지할 사용자의 사용자 이름입니다. 차단할 사용자의 목록은 NIS 사용자를 허용하는 `+` 줄 앞에 와야 합니다. 이 예제에서는 `bill`이 `basie`에 로그온하지 못하도록 차단되었습니다:

[source, shell]
....
basie# cat /etc/master.passwd
root:[password]:0:0::0:0:The super-user:/root:/bin/csh
toor:[password]:0:0::0:0:The other super-user:/root:/bin/sh
daemon:*:1:1::0:0:Owner of many system processes:/root:/usr/sbin/nologin
operator:*:2:5::0:0:System &:/:/usr/sbin/nologin
bin:*:3:7::0:0:Binaries Commands and Source,,,:/:/usr/sbin/nologin
tty:*:4:65533::0:0:Tty Sandbox:/:/usr/sbin/nologin
kmem:*:5:65533::0:0:KMem Sandbox:/:/usr/sbin/nologin
games:*:7:13::0:0:Games pseudo-user:/usr/games:/usr/sbin/nologin
news:*:8:8::0:0:News Subsystem:/:/usr/sbin/nologin
man:*:9:9::0:0:Mister Man Pages:/usr/share/man:/usr/sbin/nologin
bind:*:53:53::0:0:Bind Sandbox:/:/usr/sbin/nologin
uucp:*:66:66::0:0:UUCP pseudo-user:/var/spool/uucppublic:/usr/libexec/uucp/uucico
xten:*:67:67::0:0:X-10 daemon:/usr/local/xten:/usr/sbin/nologin
pop:*:68:6::0:0:Post Office Owner:/nonexistent:/usr/sbin/nologin
nobody:*:65534:65534::0:0:Unprivileged user:/nonexistent:/usr/sbin/nologin
-bill:::::::::
+:::::::::

basie#
....


[[network-netgroups]]
=== 넷그룹 사용하기

특정 사용자가 개별 시스템에 로그온하지 못하도록 차단하면 대규모 네트워크에서는 확장할 수 없게 되며, NIS의 주요 이점인 _중앙 집중식_ 관리 기능을 빠르게 잃게 됩니다.

넷그룹은 수백 명의 사용자와 컴퓨터가 있는 대규모의 네트워크를 처리하기 위해 개발되었습니다. 넷그룹의 사용법은 UNIX(R) 그룹과 비슷하지만, 숫자 ID가 없다는 점과 사용자 계정과 다른 넷그룹을 모두 포함하여 넷그룹을 정의할 수 있다는 점이 가장 큰 차이점입니다.

이 장에서 사용된 예제를 확장하기 위해 NIS 도메인을 확장하여 표 28.2 및 28.3에 표시된 사용자 및 시스템을 추가합니다:

.추가 사용자
[cols="1,1", frame="none", options="header"]
|===
| User Name(s)
| Description

|`alpha`, `beta`
|IT department employees

|`charlie`, `delta`
|IT department apprentices

|`echo`, `foxtrott`, `golf`, ...
|employees

|`able`, `baker`, ...
|interns
|===

.추가 시스템
[cols="1,1", frame="none", options="header"]
|===
| Machine Name(s)
| Description

|`war`, `death`, `famine`, `pollution`
|Only IT employees are allowed to log onto these servers.

|`pride`, `greed`, `envy`, `wrath`, `lust`, `sloth`
|All members of the IT department are allowed to login onto these servers.

|`one`, `two`, `three`, `four`, ...
|Ordinary workstations used by employees.

|`trashcan`
|A very old machine without any critical data. Even interns are allowed to use this system.
|===

넷그룹을 사용하여 이 시나리오를 구성하는 경우, 각 사용자는 하나 이상의 넷그룹에 할당되고 넷그룹의 모든 구성원에 대해 로그인이 허용되거나 금지됩니다. 새 머신을 추가할 때는 모든 넷그룹에 대해 로그인 제한을 정의해야 합니다. 새 사용자가 추가되면 해당 계정을 하나 이상의 넷그룹에 추가해야 합니다. NIS 설정을 신중하게 계획한 경우, 중앙 구성 파일 하나만 수정하면 머신에 대한 액세스를 허용하거나 거부할 수 있습니다.

첫 번째 단계는 NIS의 `netgroup` 맵을 초기화하는 것입니다. FreeBSD에서는 이 맵이 자동으로 생성되지 않습니다. NIS 마스터 서버에서 편집기를 사용하여 [.filename]#/var/yp/netgroup#이라는 이름의 맵을 생성합니다.

이 예에서는 IT 직원, IT 견습사원, 임직원 및 인턴을 나타내는 네 개의 넷 그룹을 만듭니다:

[.programlisting]
....
IT_EMP  (,alpha,test-domain)    (,beta,test-domain)
IT_APP  (,charlie,test-domain)  (,delta,test-domain)
USERS   (,echo,test-domain)     (,foxtrott,test-domain) \
        (,golf,test-domain)
INTERNS (,able,test-domain)     (,baker,test-domain)
....

각 항목에 대한 넷그룹을 구성합니다. 항목의 첫 번째 열은 넷그룹의 이름입니다. 괄호 안의 각 집합은 한 명 이상의 사용자로 구성된 그룹 또는 다른 넷그룹의 이름을 나타냅니다. 사용자를 지정할 때 각 그룹 내부의 쉼표로 구분된 세 개의 필드는 사용자를 나타냅니다:

. 사용자를 나타내는 다른 필드가 유효한 호스트의 이름입니다. 호스트 이름을 지정하지 않으면 모든 호스트에서 접근이 허용됩니다.
. 이 넷그룹에 속한 계정의 이름입니다.
. 계정의 NIS 도메인입니다. 다른 NIS 도메인에서 넷그룹으로 계정을 가져올 수 있습니다.

그룹에 여러 명의 사용자가 포함된 경우 각 사용자를 공백으로 구분합니다. 또한 각 필드에는 와일드카드가 포함될 수 있습니다. 자세한 내용은 man:netgroup[5]을 참조하세요.

8자를 초과하는 넷그룹 이름은 사용하지 않아야 합니다. 이름은 대소문자를 구분하며 넷그룹 이름에 대문자를 사용하면 사용자, 컴퓨터 및 넷그룹 이름을 쉽게 구분할 수 있습니다.

일부 비 FreeBSD NIS 클라이언트는 15개 이상의 항목을 포함하는 넷그룹을 처리할 수 없습니다. 이 예에서 보는 것처럼 사용자가 15명 이하인 하위 넷그룹을 여러 개 만들고 이 하위 넷그룹으로 구성된 실제 넷그룹을 만들면 이 제한을 피할 수 있습니다:

[.programlisting]
....
BIGGRP1  (,joe1,domain)  (,joe2,domain)  (,joe3,domain) [...]
BIGGRP2  (,joe16,domain)  (,joe17,domain) [...]
BIGGRP3  (,joe31,domain)  (,joe32,domain)
BIGGROUP  BIGGRP1 BIGGRP2 BIGGRP3
....

단일 넷그룹 내에 225명(15x15명) 이상의 사용자가 있는 경우 이 과정을 반복합니다.

새 NIS 맵을 활성화하고 배포합니다:

[source, shell]
....
ellington# cd /var/yp
ellington# make
....

이렇게 하면 [.filename]#netgroup#, [.filename]#netgroup.byhost# 및 [.filename]#netgroup.byuser#의 세 가지 NIS 맵이 생성됩니다. man:ypcat[1]의 맵 키 옵션을 사용하여 새 NIS 맵을 사용할 수 있는지 확인합니다:

[source, shell]
....
ellington% ypcat -k netgroup
ellington% ypcat -k netgroup.byhost
ellington% ypcat -k netgroup.byuser
....

첫 번째 명령의 출력은 [.filename]#/var/yp/netgroup#의 내용과 비슷해야 합니다. 두 번째 명령은 호스트별 넷그룹이 생성된 경우에만 출력을 생성합니다. 세 번째 명령은 사용자에 대한 넷그룹 목록을 가져오는 데 사용됩니다.

클라이언트를 구성하려면 man:vipw[8]을 사용하여 넷그룹의 이름을 지정합니다. 예를 들어 `war`라는 서버의 경우 이 줄을…:

[.programlisting]
....
+:::::::::
....

다음으로 바꿉니다

[.programlisting]
....
+@IT_EMP:::::::::
....

이는 넷그룹 `IT_EMP`에 정의된 사용자만 이 시스템의 비밀번호 데이터베이스로 가져오고 해당 사용자들만 이 시스템에 로그인할 수 있도록 지정합니다.

이 구성은 셸의 `~` 함수와 사용자 이름과, 숫자 사용자 ID를 변환하는 모든 루틴에도 적용됩니다. 즉, `cd ~_user_`는 동작하지 않고, `ls -l`은 사용자 이름 대신 숫자 ID를 표시하며, `find . -user joe -print`는 `No such user`라는 메시지와 함께 실패할 것입니다. 이 문제를 해결하려면 모든 사용자들이 서버에 로그인하지 않도록 한 후, 사용자 항목을 가져오세요. 이 작업은 다음 명령을 추가하여 수행할 수 있습니다:

[.programlisting]
....
+:::::::::/usr/sbin/nologin
....

이 명령은 클라이언트가 모든 항목을 가져오되 해당 항목의 셸을 [.filename]#/usr/sbin/nologin#으로 바꾸도록 구성합니다.

`+@IT_EMP:::::::::` 뒤에 여분의 줄을 추가해야 합니다. 그렇지 않으면 NIS에서 가져온 모든 사용자 계정의 로그인 셸이 [.filename]#/usr/sbin/nologin#으로 지정되어 아무도 시스템에 로그인할 수 없게 됩니다.

덜 중요한 서버를 구성하려면 서버의 기존 `+:::::::::`을 다음으로 바꿉니다:

[.programlisting]
....
+@IT_EMP:::::::::
+@IT_APP:::::::::
+:::::::::/usr/sbin/nologin
....

워크스테이션에 해당하는 내용은 다음과 같습니다:

[.programlisting]
....
+@IT_EMP:::::::::
+@USERS:::::::::
+:::::::::/usr/sbin/nologin
....

NIS는 사용자 액세스 관련 정책이 변경되는 경우 사용할 수 있는 다른 넷그룹으로부터 넷그룹을 만드는 기능을 지원합니다. 한 가지 방법은 역할 기반 넷그룹을 만드는 것입니다. 예를 들어, 중요한 서버에 대한 로그인 제한을 정의하기 위해 `BIGSRV`라는 넷그룹을 만들고, 덜 중요한 서버에 대해 `SMALLSRV`라는 넷그룹을 만들고, 워크스테이션을 위해 `USERBOX`라는 세 번째 넷그룹을 만들 수 있습니다. 이러한 각 넷그룹에는 해당 머신에 로그인할 수 있는 넷그룹이 포함되어 있습니다. NIS`netgroup` 맵의 새 항목은 다음과 같습니다:

[.programlisting]
....
BIGSRV    IT_EMP  IT_APP
SMALLSRV  IT_EMP  IT_APP  ITINTERN
USERBOX   IT_EMP  ITINTERN USERS
....

로그인 제한을 정의하는 이 방법은 동일한 제한을 가진 컴퓨터 그룹을 정의할 수 있을 때 매우 합리적으로 작동합니다. 안타깝게도 이것은 예외이지 규칙은 아닙니다. 대부분의 경우 컴퓨터별로 로그인 제한을 정의할 수 있는 기능이 필요합니다.

시스템별 넷그룹 정의는 정책 변경을 해결할 수 있는 또 다른 방법입니다. 이 시나리오에서는 각 시스템의 [.filename]#/etc/master.passwd#에 “+”로 시작하는 두 줄이 포함되어 있습니다. 첫 번째 줄은 이 시스템에 로그인할 수 있는 계정이 포함된 넷그룹을 추가하고, 두 번째 줄은 [.filename]#//usr/sbin/nologin#을 셸로 사용하는 다른 모든 계정을 추가합니다. 넷그룹의 이름으로 호스트 이름의 “ALL-CAPS” 버전을 사용하는 것이 좋습니다:

[.programlisting]
....
+@BOXNAME:::::::::
+:::::::::/usr/sbin/nologin
....

모든 컴퓨터에서 이 작업을 완료하면 더 이상 [.filename]#/etc/master.passwd#의 로컬 버전을 수정할 필요가 없습니다. 모든 추가 변경 사항은 NIS 맵을 수정하여 처리할 수 있습니다. 다음은 이 시나리오에 사용할 수 있는 `netgroup` 맵의 예시입니다:

[.programlisting]
....
# Define groups of users first
IT_EMP    (,alpha,test-domain)    (,beta,test-domain)
IT_APP    (,charlie,test-domain)  (,delta,test-domain)
DEPT1     (,echo,test-domain)     (,foxtrott,test-domain)
DEPT2     (,golf,test-domain)     (,hotel,test-domain)
DEPT3     (,india,test-domain)    (,juliet,test-domain)
ITINTERN  (,kilo,test-domain)     (,lima,test-domain)
D_INTERNS (,able,test-domain)     (,baker,test-domain)
#
# Now, define some groups based on roles
USERS     DEPT1   DEPT2     DEPT3
BIGSRV    IT_EMP  IT_APP
SMALLSRV  IT_EMP  IT_APP    ITINTERN
USERBOX   IT_EMP  ITINTERN  USERS
#
# And a groups for a special tasks
# Allow echo and golf to access our anti-virus-machine
SECURITY  IT_EMP  (,echo,test-domain)  (,golf,test-domain)
#
# machine-based netgroups
# Our main servers
WAR       BIGSRV
FAMINE    BIGSRV
# User india needs access to this server
POLLUTION  BIGSRV  (,india,test-domain)
#
# This one is really important and needs more access restrictions
DEATH     IT_EMP
#
# The anti-virus-machine mentioned above
ONE       SECURITY
#
# Restrict a machine to a single user
TWO       (,hotel,test-domain)
# [...more groups to follow]
....

머신 기반 넷그룹을 사용하는 것이 항상 권장되는 것은 아닙니다. 수십 또는 수백 대의 시스템을 배포할 때는 머신 기반 넷그룹 대신 역할 기반 넷그룹을 사용하여 NIS 맵의 크기를 합리적인 한도 내에서 유지할 수 있습니다.

=== 패스워드 형식(규칙)

NIS에서는 NIS 도메인 내의 모든 호스트가 암호를 암호화할 때 동일한 형식을 사용하도록 요구합니다. 사용자가 NIS 클라이언트에서 인증하는 데 문제가 있는 경우 암호 형식이 다르기 때문일 수 있습니다. 이기종 네트워크에서는 모든 운영 체제에서 이 형식을 지원해야 하며, 여기서 DES는 가장 낮은 공통 표준입니다.

서버 또는 클라이언트가 어떤 형식을 사용하는지 확인하려면 [.filename]#/etc/login.conf#의 다음 섹션을 확인하세요:

[.programlisting]
....
default:\
	:passwd_format=des:\
	:copyright=/etc/COPYRIGHT:\
	[Further entries elided]
....

이 예에서는 시스템에서 비밀번호 해싱에 DES 형식을 사용하고 있습니다. 다른 가능한 값으로는 Blowfish의 경우 `blf`, MD5의 경우 `md5`, SHA-256 및 SHA-512의 경우 각각 `sha256` 및 `sha512`가 있습니다. 자세한 정보와 시스템에서 사용할 수 있는 최신 목록은 man:crypt[3] 페이지를 참조하세요.

호스트의 형식을 NIS 도메인에서 사용 중인 형식과 일치하도록 편집해야 하는 경우, 변경 사항을 저장한 후 로그인 기능 데이터베이스를 다시 빌드해야 합니다:

[source, shell]
....
# cap_mkdb /etc/login.conf
....

[NOTE]
====
기존 사용자 계정의 비밀번호 형식은 로그인 기능 데이터베이스가 재구축된 후 각 사용자가 비밀번호를 변경할 때까지 업데이트되지 않습니다.
====

[[network-ldap]]
== 경량 디렉터리 액세스 프로토콜 (Lightweight Directory Access Protocol, LDAP)

LDAP(경량 디렉토리 액세스 프로토콜)는 분산 디렉토리 정보 서비스를 사용하여 개체에 액세스, 수정 및 인증하는 데 사용되는 애플리케이션 계층 프로토콜입니다. 여러 수준의 계층적이고 동질적인 정보를 저장하는 전화번호부라고 생각하면 됩니다. 분산 디렉터리 정보 서비스는 Active Directory 및 OpenLDAP 네트워크에서 사용되며 사용자가 단일 계정을 사용하여 여러 수준의 내부 정보에 액세스할 수 있도록 합니다. 예를 들어 이메일 인증, 직원 연락처 정보 가져오기, 내부 웹사이트 인증은 모두 LDAP 서버의 레코드 베이스에 있는 단일 사용자 계정을 사용할 수 있습니다.

이 섹션에서는 FreeBSD 시스템에서 LDAP 서버를 구성하기 위한 빠른 시작 가이드를 제공합니다. 여기서는 관리자가 저장할 정보의 유형, 해당 정보가 사용될 용도, 해당 정보에 액세스할 수 있는 사용자, 무단 액세스로부터 해당 정보를 보호하는 방법 등을 포함하는 설계 계획을 이미 가지고 있다고 가정합니다.

=== LDAP의 용어와 구조

LDAP의 구성을 시작하기 전에 이해해야 하는 몇 가지 용어를 설명합니다. 모든 디렉터리 항목은 _속성_ 그룹으로 구성됩니다. 이러한 각 속성 집합에는 _고유 이름 (Distinguished Name, DN)_이라는 고유 식별자가 포함되어 있으며, 일반적으로 공통 또는 _상대 고유 이름 (Relative Distinguished Name, RDN)_과 같은 여러가지 다양한 속성을 기반으로 구축됩니다. 디렉터리에 절대 경로와 상대 경로가 있는 것과 유사하게 DN을 절대 경로로, RDN을 상대 경로로 생각하면 됩니다.

LDAP 항목의 예는 다음과 같습니다. 이 예에서는 지정된 사용자 계정(`uid`), 조직 단위(`ou`) 및 조직(`o`)에 대한 항목을 검색합니다:

[source, shell]
....
% ldapsearch -xb "uid=trhodes,ou=users,o=example.com"
# extended LDIF
#
# LDAPv3
# base <uid=trhodes,ou=users,o=example.com> with scope subtree
# filter: (objectclass=*)
# requesting: ALL
#

# trhodes, users, example.com
dn: uid=trhodes,ou=users,o=example.com
mail: trhodes@example.com
cn: Tom Rhodes
uid: trhodes
telephoneNumber: (123) 456-7890

# search result
search: 2
result: 0 Success

# numResponses: 2
# numEntries: 1
....

이 예제 항목은 `dn`, `mail`, `cn`, `uid` 및 `telephoneNumber` 속성에 대한 값을 보여줍니다. cn 특성은 RDN입니다.

LDAP 및 해당 용어에 대한 자세한 내용은 http://www.openldap.org/doc/admin24/intro.html[http://www.openldap.org/doc/admin24/intro.html]에서 확인할 수 있습니다.

[[ldap-config]]
=== LDAP 서버 구성하기

FreeBSD는 내장 LDAP 서버를 제공하지 않습니다. package:net/openldap-server[] 패키지 또는 포트를 설치하여 구성을 시작하세요:

[source, shell]
....
# pkg install openldap-server
....

extref:{linux-users}[package, software]에 활성화된 기본 옵션이 많이 있습니다. `pkg info openldap-server`를 실행하여 검토하세요. 기본 옵션이 충분하지 않은 경우(예: SQL 지원이 필요한 경우) 적절한 crossref:ports[ports-using,framework]를 사용하여 포트를 다시 컴파일하는 것을 고려하세요.

설치하면 데이터를 저장하기 위해 [.filename]#/var/db/openldap-data# 디렉터리가 만들어집니다. 인증서를 저장할 디렉터리를 만들어야 합니다:

[source, shell]
....
# mkdir /usr/local/etc/openldap/private
....

다음 단계는 인증 기관(Certificate Authority)을 구성하는 것입니다. 다음 명령은 [.filename]#/usr/local/etc/openldap/private#에서 실행해야 합니다. 파일 권한이 제한적이어야 하며 사용자가 이러한 파일에 액세스할 수 없어야 하므로 이 작업은 매우 중요합니다. 인증서 및 해당 매개변수에 대한 자세한 정보는 crossref:security[openssl,”OpenSSL”]에서 확인할 수 있습니다. 인증 기관을 만들려면 이 명령으로 시작하여 메시지에 따라 진행합니다:

[source, shell]
....
# openssl req -days 365 -nodes -new -x509 -keyout ca.key -out ../ca.crt
....

프롬프트에 입력할 항목은 `Common Name`을 _제외_하고 대개 일반적인 것들입니다. 이 항목은 시스템 호스트 이름과 반드시 _다른_ 이름이어야 합니다. 자체 서명된 인증서인 경우, 호스트 이름 앞에 인증 기관의 `CA`를 붙입니다.

다음 작업은 인증서 서명 요청과 개인 키를 만드는 것입니다. 이 명령을 입력하고 지시를 따릅니다:

[source, shell]
....
# openssl req -days 365 -nodes -new -keyout server.key -out server.csr
....

인증서 생성 과정에서 `Common Name` 속성을 올바르게 설정해야 합니다. 인증서 서명 요청을 유효한 인증서로 사용하려면 인증 기관에서 서명해야 합니다:

[source, shell]
....
# openssl x509 -req -days 365 -in server.csr -out ../server.crt -CA ../ca.crt -CAkey ca.key -CAcreateserial
....

인증서 생성 프로세스의 마지막 부분은 클라이언트 인증서를 생성하고 서명하는 것입니다:

[source, shell]
....
# openssl req -days 365 -nodes -new -keyout client.key -out client.csr
# openssl x509 -req -days 3650 -in client.csr -out ../client.crt -CA ../ca.crt -CAkey ca.key
....

메시지가 표시되면 동일한 `Common Name` 속성을 사용해야 합니다. 완료되면 진행 과정을 확인해 총 8개의 새 파일이 생성되었는지 확인합니다.

OpenLDAP 서버를 실행하는 데몬은 [.filename]#slapd#입니다. 이 구성은 [.filename]#slapd.ldif#를 통해 수행됩니다. 이전 [.filename]#slapd.conf#는 OpenLDAP에서 더 이상 사용되지 않습니다.

http://www.openldap.org/doc/admin24/slapdconf2.html[Configuration examples]는 [.filename]#slapd.ldif#에 사용할 수 있으며 [.filename]#/usr/local/etc/openldap/slapd.ldif.sample#에서도 다른 예제를 찾을 수 있습니다. 이에대한 옵션은 slapd-config(5)에 문서화되어 있습니다. 다른 모든 LDAP 어트리뷰트 세트와 마찬가지로 [.filename]#slapd.ldif#의 각 섹션은 DN을 통해 고유하게 구분됩니다. `dn:` 문과 섹션의 끝 사이에는 절대 빈 줄이 없어야 합니다. 다음 예에서는 보안 채널을 구현하는 데 TLS를 사용합니다. 첫 번째 섹션은 글로벌 구성을 나타냅니다:

[.programlisting]
....
#
# See slapd-config(5) for details on configuration options.
# This file should NOT be world readable.
#
dn: cn=config
objectClass: olcGlobal
cn: config
#
#
# Define global ACLs to disable default read access.
#
olcArgsFile: /var/run/openldap/slapd.args
olcPidFile: /var/run/openldap/slapd.pid
olcTLSCertificateFile: /usr/local/etc/openldap/server.crt
olcTLSCertificateKeyFile: /usr/local/etc/openldap/private/server.key
olcTLSCACertificateFile: /usr/local/etc/openldap/ca.crt
#olcTLSCipherSuite: HIGH
olcTLSProtocolMin: 3.1
olcTLSVerifyClient: never
....

여기에 인증 기관, 서버 인증서 및 서버 개인 키 파일을 지정해야 합니다. 클라이언트가 보안 암호를 선택하도록 하고 `olcTLSCipherSuite` 옵션은 생략하는 것이 좋습니다([.filename]#openssl# 이외의 TLS 클라이언트와 호환되지 않음). 옵션 `olcTLSProtocolMin`을 사용하면 서버에 최소한의 보안 수준을 요구할 수 있으므로 권장됩니다. 서버에서는 인증절차가 필수이지만 클라이언트 측에서는 인증절차가 필요하지 않습니다(`olcTLSVerifyClient: never`).

두 번째 섹션은 백엔드 모듈에 관한 것으로 다음과 같이 구성할 수 있습니다:

[.programlisting]
....
#
# Load dynamic backend modules:
#
dn: cn=module,cn=config
objectClass: olcModuleList
cn: module
olcModulepath:	/usr/local/libexec/openldap
olcModuleload:	back_mdb.la
#olcModuleload:	back_bdb.la
#olcModuleload:	back_hdb.la
#olcModuleload:	back_ldap.la
#olcModuleload:	back_passwd.la
#olcModuleload:	back_shell.la
....

세 번째 섹션은 데이터베이스를 사용할 때 필요한 `ldif` 스키마 로딩에 대해 기술하고 있습니다: 이 부분은 매우 중요합니다.

[.programlisting]
....
dn: cn=schema,cn=config
objectClass: olcSchemaConfig
cn: schema

include: file:///usr/local/etc/openldap/schema/core.ldif
include: file:///usr/local/etc/openldap/schema/cosine.ldif
include: file:///usr/local/etc/openldap/schema/inetorgperson.ldif
include: file:///usr/local/etc/openldap/schema/nis.ldif
....

다음은 프론트엔드 구성 섹션:

[.programlisting]
....
# Frontend settings
#
dn: olcDatabase={-1}frontend,cn=config
objectClass: olcDatabaseConfig
objectClass: olcFrontendConfig
olcDatabase: {-1}frontend
olcAccess: to * by * read
#
# Sample global access control policy:
#	Root DSE: allow anyone to read it
#	Subschema (sub)entry DSE: allow anyone to read it
#	Other DSEs:
#		Allow self write access
#		Allow authenticated users read access
#		Allow anonymous users to authenticate
#
#olcAccess: to dn.base="" by * read
#olcAccess: to dn.base="cn=Subschema" by * read
#olcAccess: to *
#	by self write
#	by users read
#	by anonymous auth
#
# if no access controls are present, the default policy
# allows anyone and everyone to read anything but restricts
# updates to rootdn.  (e.g., "access to * by * read")
#
# rootdn can always read and write EVERYTHING!
#
olcPasswordHash: {SSHA}
# {SSHA} is already the default for olcPasswordHash
....

또 다른 섹션에서는 나중에 글로벌 수퍼유저로서만 OpenLDAP 서버 구성에 액세스할 수 있는 유일한 방법인 _configuration 백엔드_에 대해 기술하고 있습니다.

[.programlisting]
....
dn: olcDatabase={0}config,cn=config
objectClass: olcDatabaseConfig
olcDatabase: {0}config
olcAccess: to * by * none
olcRootPW: {SSHA}iae+lrQZILpiUdf16Z9KmDmSwT77Dj4U
....

기본 관리자 이름은 `cn=config`입니다. 셸에 [.filename]#slappasswd#를 입력하고 비밀번호를 선택한 다음 `olcRootPW`에서 해당 해시를 사용합니다. 지금 이 옵션을 지정하지 않으면 [.filename]#slapd.ldif#를 가져오기 전에 아무도 나중에 _global configuration_ 섹션을 수정할 수 없습니다.

마지막 섹션은 데이터베이스 백앤드에 대한 것입니다:

[.programlisting]
....
#######################################################################
# LMDB database definitions
#######################################################################
#
dn: olcDatabase=mdb,cn=config
objectClass: olcDatabaseConfig
objectClass: olcMdbConfig
olcDatabase: mdb
olcDbMaxSize: 1073741824
olcSuffix: dc=domain,dc=example
olcRootDN: cn=mdbadmin,dc=domain,dc=example
# Cleartext passwords, especially for the rootdn, should
# be avoided.  See slappasswd(8) and slapd-config(5) for details.
# Use of strong authentication encouraged.
olcRootPW: {SSHA}X2wHvIWDk6G76CQyCMS1vDCvtICWgn0+
# The database directory MUST exist prior to running slapd AND
# should only be accessible by the slapd and slap tools.
# Mode 700 recommended.
olcDbDirectory:	/var/db/openldap-data
# Indices to maintain
olcDbIndex: objectClass eq
....

이 데이터베이스는 LDAP 디렉터리의 _실제 내용_을 호스팅합니다. `mdb` 이외의 유형도 사용할 수 있습니다. 글로벌 사용자와 혼동하지 말아야 할 수퍼유저가 여기에 구성됩니다: `olcRootDN`에 (사용자 정의 가능한) 사용자 이름, `olcRootPW`에 비밀번호 해시; [.filename]#slappasswd#는 이전과 같이 사용할 수 있습니다.

이 http://www.openldap.org/devel/gitweb.cgi?p=openldap.git;a=tree;f=tests/data/regressions/its8444;h=8a5e808e63b0de3d2bdaf2cf34fecca8577ca7fd;hb=HEAD[repository]에는 [.filename]#slapd.ldif#의 네 가지 예가 포함되어 있습니다. 기존 [.filename]#slapd.conf#를 [.filename]#slapd.ldif#로 변환하려면 http://www.openldap.org/doc/admin24/slapdconf2.html[이 페이지]를 참조하세요(일부 쓸모없는 옵션이 생길 수 있음에 유의하세요).

구성이 완료되면 [.filename]#slapd.ldif#를 빈 디렉터리에 배치해야 합니다. 다음과 같이 생성하는 것이 좋습니다:

[source, shell]
....
# mkdir /usr/local/etc/openldap/slapd.d/
....

구성 데이터베이스를 가져옵니다:

[source, shell]
....
# /usr/local/sbin/slapadd -n0 -F /usr/local/etc/openldap/slapd.d/ -l /usr/local/etc/openldap/slapd.ldif
....

[.filename]#slapd# 데몬을 실행합니다:

[source, shell]
....
# /usr/local/libexec/slapd -F /usr/local/etc/openldap/slapd.d/
....

옵션 `-d`는 slapd(8)에 지정된 대로, 디버깅에 사용할 수 있습니다. 서버가 실행 중이고 작동하는지 확인합니다:

[source, shell]
....
# ldapsearch -x -b '' -s base '(objectclass=*)' namingContexts
# extended LDIF
#
# LDAPv3
# base <> with scope baseObject
# filter: (objectclass=*)
# requesting: namingContexts
#

#
dn:
namingContexts: dc=domain,dc=example

# search result
search: 2
result: 0 Success

# numResponses: 2
# numEntries: 1
....

서버를 여전히 신뢰할 수 있어야 합니다(인증되어 있어야 한다는 뜻). 이전에 이 작업을 수행한 적이 없다면 다음 지침을 따르세요. OpenSSL 패키지 또는 포트를 설치합니다:

[source, shell]
....
# pkg install openssl
....

[.filename]#ca.crt#가 저장되어 있는 디렉터리(이 예에서는 [.파일 이름]#/usr/local/etc/openldap#)에서 다음을 실행합니다:

[source, shell]
....
# c_rehash .
....

이제 CA 및 서버 인증서가 모두 각자의 역할에서 올바르게 인식됩니다. 이를 확인하려면 [.filename]#server.crt# 디렉토리에서 이 명령을 실행합니다:

[source, shell]
....
# openssl verify -verbose -CApath . server.crt
....

[.filename]#slapd#가 실행 중이라면 다시 시작합니다. [.filename]#/usr/local/etc/rc.d/slapd#에 명시된 대로, 부팅 시 [.filename]#slapd#를 제대로 실행하려면 [.filename]#/etc/rc.conf#에 다음 줄을 추가해야 합니다:

[.programlisting]
....
slapd_enable="YES"
slapd_flags='-h "ldapi://%2fvar%2frun%2fopenldap%2fldapi/
ldap://0.0.0.0/"'
slapd_sockets="/var/run/openldap/ldapi"
slapd_cn_config="YES"
....

[.filename]#slapd#는 부팅 시 디버깅을 제공하지 않습니다. 만약 확인이 필요하다면 [.filename]#/var/log/debug.log#, [.filename]#dmesg -a# 및 [.filename]#/var/log/messages#를 확인하시기 바랍니다.

다음 예에서는 아직 비어 있는 `domain.example` LDAP 데이터베이스에 그룹 `team`과 사용자 `john`을 추가합니다. 먼저 [.filename]#domain.ldif# 파일을 만듭니다:

[source, shell]
....
# cat domain.ldif
dn: dc=domain,dc=example
objectClass: dcObject
objectClass: organization
o: domain.example
dc: domain

dn: ou=groups,dc=domain,dc=example
objectClass: top
objectClass: organizationalunit
ou: groups

dn: ou=users,dc=domain,dc=example
objectClass: top
objectClass: organizationalunit
ou: users

dn: cn=team,ou=groups,dc=domain,dc=example
objectClass: top
objectClass: posixGroup
cn: team
gidNumber: 10001

dn: uid=john,ou=users,dc=domain,dc=example
objectClass: top
objectClass: account
objectClass: posixAccount
objectClass: shadowAccount
cn: John McUser
uid: john
uidNumber: 10001
gidNumber: 10001
homeDirectory: /home/john/
loginShell: /usr/bin/bash
userPassword: secret
....

자세한 내용은 OpenLDAP 설명서를 참조하세요. [.filename]#slappasswd#를 사용하여 일반 텍스트 비밀번호 `secret`을 `userPassword`의 해시로 바꿉니다. `loginShell`로 지정된 경로는 `john`이 로그인할 수 있는 모든 시스템에 존재해야 합니다. 마지막으로 `mdb` 관리자를 사용하여 데이터베이스를 수정합니다:

[source, shell]
....
# ldapadd -W -D "cn=mdbadmin,dc=domain,dc=example" -f domain.ldif
....

_global configuration_ 섹션에 대한 수정은 글로벌 수퍼유저만 수행할 수 있습니다. 예를 들어, 옵션 `olcTLSCipherSuite: HIGH:MEDIUM:SSLv3` 옵션이 처음에 지정되었으므로 이제 삭제해야 한다고 가정합니다. 먼저 다음을 포함하는 파일을 만듭니다:

[source, shell]
....
# cat global_mod
dn: cn=config
changetype: modify
delete: olcTLSCipherSuite
....

그리고 나서 다음과 같이 수정합니다:

[source, shell]
....
# ldapmodify -f global_mod -x -D "cn=config" -W
....

메시지가 표시되면 _configuration backend_ 섹션에서 선택한 비밀번호를 입력합니다. 사용자 이름은 필수가 아닙니다. 여기서 `cn=config`는 수정할 데이터베이스 섹션의 DN을 나타냅니다. 또는 데이터베이스의 한 줄을 삭제하려면 `ldapmodify`를, 전체 항목을 삭제하려면 `ldapdelete`를 사용합니다.

문제가 발생하거나 글로벌 수퍼유저가 구성 백엔드에 액세스할 수 없는 경우 전체 구성을 삭제하고 다시 작성할 수 있습니다:

[source, shell]
....
# rm -rf /usr/local/etc/openldap/slapd.d/
....

그런 다음 [.filename]#slapd.ldif#를 편집하고 다시 가져올 수 있습니다. 다른 해결 방법을 사용할 수 없는 경우에만 이 절차를 따르세요.

이것은 서버만 해당되는 구성입니다. 동일한 컴퓨터에서 별도의 구성을 통해 LDAP 클라이언트를 호스팅할 수도 있습니다.

[[network-dhcp]]
== 동적 호스트 구성 프로토콜(Dynamic Host Configuration Protocol, DHCP)

동적 호스트 구성 프로토콜(DHCP)은 시스템이 네트워크에 연결하여 해당 네트워크에서 통신하는 데 필요한 주소 지정 정보를 할당받을 수 있도록 합니다. FreeBSD에는 클라이언트가 주소 지정 정보를 얻기 위해 사용하는 `dhclient`의 OpenBSD 버전이 포함되어 있습니다. FreeBSD는 DHCP 서버를 기본설치하지 않지만, 이와 관련된 여러가지 서버를 FreeBSD 포트 컬렉션에서 사용할 수 있습니다. DHCP 프로토콜은 http://www.freesoft.org/CIE/RFC/2131/[RFC 2131]에 자세히 설명되어 있습니다. 정보 리소스는 http://www.isc.org/downloads/dhcp/[isc.org/downloads/dhcp/]에서도 확인할 수 있습니다.

이 섹션에서는 기본 제공 DHCP 클라이언트를 사용하는 방법을 설명합니다. 그런 다음 DHCP 서버를 설치하고 구성하는 방법을 설명합니다.

[NOTE]
====
FreeBSD에서, man:bpf[4] 장치는 DHCP 서버와 DHCP 클라이언트 모두에 필요합니다. 이 장치는 FreeBSD와 함께 설치되는 [.filename]#GENERIC# 커널에 포함되어 있습니다. 사용자 정의 커널을 만들고자 하는 사용자는 DHCP를 사용하는 경우 이 장치를 유지해야 합니다.

[.filename]#bpf#를 사용하면 권한 있는 사용자가 해당 시스템에서 네트워크 패킷 스니퍼를 실행할 수도 있습니다.
====


=== DHCP 클라이언트 구성하기

DHCP 클라이언트 지원은 FreeBSD 설치 프로그램에 포함되어 있어, 새로 설치한 시스템이 기존 DHCP 서버로부터 네트워크 주소 정보를 자동으로 수신하도록 쉽게 구성할 수 있습니다. 네트워크 구성의 예는 crossref:bsdinstall[bsdinstall-post,”Accounts, Time Zone, Services and Hardening”]를 참조하세요.

클라이언트 시스템에서 `dhclient`가 실행되면 구성 정보에 대한 브로드캐스트 요청을 시작합니다. 기본적으로 이러한 요청은 UDP 포트 68을 사용합니다. 서버는 UDP 포트 67로 응답하여 클라이언트에 IP 주소와 서브넷 마스크, 기본 게이트웨이 및 DNS 서버 주소와 같은 기타 관련 네트워크 정보를 제공합니다. 이 정보는 DHCP “임대” 형태로 제공되며 구성 가능한 시간 동안 유효합니다. 따라서 더 이상 네트워크에 연결되지 않는 클라이언트의 오래된 IP 주소를 자동으로 재사용할 수 있습니다. DHCP 클라이언트는 서버로부터 많은 정보를 얻을 수 있습니다. 전체 목록은 man:dhcp-options[5]에서 확인할 수 있습니다.

기본적으로 FreeBSD 시스템이 부팅되면 DHCP 클라이언트가 백그라운드에서 실행되거나 _비동기적으로_ 실행됩니다. DHCP 프로세스가 완료되는 동안 다른 시작 스크립트가 계속 실행되므로 시스템 시작 속도가 빨라집니다.

백그라운드 DHCP는 DHCP 서버가 클라이언트의 요청에 빠르게 응답할 때 잘 작동합니다. 그러나 일부 시스템에서는 DHCP를 완료하는 데 시간이 오래 걸릴 수 있습니다. DHCP가 네트워크 주소 지정 정보를 할당하기 전에 네트워크 서비스가 실행을 시도하면 실패합니다. _동기화_모드에서 DHCP를 사용하면 DHCP 구성이 완료될 때까지 시작을 일시 중지하므로 이 문제를 방지할 수 있습니다.

[.filename]#/etc/rc.conf#의 이 줄은 백그라운드 또는 비동기 모드를 구성하는 데 사용됩니다:

[.programlisting]
....
ifconfig_fxp0="DHCP"
....

설치 중에 시스템이 DHCP를 사용하도록 구성되었다면 이 줄이 이미 존재할 수 있습니다. 이 예제에 표시된 _fxp0_을 crossref:config[config-network-setup,“Setting Up Network Interface Cards”]에 설명한 대로 동적으로 구성할 인터페이스의 이름으로 바꿉니다.

동기 모드(Synchronous mode)를 사용하도록 시스템을 구성하고 DHCP가 완료되는 동안 시작 중에 일시중지를 하려면 “`SYNCDHCP`”를 사용합니다:

[.programlisting]
....
ifconfig_fxp0="SYNCDHCP"
....

추가 클라이언트 옵션을 사용할 수 있습니다. 자세한 내용은 man:rc.conf[5]에서 `dhclient`를 검색하세요.

DHCP 클라이언트는 다음 파일들을 사용합니다:

* [.filename]#/etc/dhclient.conf#
+
`dhclient`에서 사용하는 구성 파일입니다. 일반적으로 이 파일에는 기본값이 대부분의 클라이언트에 적합하므로 주석만 포함되어 있습니다. 이 구성 파일은 man:dhclient.conf[5]에서 자세히 설명하고 있습니다.
* [.filename]#/sbin/dhclient#
+
명령 자체에 대한 자세한 정보는 man:dhclient[8]에서 확인할 수 있습니다.
* [.filename]#/sbin/dhclient-script#
+
FreeBSD 전용 DHCP 클라이언트 구성 스크립트. 이 스크립트는 man:dhclient-script[8]에 설명되어 있지만, 제대로 작동하기 위해 사용자가 수정할 필요는 없습니다.
* [.filename]#/var/db/dhclient.leases.interface#
+
DHCP 클라이언트는 이 파일에 유효한 임대 데이터베이스를 보관하며, 이 데이터베이스는 로그로 작성되어 man:dhclient.leases[5]에 설명되어 있습니다.


[[network-dhcp-server]]
=== DHCP 서버 설치와 구성하기

이 섹션에서는 DHCP 서버의 인터넷 시스템 컨소시엄(Internet Systems Consortium, ISC) 구현을 사용하여 FreeBSD 시스템이 DHCP 서버로 작동하도록 구성하는 방법을 설명합니다. 이 구현과 해당 문서는 package:net/isc-dhcp44-server[] 패키지 또는 포트를 사용하여 설치할 수 있습니다.

package:net/isc-dhcp44-server[]를 설치하면 샘플 구성 파일이 설치됩니다. [.filename]#/usr/local/etc/dhcpd.conf.example#를 [.filename]#/usr/local/etc/dhcpd.conf.example#에 복사하고 이 새 파일을 편집합니다.

구성 파일은 서브넷 및 호스트에 대한 선언으로 구성되며, 이 선언은 DHCP 클라이언트에 제공되는 정보를 정의합니다. 예를 들어, 다음 내용은 해당 내용을 구성합니다:

[.programlisting]
....
option domain-name "example.org";<.>
option domain-name-servers ns1.example.org;<.>
option subnet-mask 255.255.255.0;<.>

default-lease-time 600;<.>
max-lease-time 72400;<.>
ddns-update-style none;<.>

subnet 10.254.239.0 netmask 255.255.255.224 {
  range 10.254.239.10 10.254.239.20;<.>
  option routers rtr-239-0-1.example.org, rtr-239-0-2.example.org;<.>
}

host fantasia {
  hardware ethernet 08:00:07:26:c0:a5;<.>
  fixed-address fantasia.fugue.com;<.>
}
....

<.> 이 옵션은 클라이언트에 제공될 기본 검색 도메인을 지정합니다. 자세한 내용은 man:resolv.conf[5]를 참조하세요.
<.> 이 옵션은 클라이언트가 사용해야 하는 DNS 서버의 쉼표로 구분된 목록을 지정합니다. 서버는 예시에서와 같이 FQDN(정규화된 도메인 이름)으로 나열하거나 IP 주소로 나열할 수 있습니다.
<.> 클라이언트에 제공될 서브넷 마스크입니다.
<.> 기본 임대 만료 시간(초)입니다. 클라이언트가 이 값을 재정의하도록 구성할 수 있습니다.
<.> 임대에 허용되는 최대 기간(초)입니다. 클라이언트가 더 긴 임대를 요청하는 경우에도 임대가 발급되지만 `max-lease-time`을 넘길수는 없습니다.
<.> 기본값인 `none`은 동적 DNS 업데이트를 비활성화합니다. 이 설정을 `interim`으로 변경하면 DHCP 서버가 임대를 제공할 때마다 DNS 서버를 업데이트하도록 구성하여 DNS 서버가 네트워크의 어떤 IP 주소가 어떤 컴퓨터와 연결되어 있는지 알 수 있도록 합니다. DNS 서버가 동적 DNS를 지원하도록 구성되지 않은 경우 기본 설정을 변경하지 마세요.
<.> 이 줄은 DHCP 클라이언트에 할당하기 위해 예약된 사용 가능한 IP 주소 풀을 생성합니다. 주소 범위는 이전 줄에 지정된 네트워크 또는 서브넷에 유효해야 합니다.
<.> 여는 `{` 괄호 앞에 지정된 네트워크 또는 서브넷에 유효한 기본 게이트웨이를 선언합니다.
<.> 클라이언트가 요청할 때 DHCP 서버가 클라이언트를 인식할 수 있도록 클라이언트의 하드웨어 MAC 주소를 지정합니다.
<.> 이 호스트에 항상 동일한 IP 주소를 할당하도록 지정합니다. DHCP 서버가 임대 정보를 반환하기 전에 호스트 이름을 확인하므로 호스트 이름을 사용하는 것이 올바릅니다.

이 설정 파일은 더 많은 옵션을 지원합니다. 자세한 내용과 예제는 서버와 함께 설치되는 dhcpd.conf(5)를 참조하세요.

[.filename]#dhcpd.conf#의 구성이 완료되면 [.filename]#/etc/rc.conf#에서 DHCP 서버를 활성화합니다:

[.programlisting]
....
dhcpd_enable="YES"
dhcpd_ifaces="dc0"
....

공백으로 구분된 `dc0`을 DHCP 서버가 DHCP 클라이언트 요청에 대해 수신 대기해야 하는 인터페이스로 바꿉니다.

다음 명령을 실행하여 서버를 시작합니다:

[source, shell]
....
# service isc-dhcpd start
....

향후 서버 구성을 변경하려면 dhcpd 서비스를 중지한 다음 man:service[8]를 사용하여 시작해야 합니다.

DHCP 서버는 다음 파일을 사용합니다. 매뉴얼 페이지는 서버 소프트웨어와 함께 설치됩니다.

* [.filename]#/usr/local/sbin/dhcpd#
+
dhcpd 서버에 대한 자세한 정보는 dhcpd(8)에서 확인할 수 있습니다.
* [.filename]#/usr/local/etc/dhcpd.conf#
+
서버 구성 파일에는 서버 작동에 관한 정보와 함께 클라이언트에 제공해야 하는 모든 정보가 포함되어야 합니다. 이 구성 파일은 dhcpd.conf(5)에 설명되어 있습니다.
* [.filename]#/var/db/dhcpd.leases#
+
DHCP 서버는 이 파일에 발급한 임대 데이터베이스를 보관하며, 이 데이터베이스는 로그로 기록됩니다. 조금 더 자세한 설명은 dhcpd.leases(5)를 참조하세요.
* [.filename]#/usr/local/sbin/dhcrelay#
+
이 데몬은 한 DHCP 서버가 클라이언트의 요청을, 별도의 네트워크에 있는 다른 DHCP 서버로 전달하는 고급 환경에서 사용됩니다. 이 기능이 필요한 경우 package:net/isc-dhcp44-relay[] 패키지 또는 포트를 설치하세요. 이 설치에 대한 더 자세한 내용은 함께 제공되는 dhcrelay(8)에 포함되어 있습니다.


[[network-dns]]
== 도메인 네임 시스템(DNS)

DNS(도메인 네임 시스템)는 도메인 네임이 IP 주소에 매핑되거나 그 반대로 매핑되는 형태의 프로토콜입니다. DNS는 개별 도메인 정보를 호스팅하고 캐시하는 권한 있는 루트, TLD(최상위 도메인) 및 기타 소규모 네임 서버로 구성된 다소 복잡한 시스템을 통해 인터넷 전체에서 조정됩니다. 시스템에서 DNS 조회를 수행하기 위해 네임 서버를 실행할 필요는 없습니다.

다음 표에서는 DNS와 관련된 몇 가지 용어에 대해 설명합니다:

.DNS 용어
[cols="1,1", frame="none", options="header"]
|===
| Term
| Definition

|Forward DNS
|Mapping of hostnames to IP addresses.

|Origin
|Refers to the domain covered in a particular zone file.

|Resolver
|A system process through which a machine queries a name server for zone information.

|Reverse DNS
|Mapping of IP addresses to hostnames.

|Root zone
|The beginning of the Internet zone hierarchy. All zones fall under the root zone, similar to how all files in a file system fall under the root directory.

|Zone
|An individual domain, subdomain, or portion of the DNS administered by the same authority.
|===

영역(zone)의 예:

* `.` 는 일반적으로 문서에서 루트 영역을 지칭하는 방식입니다.
* `org.`는 루트 영역 아래의 최상위 도메인(TLD)입니다.
* `example.org.` 는 `org.`TLD 아래의 영역입니다.
* `1.168.192.in-addr.arpa`는 `192.168.1.*` IP 주소 공간에 속하는 모든 IP 주소를 참조하는 영역입니다.

보시다시피 호스트 이름의 더 구체적인 부분은 왼쪽에 표시됩니다. 예를 들어, `example.org.`는 `org.`보다 더 구체적인데, 이는 `org.`가 루트 영역보다 더 구체적이기 때문입니다. 호스트 이름의 각 부분의 레이아웃은 파일 시스템과 매우 유사합니다: [.filename]#/dev# 디렉토리는 루트 디렉터리 아래에 속하고, 서브 디렉터리는 그 아래에 속하는 방식입니다.

=== 네임 서버를 운영해야 하는 이유

네임 서버는 일반적으로 권한 있는 네임 서버와 캐싱(리졸버라고도 함) 네임 서버의 두 가지 형태로 제공됩니다.

다음과 같은 경우 권한 있는 네임 서버가 필요합니다:

* 전 세계에 DNS 정보를 제공하여 쿼리에 권위 있게 응답하고자 합니다.
* `example.org`와 같은 도메인을 등록하고 그 아래 호스트 네임에 IP 주소를 할당해야 합니다.
* IP 주소 블록에는 역참조 DNS 항목(IP에서 호스트 이름으로)이 필요합니다.
* 슬레이브라고 하는 백업 또는 두 번째 네임 서버가 쿼리에 응답할 수 있습니다.

캐싱 네임 서버는 다음과 같은 경우에 필요합니다:

* 로컬 DNS 서버는 외부 네임 서버를 쿼리하는 것보다 더 빠르게 캐시하고 응답할 수 있습니다.

일반적으로 `www.FreeBSD.org`을 쿼리하면 리졸버는 업링크 ISP의 네임 서버에 쿼리하고 응답을 검색합니다. 로컬 캐싱 DNS 서버를 사용하면 캐싱 DNS 서버를 통해 외부로 한 번만 쿼리하면 됩니다. 정보가 로컬에 캐시되므로 추가 쿼리는 로컬 네트워크 외부로 나갈 필요가 없습니다.

=== DNS 서버 구성

언바운드는 FreeBSD 기본 시스템에서 제공됩니다. 기본적으로 로컬 머신에 대해서만 DNS 확인을 제공합니다. 기본 시스템 패키지는 로컬 머신을 넘어서는 확인 서비스를 제공하도록 구성할 수 있지만, 이러한 요구사항은 FreeBSD 포트 컬렉션에서 언바운드를 설치하여 해결하는 것이 좋습니다.

언바인드를 사용 설정하려면 [.filename]#/etc/rc.conf#에 다음을 추가합니다:

[.programlisting]
....
local_unbound_enable="YES"
....

[.filename]#/etc/resolv.conf#에 있는 모든 기존 네임서버는 새 언바운드 구성에서 전달자(forwarder)로 구성됩니다.

[NOTE]
====
나열된 네임서버 중 하나라도 DNSSEC을 지원하지 않는 경우 로컬 DNS 확인이 실패합니다. 각 네임서버를 테스트하고 테스트에 실패한 네임서버를 제거해야 합니다. 다음 명령은 `192.168.1.1`에서 실행 중인 네임서버의 트러스트 트리 또는 실패를 표시합니다:

[source, shell]
....
% drill -S FreeBSD.org @192.168.1.1
....
====

각 네임서버가 DNSSEC을 지원하는 것으로 확인되면 언바인딩을 시작합니다:

[source, shell]
....
# service local_unbound onestart
....

이렇게 하면 [.filename]#/etc/resolv.conf#을 업데이트하여 이제 DNSSEC 보안 도메인에 대한 쿼리가 작동하게 됩니다. 예를 들어, 다음을 실행하여 FreeBSD.org DNSSEC 트러스트 트리의 유효성을 검사합니다:

[source, shell]
....
% drill -S FreeBSD.org
;; Number of trusted keys: 1
;; Chasing: freebsd.org. A

DNSSEC Trust tree:
freebsd.org. (A)
|---freebsd.org. (DNSKEY keytag: 36786 alg: 8 flags: 256)
    |---freebsd.org. (DNSKEY keytag: 32659 alg: 8 flags: 257)
    |---freebsd.org. (DS keytag: 32659 digest type: 2)
        |---org. (DNSKEY keytag: 49587 alg: 7 flags: 256)
            |---org. (DNSKEY keytag: 9795 alg: 7 flags: 257)
            |---org. (DNSKEY keytag: 21366 alg: 7 flags: 257)
            |---org. (DS keytag: 21366 digest type: 1)
            |   |---. (DNSKEY keytag: 40926 alg: 8 flags: 256)
            |       |---. (DNSKEY keytag: 19036 alg: 8 flags: 257)
            |---org. (DS keytag: 21366 digest type: 2)
                |---. (DNSKEY keytag: 40926 alg: 8 flags: 256)
                    |---. (DNSKEY keytag: 19036 alg: 8 flags: 257)
;; Chase successful
....


[[network-apache]]
== 아파치 HTTP 서버

오픈 소스 아파치 HTTP 서버는 가장 널리 사용되는 웹 서버입니다. FreeBSD는 기본적으로 이 웹 서버를 설치하지 않지만, package:www/apache24[] 패키지 또는 포트에서 설치할 수 있습니다.

이 섹션은 FreeBSD에서 아파치 웹서버 2.x_ 버전을 설정하고 시작하는 방법을 설명하고 있습니다. 아파치 2.X와 설정 지시어에 대한 더 자세한 정보는 http://httpd.apache.org/[httpd.apache.org]를 참고하세요.

=== 아파치 구성과 시작하기

FreeBSD에서 기본 아파치 웹서버 설정파일은 [.filename]#/usr/local/etc/apache2x/httpd.conf#로 설치되며, 여기서 _x_는 버전번호를 나타냅니다. 이 ASCII 텍스트 파일은 `+#+`로 주석줄을 시작합니다. 가장 자주 수정하게되는 지시어는 다음과 같습니다:

`ServerRoot "/usr/local"`::
아파치 설치의 기본 디렉토리 계층구조를 지정합니다. 바이너리는 서버 루트의 [.filename]#bin# 및 [.filename]#sbin# 하위 디렉터리에 저장되며 구성 파일은 [.filename]#etc/apache2x# 하위 디렉터리에 저장됩니다.

`ServerAdmin you@example.com`::
이 주소를 서버 관련 문제를 수신할 이메일 주소로 변경합니다. 이 주소는 오류 문서와 같이 서버에서 생성한 일부 페이지에도 표시됩니다.

`ServerName www.example.com:80`::
관리자가 서버에 대해 클라이언트에 다시 전송되는 호스트 이름을 설정할 수 있습니다. 예를 들어 실제 호스트 이름 대신 `www`를 사용할 수 있습니다. 시스템에 등록된 DNS 이름이 없는 경우 대신 IP 주소를 입력합니다. 서버가 대체 보고서(대체 연락?)의 수신을 기다릴 경우 `80`을 다른 포트 번호로 변경합니다.

`DocumentRoot "/usr/local/www/apache2_x_/data"`::
문서가 제공될 디렉터리입니다. 기본적으로 모든 요청은 이 디렉터리에서 가져오지만 심볼릭 링크와 별칭을 사용하여 다른 위치를 가리킬 수 있습니다.

변경하기 전에 항상 기본 Apache 구성 파일의 백업본을 만드는 것이 좋습니다. 아파치 설정이 완료되면 파일을 저장하고 `apachectl`을 사용하여 설정을 확인합니다. `apachectl configtest`를 실행(아파치 설정 테스트)하면 `Syntax OK`가 반환되어야 합니다.

시스템 시작 시 Apache를 시작하려면 [.filename]#/etc/rc.conf#에 다음 줄을 추가합니다:

[.programlisting]
....
apache24_enable="YES"
....

기본 옵션이 아닌 옵션으로 아파치를 시작해야 하는 경우, [.filename]#/etc/rc.conf#에 다음 줄을 추가하여 필요한 플래그를 지정할 수 있습니다:

[.programlisting]
....
apache24_flags=""
....

만약 apachectl이 설정 오류를 보고하지 않으면, 바로 `httpd`를 시작하세요:

[source, shell]
....
# service apache24 start
....

웹 브라우저에 `http://_localhost_`을 입력하고 _localhost_를 `httpd`를 실행하는 컴퓨터의 정규화된 도메인 이름으로 바꾸면 `httpd` 서비스를 테스트할 수 있습니다. 표시되는 기본 웹 페이지는 [.filename]#/usr/local/www/apache24/data/index.html#입니다.

다음 명령을 사용하여 `httpd`가 실행되는 동안 후속 구성을 변경한 후 아파치 구성을 테스트하여 오류를 확인할 수 있습니다:

[source, shell]
....
# service apache24 configtest
....

[NOTE]
====
`configtest`는 man:rc[8] 표준이 아니며 모든 시작 스크립트에서 정상작동할 것으로 기대해서는 안 된다는 점에 유의해야 합니다.
====

=== 가상 호스팅 (Virtual Hosting)

가상 호스팅을 사용하면 하나의 아파치 서버에서 여러 웹사이트를 실행할 수 있습니다. 가상 호스트는 _IP 기반_ 또는 _이름 기반_이 될 수 있습니다. IP 기반 가상호스팅은 각 웹사이트마다 다른 IP 주소를 사용합니다. 이름 기반 가상호스팅은 클라이언트 HTTP/1.1 헤더를 사용하여 호스트 이름을 파악하므로 웹사이트가 동일한 IP 주소를 공유할 수 있습니다.

이름 기반 가상 호스팅을 사용하도록 Apache를 설정하려면 각 웹사이트에 대해 `VirtualHost` 블록을 추가합니다. 예를 들어, `www.someotherdomain.tld`의 가상 호스팅인 `www.domain.tld`이라는 웹서버의 경우 [.filename]#httpd.conf#에 다음 항목을 추가합니다:

[.programlisting]
....
<VirtualHost *>
    ServerName www.domain.tld
    DocumentRoot /www/domain.tld
</VirtualHost>

<VirtualHost *>
    ServerName www.someotherdomain.tld
    DocumentRoot /www/someotherdomain.tld
</VirtualHost>
....

각 가상 호스트에 대해 `ServerName` 및 `DocumentRoot`의 값을 사용할 값으로 바꿉니다.

가상 호스트 설정에 대한 자세한 내용은 공식 Apache 문서(http://httpd.apache.org/docs/vhosts/[http://httpd.apache.org/docs/vhosts/])를 참조하세요.

=== 아파치 모듈

아파치는 모듈을 사용하여 기본 서버에서 제공하는 기능을 보강합니다. 사용 가능한 모듈의 전체 목록과 구성 세부 정보는 http://httpd.apache.org/docs/current/mod/[http://httpd.apache.org/docs/current/mod/]를 참조하세요.

FreeBSD에서 일부 모듈은 package:www/apache24[] 포트로 컴파일할 수 있습니다. [.filename]#/usr/ports/www/apache24#에서 `make config`를 입력하면 어떤 모듈을 사용할 수 있고 어떤 모듈이 기본적으로 활성화되어 있는지 확인할 수 있습니다. 모듈이 포트와 함께 컴파일되지 않은 경우, FreeBSD 포트 컬렉션은 많은 모듈을 쉽게 설치할 수 있는 방법을 제공합니다. 이 섹션에서는 가장 일반적으로 사용되는 세 가지 모듈에 대해 설명합니다.

==== SSL 지원

한때 Apache 내부에서 SSL을 지원하려면 [.filename]#mod_ssl#이라는 보조 모듈이 필요했습니다. 하지만 이제는 더 이상 필요하지 않으며 Apache를 기본 설치하면 웹 서버에 SSL이 내장되어 있습니다. SSL 웹사이트 지원을 활성화하는 방법의 예는 설치된 파일인 [.filename]#/usr/local/etc/apache24/extra# 디렉토리에 있는 [.filename]#httpd-ssl.conf#에서 확인할 수 있습니다. 이 디렉토리 안에는 [.filename]#ssl.conf-sample#이라는 이름의 샘플 파일도 포함되어 있습니다. Apache 웹 서버에서 보안 웹사이트를 올바르게 설정하려면 두 파일을 모두 확인하는 것이 좋습니다.

SSL 구성이 완료된 후 Apache를 재시작하거나 다시 로드할 때 변경 사항을 활성화하려면 기본 [.filename]#http.conf#에 다음 줄을 주석 처리하지 않아야 합니다:

[.programlisting]
....
#Include etc/apache24/extra/httpd-ssl.conf
....

[WARNING]
====
SSL 버전 2와 버전 3에는 알려진 취약성 문제가 있습니다. 이전 SSL 옵션 대신 TLS 버전 1.2 및 1.3을 사용할 것을 적극 권장합니다. 이는 [.filename]#ssl.conf#에서 다음 옵션을 설정하여 수행할 수 있습니다:
====


[.programlisting]
....
SSLProtocol all -SSLv3 -SSLv2 +TLSv1.2 +TLSv1.3
SSLProxyProtocol all -SSLv2 -SSLv3 -TLSv1 -TLSv1.1
....

웹 서버에서 SSL 구성을 완료하려면 다음 줄의 주석 처리를 해제하여 재시작 또는 재로딩 중에 구성이 Apache로 가져올 수 있도록 합니다:

[.programlisting]
....
# Secure (SSL/TLS) connections
Include etc/apache24/extra/httpd-ssl.conf
....

Apache에서 SSL을 완전히 지원하려면 [.filename]#httpd.conf#에서 다음 줄도 주석 처리하지 않아야 합니다:

[.programlisting]
....
LoadModule authn_socache_module libexec/apache24/mod_authn_socache.so
LoadModule socache_shmcb_module libexec/apache24/mod_socache_shmcb.so
LoadModule ssl_module libexec/apache24/mod_ssl.so
....

다음 단계는 인증 기관과 협력하여 시스템에 적절한 인증서를 설치하는 것입니다. 이렇게 하면 사이트에 대한 신뢰 체인이 설정되고 자체 서명 인증서에 대한 경고를 방지할 수 있습니다.

==== [.filename]#mod_perl#

[.filename]#mod_perl#모듈을 사용하면 Perl로 Apache 모듈을 작성할 수 있습니다. 또한 서버에 내장된 영구 인터프리터를 사용하면 외부 인터프리터를 시작해야 하는 오버헤드와 Perl 시작 시간에 따른 불이익을 피할 수 있습니다.

[.filename]#mod_perl#은 package:www/mod_perl2[] 패키지 또는 포트를 사용하여 설치할 수 있습니다. 이 모듈 사용에 대한 설명서는 http://perl.apache.org/docs/2.0/index.html[http://perl.apache.org/docs/2.0/index.html]에서 확인할 수 있습니다.

==== [.filename]#mod_php#

_PHP: 하이퍼텍스트 전처리기 (Hypertext Preprocessor, PHP)_는 웹 개발에 특히 적합한 범용 스크립트 언어입니다. HTML에 포함될 수 있는 이 구문은 웹 개발자가 동적으로 생성된 웹페이지를 빠르게 작성할 수 있도록 C, Java(TM), Perl을 기반으로 작성되었습니다.

Apache용 PHP 및 해당 언어로 작성된 기타 기능에 대한 지원은 해당 포트를 설치하여 추가할 수 있습니다.

지원되는 모든 버전은 `pkg`를 사용하여 패키지 데이터베이스를 검색하세요:

[source, shell]
....
# pkg search php
....

제공되는 버전과 추가 기능이 포함된 목록이 표시됩니다. 구성 요소는 완전히 모듈화되어 있으므로 적절한 포트를 설치하면 기능을 사용할 수 있습니다. Apache용 PHP 버전 7.4를 설치하려면 다음 명령을 실행합니다:

[source, shell]
....
# pkg install mod_php74
....

종속성 패키지를 설치해야 하는 경우 해당 패키지도 함께 설치됩니다.

기본적으로 PHP는 활성화되지 않습니다. 활성화하려면 [.filename]#/usr/local/etc/apache24#에 있는 Apache 구성 파일에 다음 줄을 추가해야 합니다:

[.programlisting]
....
<FilesMatch "\.php$">
    SetHandler application/x-httpd-php
</FilesMatch>
<FilesMatch "\.phps$">
    SetHandler application/x-httpd-php-source
</FilesMatch>
....

또한 설정파일의 `DirectoryIndex`도 업데이트해야 하며, 변경 사항을 적용하려면 아파치를 재시작하거나 다시 로드해야 합니다.

많은 PHP 기능에 대한 지원은 `pkg`를 사용하여 설치할 수도 있습니다. 예를 들어 XML 또는 SSL에 대한 지원을 설치하려면 해당 포트를 설치하세요:

[source, shell]
....
# pkg install php74-xml php74-openssl
....

이전과 마찬가지로 변경 사항을 적용하려면 모듈만 설치한 경우에도 Apache 구성을 다시 로드해야 합니다.

구성을 다시 로드하기 위해 정상 재시작을 수행하려면 다음 명령을 실행합니다:

[source, shell]
....
# apachectl graceful
....

설치가 완료되면 설치된 PHP 지원 모듈과 빌드의 환경 정보를 얻는 방법에는 두 가지가 있습니다. 첫 번째는 전체 PHP 바이너리를 설치한 후 명령을 실행하여 정보를 얻는 것입니다:

[source, shell]
....
# pkg install php74
....


[source, shell]
....
# php -i |less
....

출력량을 쉽게 소화하기 위해 `more` 또는 `less`와 같은 호출기로 출력을 전달해야 합니다.

마지막으로, PHP의 전역 구성을 변경하려면 [.filename]#/usr/local/etc/php.ini#에 잘 문서화된 파일이 설치되어 있습니다. 설치 시에는 이 파일이 존재하지 않는데, 그 이유는 [.filename]#php.ini-development#와 [.filename]#php.ini-production# 두 가지 버전 중 하나를 선택할 수 있기 때문입니다. 이는 관리자의 배포를 돕기 위한 시작점입니다.

==== HTTP2 지원

`pkg`로 포트를 설치하면 HTTP2 프로토콜에 대한 아파치 지원이 기본적으로 포함됩니다. 새 버전의 HTTP에는 웹 사이트에 대한 단일 연결을 활용하여 전체 TCP 연결의 왕복 횟수를 줄이는 등 이전 버전에 비해 많은 개선 사항이 포함되어 있습니다. 또한 패킷 헤더 데이터가 압축되고 HTTP2는 기본적으로 암호화를 요구합니다.

Apache가 HTTP2만 사용하도록 설정된 경우 웹 브라우저는 안전한 암호화된 HTTPS 연결을 요구합니다. Apache가 두 버전을 모두 사용하도록 구성된 경우 연결 중에 문제가 발생하면 HTTP1.1이 대체 옵션으로 간주됩니다.

이 변경은 관리자의 권한이 필요하지만, 이는 모두에게 더 안전한 인터넷을 제공하는 긍정적인 변화입니다. 이 변경은 현재 SSL 및 TLS를 구현하지 않는 사이트에만 필요합니다.

[NOTE]
====
이 구성은 TLS 지원을 포함한 이전 섹션에 따라 달라집니다. 이 구성을 계속하기 전에 해당 지침을 따르는 것이 좋습니다.
====

[.filename]#/usr/local/etc/apache24/httpd.conf#에서 해당 줄을 주석 처리해 http2 모듈을 활성화하여 프로세스를 시작하고, mpm_prefork 모듈은 HTTP2를 지원하지 않으므로 mpm_event로 대체합니다.

[.programlisting]
....
LoadModule http2_module libexec/apache24/mod_http2.so
LoadModule mpm_event_module libexec/apache24/mod_mpm_event.so
....

[NOTE]
====
사용할 수 있는 별도의 [.filename]#mod_http2# 포트가 있습니다. 이 포트는 번들로 제공되는 [.filename]#apache24# 포트와 함께 설치된 모듈보다 더 빠르게 보안 및 버그 수정을 제공하기 위해 존재합니다. HTTP2 지원에는 필요하지 않지만 사용할 수 있습니다. 이 모듈을 설치하면 Apache 구성에서 [.filename]#mod_h2.so# 대신 [.filename]#mod_http2.so#를 사용해야 합니다.
====

아파치에서 HTTP2를 구현하는 방법은 두 가지가 있는데, 한 가지 방법은 모든 사이트와 시스템에서 실행되는 각 VirtualHost에 전역적으로 적용하는 것입니다. HTTP2를 전역적으로 사용하려면 ServerName 지시어 아래에 다음 줄을 추가합니다:

[.programlisting]
....
Protocols h2 http/1.1
....

[NOTE]
====
HTTP2를 평문으로 사용하려면 [.filename]#httpd.conf#에 h2h2chttp/1.1을 사용하세요.
====

여기에 h2c를 사용하면 평문으로 HTTP2 데이터가 시스템에 전달될 수 있지만 권장되지는 않습니다. 또한 여기에 http/1.1을 사용하면 시스템에서 필요한 경우 HTTP1.1 버전의 프로토콜로 폴백할 수 있습니다.

개별 가상 호스트에 대해 HTTP2를 사용 설정하려면 VirtualHost 지시어 내에 [.filename]#httpd.conf# 또는 [.filename]#httpd-ssl.conf#에 동일한 줄을 추가합니다.

`apachectl`[parameter]#reload# 명령을 사용하여 구성을 다시 로드하고 호스팅된 페이지 중 하나를 방문한 후 다음 방법 중 하나를 사용하여 구성을 테스트합니다:

[source, shell]
....
# grep "HTTP/2.0" /var/log/httpd-access.log
....

그러면 다음과 비슷한 결과가 반환됩니다:

[.programlisting]
....
192.168.1.205 - - [18/Oct/2020:18:34:36 -0400] "GET / HTTP/2.0" 304 -
192.0.2.205 - - [18/Oct/2020:19:19:57 -0400] "GET / HTTP/2.0" 304 -
192.0.0.205 - - [18/Oct/2020:19:20:52 -0400] "GET / HTTP/2.0" 304 -
192.0.2.205 - - [18/Oct/2020:19:23:10 -0400] "GET / HTTP/2.0" 304 -
....

다른 방법은 웹 브라우저에 내장된 사이트 디버거 또는 `tcpdump`를 사용하는 것이지만, 두 가지 방법중 어느 것을 사용하든 이 문서의 범위를 벗어납니다.

[.filename]#mod_proxy_http2.so# 모듈을 사용하여 HTTP2 역방향 프록시 연결을 지원합니다. ProxyPass 또는 RewriteRules [P] 문을 구성할 때는 연결에 h2://를 사용해야 합니다.

=== 동적 웹사이트

동적 웹 콘텐츠를 만드는 데 mod_perl 및 mod_php 외에도 다른 언어를 사용할 수 있습니다. 여기에는 장고(Django)와 루비 온 레일즈(Ruby on Rails)가 포함됩니다.

==== 장고 (Django)

Django는 개발자가 고성능의 우아한 웹 애플리케이션을 빠르게 작성할 수 있도록 설계된 BSD 라이선스 프레임워크입니다. 이 프레임워크는 객체 관계형 매퍼를 제공하여 데이터 유형을 Python 객체로 개발할 수 있도록 합니다. 개발자가 SQL을 작성할 필요 없이 이러한 객체에 대한 풍부한 동적 데이터베이스 액세스 API가 제공됩니다. 또한 확장 가능한 템플릿 시스템을 제공하여 애플리케이션의 로직이 HTML 프레젠테이션과 분리되도록 합니다.

Django는 [.filename]#mod_python# 및 SQL 데이터베이스 엔진에 의존합니다. FreeBSD에서는 package:www/py-django[] 포트가 자동으로 [.filename]#mod_python#을 설치하며, 기본값은 SQLite이나 PostgreSQL, MySQL 또한 지원합니다. 데이터베이스 엔진을 변경하려면 [.filename]#/usr/ports/www/py-django# 내에서 `make config`를 입력한 다음 포트를 설치합니다.

Django가 설치된 후, 애플리케이션이 내장된 Python 인터프리터를 사용하려면 Apache 구성과 함께 프로젝트 디렉터리가 필요합니다. 이 인터프리터는 사이트의 특정 URL에 대해 애플리케이션을 호출하는 데 사용됩니다.

특정 URL에 대한 요청을 웹 애플리케이션에 전달하도록 Apache를 구성하려면 프로젝트 디렉터리의 전체 경로를 지정하여 [.filename]#httpd.conf#에 다음을 추가합니다:

[.programlisting]
....
<Location "/">
    SetHandler python-program
    PythonPath "['/dir/to/the/django/packages/'] + sys.path"
    PythonHandler django.core.handlers.modpython
    SetEnv DJANGO_SETTINGS_MODULE mysite.settings
    PythonAutoReload On
    PythonDebug On
</Location>
....

Django의 사용 방법에 대한 자세한 내용은 https://docs.djangoproject.com[https://docs.djangoproject.com]를 참조하세요.

==== 루비 온 레일즈 (Ruby on Rails)

Ruby on Rails는 전체 개발 스택을 제공하는 또 다른 오픈 소스 웹 프레임워크입니다. 웹 개발자의 생산성을 높이고 강력한 애플리케이션을 빠르게 작성할 수 있도록 최적화되어 있습니다. FreeBSD에서는 package:www/rubygem-rails[] 패키지 또는 포트를 사용하여 설치할 수 있습니다.

루비 온 레일즈 사용 방법에 대한 자세한 내용은 http://guides.rubyonrails.org[http://guides.rubyonrails.org]를 참조하세요.

[[network-ftp]]
== 파일 전송 프로토콜 (File Transfer Protocol, FTP)

파일 전송 프로토콜(FTP)은 사용자에게 FTP 서버와 파일을 주고받을 수 있는 간단한 방법을 제공합니다. FreeBSD는 기본 시스템에 FTP 서버 소프트웨어인 ftpd를 포함하고 있습니다.

FreeBSD는 FTP 서버에 대한 액세스를 제어하기 위한 몇 가지 구성 파일을 제공합니다. 이 섹션에서는 이러한 파일들을 설명합니다. 내장된 FTP 서버에 대한 자세한 내용은 man:ftpd[8]를 참고하세요.

=== 구성

가장 중요한 구성 단계는 어떤 계정이 FTP 서버에 접근하도록 허용할지 결정하는 것입니다. FreeBSD 시스템에는 FTP 액세스가 허용되지 않아야 하는 시스템 계정이 많이 있습니다. FTP 액세스가 허용되지 않는 사용자 목록은 [.filename]#/etc/ftpusers#에서 찾을 수 있습니다. 기본적으로 시스템 계정이 포함됩니다. FTP 액세스가 허용되지 않아야 하는 사용자를 추가로 추가할 수 있습니다.

경우에 따라 일부 사용자의 FTP 사용을 완전히 막지 않고 액세스를 제한하는 것이 바람직할 수 있습니다. man:ftpchroot[5]에 설명된 대로 [.filename]#/etc/ftpchroot#를 생성하면 이 작업을 수행할 수 있습니다. 이 파일에는 FTP 액세스 제한이 적용되는 사용자 및 그룹이 나열됩니다.

서버에 대한 익명의 FTP 액세스를 활성화하려면, FreeBSD 시스템에서 `ftp`라는 사용자를 만드세요. 그러면 사용자는 `ftp` 또는 `anonymous`라는 사용자 이름으로 FTP 서버에 로그온할 수 있습니다. 비밀번호를 묻는 메시지가 표시되면 어떤 입력도 허용되지만, 관례에 따라 이메일 주소를 비밀번호로 사용해야 합니다. FTP 서버는 익명 사용자가 로그인할 때 man:chroot[2]를 호출하여 `ftp` 사용자의 홈 디렉토리로만 액세스를 제한합니다.

FTP 클라이언트에 표시할 환영 메시지를 지정하기 위해 만들 수 있는 텍스트 파일은 두 가지가 있습니다. 로그인 프롬프트가 표시되기 전에 [.filename]#/etc/ftpwelcome#의 내용이 사용자에게 표시됩니다. 로그인에 성공하면 [.filename]#/etc/ftpmotd#의 내용이 표시됩니다. 이 파일의 경로는 로그인 환경에 상대적이므로 익명 사용자의 경우 [.filename]#~ftp/etc/ftpmotd#의 내용이 표시됩니다.

FTP 서버가 구성되면 [.filename]#/etc/rc.conf#에서 적절한 변수를 설정하여 부팅 중에 서비스를 시작하세요:

[.programlisting]
....
ftpd_enable="YES"
....

서비스를 바로 시작하려면:

[source, shell]
....
# service ftpd start
....

키보드 타이핑으로 FTP 서버의 연결을 테스트하려면:

[source, shell]
....
% ftp localhost
....

ftpd 데몬은 man:syslog[3]를 사용하여 메시지를 기록합니다. 기본적으로 시스템 로그 데몬은 [.filename]#/var/log/xferlog#에 FTP와 관련된 메시지를 기록합니다. FTP 로그의 위치는 [.filename]#/etc/syslog.conf#에서 다음 줄을 변경하여 수정할 수 있습니다:

[.programlisting]
....
ftp.info      /var/log/xferlog
....


[NOTE]
====
익명 FTP 서버를 실행할 때 발생할 수 있는 잠재적인 문제에 유의하세요. 특히 익명 사용자가 파일을 업로드할 수 있도록 허용하기 전에 다시 한 번 생각하세요. FTP 사이트가 라이선스가 없는 상용 소프트웨어 거래의 장이 되거나 더 심각한 문제가 발생할 수 있습니다. 익명 FTP 업로드가 필요한 경우에는 관리자가 검토할 때까지 다른 익명 사용자가 해당 파일을 읽을 수 없도록 권한을 확인해야 합니다.
====

[[network-samba]]
== Microsoft(R) Windows(R) 클라이언트를 위한 파일 및 인쇄 서비스(Samba, 삼바)

Samba는 SMB/CIFS 프로토콜을 사용하여 파일 및 인쇄 서비스를 제공하는 인기 있는 오픈 소스 소프트웨어 패키지입니다. 이 프로토콜은 Microsoft(R) Windows(R) 시스템에 내장되어 있습니다. Samba 클라이언트 라이브러리를 설치하여 비 Windows(R) 시스템에도 사용할 수 있습니다. 이 프로토콜을 통해 클라이언트는 공유 데이터와 프린터에 액세스할 수 있습니다. 이러한 공유는 로컬 디스크 드라이브로 매핑할 수 있으며 공유 프린터를 로컬 프린터처럼 사용할 수 있습니다.

FreeBSD에서 Samba 클라이언트 라이브러리는 package:net/samba413[] 포트 또는 패키지를 사용하여 설치할 수 있습니다. 이 클라이언트는 FreeBSD 시스템이 Microsoft(R) Windows(R) 네트워크의 SMB/CIFS 공유에 액세스할 수 있는 기능을 제공합니다.

또한 동일한 package:net/samba413[] 포트 또는 패키지를 설치하여 FreeBSD 시스템이 Samba 서버로 작동하도록 구성할 수도 있습니다. 이를 통해 관리자는 Microsoft(R) Windows(R) 또는 Samba 클라이언트 라이브러리를 실행하는 클라이언트에서 액세스할 수 있는 SMB/CIFS 공유를 FreeBSD 시스템에 생성할 수 있습니다.

=== 서버 구성

Samba는 [.filename]#/usr/local/etc/smb4.conf#에 구성내용을 저장합니다. 이 파일을 생성해야 Samba를 사용할 수 있습니다.

여기에는 작업 그룹의 Windows(R) 클라이언트와 디렉터리 및 프린터를 공유하기 위한 간단한 [.filename]#smb4.conf#이 나와 있습니다. LDAP 또는 Active Directory와 관련된 더 복잡한 설정의 경우, man:samba-tool[8]을 사용하여 초기 [.filename]#smb4.conf#를 만드는 것이 더 쉽습니다.

[.programlisting]
....
[global]
workgroup = WORKGROUP
server string = Samba Server Version %v
netbios name = ExampleMachine
wins support = Yes
security = user
passdb backend = tdbsam

# Example: share /usr/src accessible only to 'developer' user
[src]
path = /usr/src
valid users = developer
writable  = yes
browsable = yes
read only = no
guest ok = no
public = no
create mask = 0666
directory mask = 0755
....


==== 글로벌 설정

네트워크를 설명하는 설정은 [.filename]#/usr/local/etc/smb4.conf#에 기록됩니다:

`workgroup`::
서비스를 제공할 작업 그룹의 이름.

`netbios name`::
Samba 서버로 알려진 NetBIOS 이름입니다. 기본적으로 호스트 DNS 이름의 첫 번째 구성 요소와 동일합니다.

`server string`::
`net view` 출력에 표시될 내용과 서버에 대한 설명을 찾아 표시하기 위한 네트워크 툴.

`wins support`::
Samba가 WINS 서버로 작동할지 여부. 네트워크에 있는 둘 이상의 서버에서 WINS 지원을 사용하도록 설정하지 마세요.


==== 보안 세팅

[.filename]#/usr/local/etc/smb4.conf#에서 가장 중요한 설정은 보안 모델과 백엔드 비밀번호 형식입니다. 이 지시어는 옵션을 제어합니다:

`security`::
가장 일반적인 설정은 `security = share`와 `security = user`입니다. 클라이언트가 FreeBSD 머신의 사용자 이름과 동일한 사용자 이름을 사용하는 경우, 사용자 수준 보안(user level security)을 사용해야 합니다. 이것은 기본 보안 정책이며 클라이언트가 공유 리소스에 액세스하기 전에 먼저 로그온해야 합니다.
+
공유 수준 보안(share level security)에서는 클라이언트가 공유 리소스에 연결을 시도하기 전에 유효한 사용자 이름과 비밀번호로 서버에 로그인할 필요가 없습니다. 이는 이전 버전의 Samba의 기본 보안 모델이었습니다.

`passdb backend`::
Samba에는 여러 가지 백엔드 인증 모델이 있습니다. 클라이언트는 LDAP, NIS+, SQL 데이터베이스 또는 수정된 비밀번호 파일로 인증될 수 있습니다. 권장 인증 방법인 `tdbsam`은 간단한 네트워크에 이상적이며 여기에서 다루고 있습니다. 규모가 크거나 복잡한 네트워크의 경우 `ldapsam`을 사용하는 것이 좋습니다. `smbpasswd`는 이전 기본값이었으나 지금은 더 이상 사용되지 않습니다.

==== Samba 사용자

Windows(R) 클라이언트가 공유에 액세스하려면 FreeBSD 사용자 계정이 `SambaSAMAccount` 데이터베이스에 매핑되어 있어야 합니다. man:pdbedit[8]을 사용하여 기존 FreeBSD 사용자 계정을 매핑합니다:

[source, shell]
....
# pdbedit -a username
....

이 섹션에서는 가장 일반적으로 사용되는 설정만 언급했습니다. 사용 가능한 구성 옵션에 대한 자세한 내용은 https://wiki.samba.org[공식 삼바 위키]를 참조하세요.

=== Samba 시작하기

부팅 시 Samba를 활성화하려면 [.filename]#/etc/rc.conf#에 다음 줄을 추가합니다:

[.programlisting]
....
samba_server_enable="YES"
....

바로 Samba를 시작하려면:

[source, shell]
....
# service samba_server start
Performing sanity check on Samba configuration: OK
Starting nmbd.
Starting smbd.
....

Samba는 세 개의 별도 데몬으로 구성됩니다. nmbd 및 smbd 데몬은 모두 `samba_enable`에 의해 시작됩니다. 만약 winbind 이름 확인이 필요하다면 설정합니다:

[.programlisting]
....
winbindd_enable="YES"
....

Samba는 언제든지 다음 명령으로 중지할 수 있습니다:

[source, shell]
....
# service samba_server stop
....

Samba는 Microsoft(R) Windows(R) 네트워크와 광범위하게 통합할 수 있는 기능을 갖춘 복잡한 소프트웨어 제품군입니다. 여기에 설명된 기본 구성 이외의 기능에 대한 자세한 내용은 https://www.samba.org[https://www.samba.org]를 참조하세요.

[[network-ntp]]
== NTP로 시간 동기화

시간이 지남에 따라 컴퓨터의 시계는 오차가 생기기 쉽습니다. 이는 많은 네트워크 서비스에서 네트워크상의 컴퓨터가 정확하고 동일한 시간을 공유해야 하기 때문에 문제가 됩니다. 파일 타임스탬프가 일관성을 유지하려면 정확한 시간이 필요합니다. NTP(네트워크 시간 프로토콜)는 네트워크에서 시계 정확도를 제공하는 하나의 방법입니다.

FreeBSD에는 다른 NTP 서버를 쿼리하여 해당 컴퓨터의 시계를 동기화하거나 네트워크의 다른 컴퓨터에 시간 서비스를 제공하도록 구성할 수 있는 man:ntpd[8]가 포함되어 있습니다.

이 섹션에서는 FreeBSD에서 ntpd를 구성하는 방법을 설명합니다. 자세한 문서는 [.filename]#/usr/share/doc/ntp/#에서 HTML 형식으로 찾을 수 있습니다.

=== NTP 구성

FreeBSD에서는 내장된 ntpd를 사용하여 시스템 시계를 동기화할 수 있습니다. ntpd는 다음 섹션에 자세히 설명된 대로 man:rc.conf[5] 변수와 [.filename]#/etc/ntp.conf#를 사용하여 구성합니다.

NTP는 UDP 패킷을 사용하여 네트워크 피어와 통신합니다. 컴퓨터와 NTP 피어 사이의 모든 방화벽은 포트 123에서 UDP 패킷의 입출력을 허용하도록 구성되어야 합니다.

==== [.filename]#/etc/ntp.conf# 파일

ntpd는 [.filename]#/etc/ntp.conf#를 읽어 쿼리할 NTP 서버를 결정합니다. 서버 중 하나에 연결할 수 없거나 시계가 불안정할 경우를 대비하여 여러 개의 NTP 서버를 선택하는 것이 좋습니다. ntpd는 응답을 받을때 신뢰성이 낮은 서버보다 안정적인 서버를 선호합니다. 쿼리되는 서버는 네트워크에 로컬로 연결되어 있거나, ISP에서 제공하거나, http://support.ntp.org/bin/view/Servers/WebHome[ 공개적으로 액세스할 수 있는 NTP 서버의 온라인 목록]에서 선택할 수 있습니다. 공용 NTP 서버를 선택할 때는 지리적으로 가까운 서버를 선택하고 해당 서버의 사용 정책을 검토하세요. `pool` 구성 키워드는 서버 풀에서 하나 이상의 서버를 선택합니다. http://support.ntp.org/bin/view/Servers/NTPPoolServers[ 공개적으로 액세스 가능한 NTP 풀의 온라인 목록]은 지역별로 정리되어 있습니다. 또한, FreeBSD는 프로젝트가 후원하는 풀인 `0.freebsd.pool.ntp.org`를 제공합니다.

.[.filename]#/etc/ntp.conf# 예시
[example]
====
다음은 [.filename]#ntp.conf# 파일의 간단한 예입니다. 이 파일은 공개적으로 액세스할 수 있는 네트워크 연결에서 작동하기 위해 권장되는 `restrict` 옵션이 포함되어 있으므로 그대로 사용해도 안전합니다.

[.programlisting]
....

# Disallow ntpq control/query access.  Allow peers to be added only
# based on pool and server statements in this file.
restrict default limited kod nomodify notrap noquery nopeer
restrict source  limited kod nomodify notrap noquery

# Allow unrestricted access from localhost for queries and control.
restrict 127.0.0.1
restrict ::1

# Add a specific server.
server ntplocal.example.com iburst

# Add FreeBSD pool servers until 3-6 good servers are available.
tos minclock 3 maxclock 6
pool 0.freebsd.pool.ntp.org iburst

# Use a local leap-seconds file.
leapfile "/var/db/ntpd.leap-seconds.list"
....

====

이 파일의 형식은 man:ntp.conf[5]에 설명되어 있습니다. 아래 설명은 위의 샘플 파일에 사용된 키워드에 대한 간략한 개요를 제공합니다.

기본적으로 NTP 서버는 모든 네트워크 호스트에서 액세스할 수 있습니다. `restrict` 키워드는 서버에 액세스할 수 있는 시스템을 제어합니다. 여러 개의 `restrict` 항목이 지원되며, 각 항목은 이전 문장에서 제공된 제한을 구체화합니다. 예제에 표시된 값은 로컬 시스템에는 전체 쿼리 및 제어 액세스 권한을 부여하고 원격 시스템에는 시간 쿼리 기능만 허용합니다. 자세한 내용은 man:ntp.conf[5]의 `Access Control Support` 하위 섹션을 참조하세요.

`server` 키워드는 쿼리할 단일 서버를 지정합니다. 파일에는 여러 서버 키워드를 포함할 수 있으며, 각 줄에 하나의 서버가 나열됩니다. `pool` 키워드는 서버 풀을 지정합니다. 이 풀에 있는 하나 이상의 서버를 필요에 따라 추가하여 `tos minclock` 값을 사용하여 지정한 피어 수에 도달합니다. `iburst` 키워드는 접속이 처음 설정될 때 서버와 8번의 빠른 패킷 교환을 수행하여 시스템 시간을 빠르게 동기화하도록 ntpd에 지시합니다.

`leapfile` 키워드는 윤초에 대한 정보가 포함된 파일의 위치를 지정합니다. 이 파일은 man:periodic[8]에 의해 자동으로 업데이트됩니다. 이 키워드로 지정한 파일 위치는 [.filename]#/etc/rc.conf#의 `ntp_db_leapfile` 변수에 설정된 위치와 일치해야 합니다.

==== [.filename]#/etc/rc.conf#의 NTP 항목

부팅할 때 ntpd를 시작하려면 `ntpd_enable=YES`를 설정합니다. [.filename]#/etc/rc.conf#에 `ntpd_enable=YES`를 추가한 후 다음 명령을 통해 시스템을 재부팅하지 않고도 ntpd를 즉시 시작할 수 있습니다:

[source, shell]
....
# service ntpd start
....

ntpd를 사용하려면 `ntpd_enable`만 설정해야 합니다. 아래 나열된 [.filename]#rc.conf# 변수도 필요에 따라 설정할 수 있습니다.

`ntpd_sync_on_start=YES`를 설정하면 시작 시 한 번씩 ntpd가 시계를 원하는대로 단계적인 조정을 할 수 있습니다. 일반적으로 시계가 1000초 이상 꺼져 있으면 ntpd는 오류 메시지를 기록하고 종료합니다. 이 옵션은 배터리로 백업되는 실시간 시계가 없는 시스템에서 특히 유용합니다.

메모리 부족(OOM) 상태에서 복구를 시도하는 시스템에서, ntpd 데몬이 종료되지 않도록 보호하려면 `ntpd_oomprotect=YES`를 설정합니다.

대체(또는 백업) [.filename]#ntp.conf# 파일의 위치로 `ntpd_config=`를 설정하세요

필요에 따라 다른 ntpd 플래그를 포함하도록 `ntpd_flags=`를 설정하되, [.filename]#/etc/rc.d/ntpd#에서 내부적으로 관리하는 다음의 플래그는 사용하지 않도록 합니다:

* `-p` (pid 파일 위치)
* `-c` (`ntpd_config=` 대신 설정)


==== ntpd와 권한없는 `ntpd` 사용자

FreeBSD에서 권한이 없는 사용자도 ntpd를 시작하고 실행할 수 있습니다. 그렇게 하려면 man:mac_ntpd[4] 정책 모듈이 필요합니다. [.filename]#/etc/rc.d/ntpd# 시작 스크립트는 먼저 NTP 구성을 검사합니다. 가능하면 `mac_ntpd` 모듈을 로드한 다음 권한이 없는 사용자 `ntpd`(user id 123)로 ntpd를 시작합니다. 파일 및 디렉터리 액세스 문제를 방지하기 위해, 시작 스크립트는 구성에 파일 관련 옵션이 포함되어 있는 경우 자동으로 `ntpd`로 ntpd를 시작하지 않습니다.

`ntpd_flags`에 다음 내용이 하나라도 있으면, `ntpd` 사용자를 실행하기 위해 아래에 설명된 대로 수동구성을 해야 합니다:

* -f 또는 --driftfile
* -i 또는 --jaildir
* -k 또는 --keyfile
* -l 또는 --logfile
* -s 또는 --statsdir

[.filename]#ntp.conf#에 다음 내용이 하나라도 있으면, `ntpd` 사용자를 실행하기 위해 아래에 설명된 대로 수동구성을 해야 합니다:

* crypto
* driftfile
* key
* logdir
* statsdir

사용자 `ntpd`로 실행되도록 ntpd를 수동구성하려면 다음을 반드시 설정해야 합니다:

* ‘ntpd’ 사용자에게, 구성에 지정된 모든 파일과 디렉터리에 대한 액세스 권한이 있는지 확인합니다.
* `mac_ntpd` 모듈을 커널에 로드하거나 컴파일할 수 있도록 준비합니다. 자세한 내용은 man:mac_ntpd[4]를 참조하십시오.
* [.filename]#/etc/rc.conf#에 `ntpd_user="ntpd"`를 설정합니다

=== PPP 연결로 NTP 사용하기

NTP가 제대로 작동하기 위해 인터넷에 영구적으로 연결할 필요는 없습니다. 그러나 PPP 연결이 필요 시 전화 걸기를 하도록 구성된 경우, NTP 트래픽이 전화 걸기를 트리거하거나 연결을 계속 유지하지 못하도록 해야 합니다. 이는 [.filename]#/etc/ppp/ppp.conf#에서 `filter` 지시어를 사용하여 구성할 수 있습니다. 예를 들어:

[.programlisting]
....
set filter dial 0 deny udp src eq 123
# Prevent NTP traffic from initiating dial out
set filter dial 1 permit 0 0
set filter alive 0 deny udp src eq 123
# Prevent incoming NTP traffic from keeping the connection open
set filter alive 1 deny udp dst eq 123
# Prevent outgoing NTP traffic from keeping the connection open
set filter alive 2 permit 0/0 0/0
....

보다 자세한 내용은 man:ppp[8]의 `PACKET FILTERING` 섹션과 [.filename]#/usr/share/exples/ppp/#의 예제를 참조하세요.

[NOTE]
====
일부 인터넷 액세스 제공업체(ISP)는 낮은 번호의 포트를 차단하여 응답이 컴퓨터에 도달하지 않아 NTP가 작동하지 않도록 합니다.
====

[[network-iscsi]]
== iSCSI 초기자(Initiator)와 타깃 구성

iSCSI는 네트워크를 통해 스토리지를 공유하는 방법입니다. 파일 시스템 수준에서 작동하는 NFS와 달리 iSCSI는 블록 장치 수준에서 작동합니다.

iSCSI 용어로는 스토리지 공유를 제공하는 시스템을 _타깃_이라고 합니다. 스토리지는 물리적 디스크일 수도 있고, 여러 디스크 또는 물리적 디스크의 일부를 나타내는 영역일 수도 있습니다. 예를 들어, 디스크가 ZFS로 포맷된 경우 zvol을 생성하여 iSCSI 스토리지로 사용할 수 있습니다.

iSCSI 스토리지에 액세스하는 클라이언트를 _초기자(Initiator)_라고 합니다. 초기자가 iSCSI를 통해 사용할 수 있는 스토리지는 LUN이라고 알려진 포맷되지 않은 원시 디스크 형태로 나타납니다. 디스크의 장치 노드는 [.filename]#/dev/#에 나타나며, 장치를 별도로 포맷하고 마운트해야 합니다.

FreeBSD는 기본 커널 기반 iSCSI 타깃과 초기자를 제공합니다. 이 섹션에서는 FreeBSD 시스템을 타깃 또는 초기자로 구성하는 방법을 설명합니다.

[[network-iscsi-target]]
=== iSCSI 타깃 구성하기

iSCSI 타깃을 구성하려면 [.filename]#/etc/ctl.conf# 구성 파일을 만들고 [.filename]#/etc/rc.conf#에 man:ctld[8] 데몬이 부팅 시 자동으로 시작되도록 하는 명령을 추가한 다음 데몬을 시작하세요.

다음은 간단한 [.filename]#/etc/ctl.conf# 구성 파일의 예입니다. 이 파일에서 사용할 수 있는 옵션에 대한 자세한 설명은 man:ctl.conf[5]를 참조하세요.

[.programlisting]
....
portal-group pg0 {
	discovery-auth-group no-authentication
	listen 0.0.0.0
	listen [::]
}

target iqn.2012-06.com.example:target0 {
	auth-group no-authentication
	portal-group pg0

	lun 0 {
		path /data/target0-0
		size 4G
	}
}
....

첫 번째 항목은 `pg0` 포털 그룹을 정의합니다. 포털 그룹은 man:ctld[8] 데몬이 수신 대기할 네트워크 주소를 정의합니다. `discovery-auth-group no-authentication (발견-인증-그룹인증 없음)` 항목은 모든 초기자가 인증 없이 iSCSI 대상 검색을 수행할 수 있도록 허용됨을 나타냅니다. 세 번째와 네 번째 줄은 기본 포트 3260의 모든 IPv4(`listen 0.0.0.0`) 및 IPv6(`listen [::]`) 주소에서 수신 대기하도록 man:ctld[8]을 구성합니다.

`default`이라는 기본 포털 그룹이 있으므로 포털 그룹을 정의할 필요가 없습니다. 이 경우 `default`와 `pg0`의 차이점은 `default`를 사용하면 대상 검색이 항상 거부되는 반면 `pg0`을 사용하면 항상 허용된다는 점입니다.

두 번째 항목은 단일 타깃을 정의합니다. 타깃은 두 가지 의미로 사용할 수 있습니다. iSCSI를 제공하는 컴퓨터 또는 명명된 LUN 그룹입니다. 이 예에서는 후자의 의미를 사용하며, 여기서 `iqn.2012-06.com.example:target0`은 타깃 이름입니다. 이 타깃 이름은 테스트 목적으로 적합합니다. 실제 사용하려면 `com.example`을 실제 도메인 이름으로 변경합니다. `2012-06`은 해당 도메인 네임의 제어권을 획득한 연도와 월을 나타내며, `target0`은 어떤 값이라도 사용할 수 있습니다. 이 구성 파일에는 원하는 수의 타깃을 정의할 수 있습니다.

`auth-group no-authentication`줄은, 모든 초기자가 지정된 타깃에 연결할 수 있도록 허용하고 `portal-group pg0`은 `pg0` 포털 그룹을 통해 타깃에 연결할 수 있도록 합니다.

다음 섹션에서는 LUN을 정의합니다. 초기자에게는 각 LUN이 별도의 디스크 장치로 표시됩니다. 각각의 타깃에 대해 여러 개의 LUN을 정의할 수 있습니다. 각 LUN은 숫자로 식별되며, 여기서 LUN 0은 필수입니다. `path /data/target0-0` 줄은 LUN을 백업한 파일 또는 zvol의 전체 경로를 정의합니다. 이 경로는 man:ctld[8]을 시작하기 전에 반드시 있어야 합니다. 두 번째 줄은 선택 사항이며 LUN의 크기를 지정합니다.

다음으로, 부팅 시 man:ctld[8] 데몬이 시작되도록 하려면 [.filename]#/etc/rc.conf#에 다음 내용을 추가합니다:

[.programlisting]
....
ctld_enable="YES"
....

바로 man:ctld[8]을 시작하려면 다음 명령을 실행하세요:

[source, shell]
....
# service ctld start
....

man:ctld[8] 데몬이 시작되면 [.filename]#/etc/ctl.conf#를 읽습니다. 데몬이 시작된 후 이 파일을 편집하는 경우 다음 명령을 사용하면 변경 사항이 즉시 적용됩니다:

[source, shell]
....
# service ctld reload
....


==== 인증

이전 예는 인증을 사용하지 않아, 모든 대상에 대한 전체 액세스 권한을 누구에게나 부여하므로 본질적으로 안전하지 않습니다. 대상에 액세스하기 위해 사용자 이름과 비밀번호를 요구하려면 다음과 같이 구성을 수정합니다:

[.programlisting]
....
auth-group ag0 {
	chap username1 secretsecret
	chap username2 anothersecret
}

portal-group pg0 {
	discovery-auth-group no-authentication
	listen 0.0.0.0
	listen [::]
}

target iqn.2012-06.com.example:target0 {
	auth-group ag0
	portal-group pg0
	lun 0 {
		path /data/target0-0
		size 4G
	}
}
....

`auth-group` 섹션은 사용자 이름과 암호 쌍을 정의합니다. `iqn.2012-06.com.example:target0`에 연결하려는 초기자는 먼저 정의된 사용자 이름과 암호를 제공해야 합니다. 그러나 단순 타깃 검색은 인증 없이도 허용됩니다. 타깃 검색에도 인증을 요구하려면 `discovery-auth-group`을 `no-authentication` 대신 정의된 `auth-group` 이름으로 설정합니다.

모든 초기자에 대해 하나의 타깃만 내보내는 것이 일반적입니다. 위의 구문을 줄여서 사용자 이름과 비밀번호를 타깃 항목에 직접 지정할 수 있습니다:

[.programlisting]
....
target iqn.2012-06.com.example:target0 {
	portal-group pg0
	chap username1 secretsecret

	lun 0 {
		path /data/target0-0
		size 4G
	}
}
....


[[network-iscsi-initiator]]
=== iSCSI 초기자 구성하기

[NOTE]
====
이 섹션에서 설명하는 iSCSI 초기자는 FreeBSD 10.0-RELEASE부터 지원됩니다. 이전 버전에서도 사용할 수 있는 iSCSI 초기자를 사용하려면 man:iscontrol[8]을 참조하십시오.
====

iSCSI 초기자를 사용하려면 man:iscsid[8] 데몬이 실행 중이어야 합니다. 이 데몬은 구성 파일을 사용하지 않습니다. 부팅 시 자동으로 시작하려면 [.filename]#/etc/rc.conf#에 다음 내용을 추가합니다:

[.programlisting]
....
iscsid_enable="YES"
....

바로 man:iscsid[8]을 시작하려면 다음 명령을 사용하세요:

[source, shell]
....
# service iscsid start
....

타깃에 연결할 때 [.filename]#/etc/iscsi.conf# 구성 파일을 사용하거나 사용하지 않고 연결할 수 있습니다. 이 섹션에서는 두 가지 유형의 연결을 모두 보여줍니다.

==== 구성파일 없이 타깃에 연결하기

초기자를 단일 대상에 연결하려면 포털의 IP 주소와 타깃의 이름을 지정합니다:

[source, shell]
....
# iscsictl -A -p 10.10.10.10 -t iqn.2012-06.com.example:target0
....

연결이 성공했는지 확인하려면 인자 없이 `iscsictl`을 실행합니다. 출력은 다음과 비슷해야 합니다:

[.programlisting]
....
Target name                                     Target portal   State
iqn.2012-06.com.example:target0                 10.10.10.10     Connected: da0
....

이 예제에서는 [.filename]#/dev/da0#이 연결된 LUN을 나타내며 iSCSI 세션이 성공적으로 설정되었습니다. `iqn.2012-06.com.example:target0` 타깃이 둘 이상의 LUN을 내보내는 경우 출력의 해당 섹션에 여러 장치 노드가 표시됩니다:

[source, shell]
....
Connected: da0 da1 da2.
....

모든 오류는 출력과 시스템 로그에 기록됩니다. 예를 들어, 다음 메시지는 일반적으로 man:iscsid[8] 데몬이 실행되고 있지 않음을 의미합니다:

[.programlisting]
....
Target name                                     Target portal   State
iqn.2012-06.com.example:target0                 10.10.10.10     Waiting for iscsid(8)
....

다음 메시지는 잘못된 IP 주소 또는 포트와 같은 네트워킹 문제를 나타냅니다:

[.programlisting]
....
Target name                                     Target portal   State
iqn.2012-06.com.example:target0                 10.10.10.11     Connection refused
....

이 메시지는 지정한 타깃 이름이 잘못되었다는 의미입니다:

[.programlisting]
....
Target name                                     Target portal   State
iqn.2012-06.com.example:target0                 10.10.10.10     Not found
....

이 메시지는 타깃에 인증이 필요하다는 것을 의미합니다:

[.programlisting]
....
Target name                                     Target portal   State
iqn.2012-06.com.example:target0                 10.10.10.10     Authentication failed
....

CHAP 사용자 이름과 비밀번호를 지정하려면 다음 구문을 사용합니다:

[source, shell]
....
# iscsictl -A -p 10.10.10.10 -t iqn.2012-06.com.example:target0 -u user -s secretsecret
....


==== 구성파일로 타깃에 연결하기

구성 파일을 사용하여 연결하려면 다음과 같은 내용으로 [.filename]#/etc/iscsi.conf#를 생성합니다:

[.programlisting]
....
t0 {
	TargetAddress   = 10.10.10.10
	TargetName      = iqn.2012-06.com.example:target0
	AuthMethod      = CHAP
	chapIName       = user
	chapSecret      = secretsecret
}
....

`t0`은 구성 파일 섹션의 별명을 지정합니다. 이 별명은 초기자가 사용할 구성을 지정하는 데 사용됩니다. 다른 줄은 연결 중 사용할 매개변수를 지정합니다. `TargetAddress`와 `TargetName`은 필수이고 다른 옵션은 선택 사항입니다. 이 예에서는 CHAP 사용자 이름과 비밀번호가 표시됩니다.

지정된 타깃에만 연결하기 위해 정확한 별명을 지정해 주세요:

[source, shell]
....
# iscsictl -An t0
....

구성파일에 있는 모든 타깃에 연결하기 위해서는 다음을 사용하세요:

[source, shell]
....
# iscsictl -Aa
....

초기자가 [.filename]#/etc/iscsi.conf#의 모든 타깃에 자동으로 연결되도록 하려면 [.filename]#/etc/rc.conf#에 다음을 추가합니다:

[.programlisting]
....
iscsictl_enable="YES"
iscsictl_flags="-Aa"
....
