---
description: '가상화 소프트웨어를 사용하면 동일한 컴퓨터에서 여러 운영 체제를 동시에 실행할 수 있습니다'
next: books/handbook/l10n
part: '파트 III. 시스템 관리'
path: /books/handbook/
prev: books/handbook/filesystems
showBookMenu: 'true'
tags: ["virtualization", "Parallels", "VMware", "VirtualBox", "bhyve", "XEN"]
title: '23장. 가상화'
weight: 27
---

[[virtualization]]
= 가상화
:doctype: book
:toc: macro
:toclevels: 1
:icons: font
:sectnums:
:sectnumlevels: 6
:sectnumoffset: 23
:partnums:
:source-highlighter: rouge
:experimental:
:images-path: books/handbook/virtualization/

ifdef::env-beastie[]
ifdef::backend-html5[]
:imagesdir: ../../../../images/{images-path}
endif::[]
ifndef::book[]
include::shared/authors.adoc[]
include::shared/mirrors.adoc[]
include::shared/releases.adoc[]
include::shared/attributes/attributes-{{% lang %}}.adoc[]
include::shared/{{% lang %}}/teams.adoc[]
include::shared/{{% lang %}}/mailing-lists.adoc[]
include::shared/{{% lang %}}/urls.adoc[]
toc::[]
endif::[]
ifdef::backend-pdf,backend-epub3[]
include::../../../../../shared/asciidoctor.adoc[]
endif::[]
endif::[]

ifndef::env-beastie[]
toc::[]
include::../../../../../shared/asciidoctor.adoc[]
endif::[]

[[virtualization-synopsis]]
== 요약

가상화 소프트웨어를 사용하면 동일한 컴퓨터에서 여러 운영 체제를 동시에 실행할 수 있습니다. 이러한 PC용 소프트웨어 시스템에는 가상화 소프트웨어를 실행하고 여러 개의 게스트 운영 체제를 지원하는 호스트 운영 체제가 포함되는 경우가 많습니다.

이 챕터를 읽고 나면, 여러분은:

* 호스트 운영 체제와 게스트 운영 체제의 차이점.
* 다음 가상화 플랫폼에 FreeBSD를 설치하는 방법:
** Parallels Desktop(Intel(R)-based Apple(R) macOS(R))
** VMware Fusion(Intel(R)-based Apple(R) macOS(R))
** VirtualBox(TM)(Microsoft(R) Windows(R), Intel(R)-based Apple(R) macOS(R), Linux)
** bhyve(FreeBSD)
* 가상화에서 최상의 성능을 발휘하도록 FreeBSD 시스템을 튜닝하는 방법.

이 챕터를 읽기 전에 여러분은:

* crossref:basics[basics,basics of UNIX(R) and FreeBSD]를 이해합니다.
* crossref:bsdinstall[bsdinstall,install FreeBSD]방법.
* crossref:advanced-networking[advanced-networking,set up a network connection] 방법.
* crossref:ports[ports,install additional third-party software] 방법.

[[virtualization-guest-parallelsdesktop]]
== macOS(R)용 Parallels Desktop에서 게스트로서의 FreeBSD

Parallels Desktop for Mac(R) is a commercial software product available for Intel(R) based Apple(R) Mac(R) computers running macOS(R) 10.4.6 or higher. FreeBSD is a fully supported guest operating system. Once Parallels has been installed on macOS(R), the user must configure a virtual machine and then install the desired guest operating system.

[[virtualization-guest-parallelsdesktop-install]]
=== Mac(R)의 Parallels Desktop에 FreeBSD 설치하기

The first step in installing FreeBSD on Parallels is to create a new virtual machine for installing FreeBSD. Select [.guimenuitem]#FreeBSD# as the menu:Guest OS Type[] when prompted:

image::parallels-freebsd1.png["Parallels setup wizard showing FreeBSD as chosen OS"]

Choose a reasonable amount of disk and memory depending on the plans for this virtual FreeBSD instance. 4GB of disk space and 512MB of RAM work well for most uses of FreeBSD under Parallels:

image::parallels-freebsd2.png["Parallels setup wizard showing the amount of RAM allocated"]

image::parallels-freebsd3.png["Parallels setup wizard showing the disk menu"]

image::parallels-freebsd4.png["Parallels setup wizard showing the menu for setting the disk size and type"]

image::parallels-freebsd5.png["Parallels setup wizard showing the menu for setting the disk location"]

Select the type of networking and a network interface:

image::parallels-freebsd6.png["Parallels setup wizard showing the network menu"]

image::parallels-freebsd7.png["Parallels setup wizard showing the menu with the network type options"]

Save and finish the configuration:

image::parallels-freebsd8.png["Parallels setup wizard showing the menu to configure the name of the machine and the directory where to save the configuration"]

image::parallels-freebsd9.png["Parallels setup wizard indicating that the configuration is complete and asking the user if he wants to start guest OS installation"]

After the FreeBSD virtual machine has been created, FreeBSD can be installed on it. This is best done with an official FreeBSD CD/DVD or with an ISO image downloaded from an official FTP site. Copy the appropriate ISO image to the local Mac(R) filesystem or insert a CD/DVD in the Mac(R)'s CD-ROM drive. Click on the disc icon in the bottom right corner of the FreeBSD Parallels window. This will bring up a window that can be used to associate the CD-ROM drive in the virtual machine with the ISO file on disk or with the real CD-ROM drive.

image::parallels-freebsd11.png["Parallels showing a summary of the newly created machine with information and actions to execute on the machine"]

Once this association with the CD-ROM source has been made, reboot the FreeBSD virtual machine by clicking the reboot icon. Parallels will reboot with a special BIOS that first checks if there is a CD-ROM.

image::parallels-freebsd10.png["Parallels showing the BIOS running"]

In this case it will find the FreeBSD installation media and begin a normal FreeBSD installation. Perform the installation, but do not attempt to configure Xorg at this time.

image::parallels-freebsd12.png["Parallels showing a snippet of the FreeBSD installation process"]

When the installation is finished, reboot into the newly installed FreeBSD virtual machine.

image::parallels-freebsd13.png["Parallels showing the boot of FreeBSD"]

[[virtualization-guest-parallels-configure]]
=== Parallels에서 FreeBSD 구성하기

macOS(R) X 의 Parallels에 FreeBSD를 성공적으로 설치한 후, 시스템을 최적화하기 위해 수행할 수 있는 여러 가지 구성 단계가 있습니다.

[.procedure]
. 부트 로더 변수 설정
+
가장 중요한 단계는 Parallels 환경에서 FreeBSD의 CPU 사용률을 낮추기 위해 `kern.hz` 튜너블을 낮추는 것입니다. 이는 [.filename]#/boot/loader.conf#에 다음 줄을 추가하여 수행합니다:
+
[.programlisting]
....
kern.hz=100
....
+
이 설정이 없으면 유휴 상태의 FreeBSD Parallels 게스트는 단일 프로세서 iMac(R) CPU의 약 15%를 사용하게 됩니다. 이 변경 후에는 사용량이 5%에 가까워집니다.
. 새 커널 구성 파일 생성
+
사용자 정의 커널 구성 파일에서 모든 SCSI, FireWire 및 USB 장치 드라이버를 제거할 수 있습니다. Parallels는 man:ed[4] 드라이버에서 사용하는 가상 네트워크 어댑터를 제공하므로 man:ed[4] 및 man:miibus[4]를 제외한 모든 네트워크 장치를 커널에서 제거할 수 있습니다.
. 네트워킹 구성하기
+
가장 기본적인 네트워킹 설정은 DHCP를 사용하여 가상 머신을 호스트 Mac(R)과 동일한 로컬 영역 네트워크에 연결하는 것입니다. 이 설정은 [.filename]#/etc/rc.conf#에 `ifconfig_ed0="DHCP"`를 추가하여 수행할 수 있습니다. 보다 고급 네트워킹 설정은 crossref:advanced-networking[advanced-networking,Advanced Networking]에 설명되어 있습니다.

[[virtualization-guest-vmware]]
== macOS(R)용 VMware Fusion에서 게스트로서의 FreeBSD

VMware Fusion for Mac(R) is a commercial software product available for Intel(R) based Apple(R) Mac(R) computers running macOS(R) 10.11 or higher. FreeBSD is a fully supported guest operating system. Once VMware Fusion has been installed on macOS(R), the user can configure a virtual machine and then install the desired guest operating system.

[[virtualization-guest-vmware-install]]
=== VMware Fusion에 FreeBSD 설치하기

첫 번째 단계는 가상 머신 라이브러리를 로드하는 VMware Fusion을 시작하는 것입니다. [.guimenuitem]#+->New#를 클릭하여 가상 머신을 생성합니다:

image::vmware-freebsd01.png[width=35%]

그러면 새 가상 머신 도우미가 로드됩니다. [.guimenuitem]#사용자 지정 가상 머신 만들기#를 선택하고 [.guimenuitem]#계속#을 클릭하여 계속 진행합니다:

image::vmware-freebsd02.png[width=45%]

메시지가 표시되면 [.guimenuem]#Other#를 [.guimenuem]#Operating System#로 선택하고 메뉴:버전[]에서 [.guimenuem]#FreeBSD X# 또는 [.guimenuem]#FreeBSD X 64-bit# 중 하나를 선택합니다:

image::vmware-freebsd03.png[width=45%]

펌웨어를 선택합니다(UEFI 권장):

image::vmware-freebsd04.png[width=45%]

[.guimenuitem]#Create a new virtual disk# 를 선택하고 [.guimenuitem]#Continue#를 클릭합니다:

image::vmware-freebsd05.png[width=45%]

구성을 확인하고 [.guimenuitem]#Finish#를 클릭합니다:

image::vmware-freebsd06.png[width=45%]

가상 머신의 이름과 가상 머신을 저장할 디렉터리를 선택합니다:

image::vmware-freebsd07.png[width=45%]

Command+E를 눌러 가상 머신 설정을 열고 [.guimenuitem]#CD/DVD#를 클릭합니다:

image::vmware-freebsd08.png[width=45%]

FreeBSD ISO 이미지 또는 CD/DVD를 선택합니다:

image::vmware-freebsd09.png[width=45%]

가상 머신을 시작합니다:

image::vmware-freebsd10.png[width=25%]

평소와 같이 FreeBSD를 설치합니다:

image::vmware-freebsd11.png[width=25%]

설치가 완료되면 메모리 사용량, 가상 머신이 액세스할 수 있는 CPU 수 등 가상 머신의 설정을 수정할 수 있습니다:

[NOTE]
====
가상 머신이 실행 중인 동안에는 가상 머신의 시스템 하드웨어 설정을 수정할 수 없습니다.
====

image::vmware-freebsd12.png[width=45%]

CD-ROM 장치의 상태입. 일반적으로 CD/DVD/ISO는 더 이상 필요하지 않을 때 가상 머신에서 연결이 끊어집니다.

image::vmware-freebsd09.png[width=45%]

마지막으로 변경할 사항은 가상 머신이 네트워크에 연결하는 방식입니다. 호스트 이외의 다른 머신에서 가상 머신에 연결할 수 있도록 하려면 [.guimenuitem]#Connect directly to the physical network (Bridged)#을 선택합니다. 그렇지 않으면 가상 머신이 인터넷에 액세스할 수 있지만 네트워크가 가상 머신에 액세스할 수 없도록 [.guimenuitem]#Share the host's internet connection (NAT)#를 선택하는 것이 좋습니다.

image::vmware-freebsd13.png[width=45%]

설정을 수정한 후 새로 설치된 FreeBSD 가상 머신을 부팅합니다.

[[virtualization-guest-vmware-configure]]
=== VMware Fusion에서 FreeBSD 구성하기

FreeBSD가 VMware Fusion을 통해 macOS(R) X에 성공적으로 설치한 후에는 가상화된 운영을 위해 시스템을 최적화하기 위해 수행할 수 있는 여러 구성 단계가 있습니다.

[.procedure]
. 부트 로더 변수 설정
+
가장 중요한 단계는 VMware Fusion 환경에서 FreeBSD의 CPU 사용률을 낮추기 위해 `kern.hz` 튜너블을 낮추는 것입니다. 이는 [.filename]#/boot/loader.conf#에 다음 줄을 추가하여 수행됩니다:
+
[.programlisting]
....
kern.hz=100
....
+
이 설정을 사용하지 않으면 유휴 FreeBSD VMware Fusion 게스트가 단일 프로세서 iMac(R) CPU의 약 15%를 사용하게 됩니다. 이 변경 후에는 사용량이 5%에 가까워집니다.
. 새 커널 구성 파일 생성
+
사용자 지정 커널 구성 파일에서 모든 FireWire 및 USB 장치 드라이버를 제거할 수 있습니다. VMware Fusion은 man:em[4] 드라이버에서 사용하는 가상 네트워크 어댑터를 제공하므로 man:em[4]를 제외한 모든 네트워크 디바이스를 커널에서 제거할 수 있습니다.
. 네트워킹 구성하기
+
가장 기본적인 네트워킹 설정은 DHCP를 사용하여 가상 머신을 호스트 Mac(R)과 동일한 로컬 영역 네트워크에 연결하는 것입니다. 이 설정은 [.filename]#/etc/rc.conf#에 `ifconfig_em0="DHCP"`를 추가하여 수행할 수 있습니다. 보다 고급 네트워킹 설정은 crossref:advanced-networking[advanced-networking,Advanced Networking]에 설명되어 있습니다.
+
. 드라이버 및 open-vm-tools 설치
+
VMWare에서 FreeBSD를 원활하게 실행하려면 드라이버를 설치해야 합니다:
+
[source, shell]
....
# pkg install xf86-video-vmware xf86-input-vmmouse open-vm-tools
....

[[virtualization-guest-virtualbox]]
== VirtualBox(TM)의 게스트로서의 FreeBSD

FreeBSD는 VirtualBox(TM)에서 게스트로서 잘 작동합니다. 이 가상화 소프트웨어는 FreeBSD 자체를 포함하여 대부분의 일반적인 운영 체제에서 사용할 수 있습니다.

VirtualBox(TM) 게스트 추가 기능은 다음을 지원합니다:

* 클립보드 공유.
* 마우스 포인터 통합.
* 호스트 시간 동기화.
* 창 크기 조정.
* 심리스 모드.

[NOTE]
====
이러한 명령은 FreeBSD 게스트에서 실행됩니다.
====

먼저, FreeBSD 게스트에 package:emulators/virtualbox-ose-additions[] 패키지 또는 포트를 설치합니다. 그러면 포트가 설치됩니다:

[source, shell]
....
# cd /usr/ports/emulators/virtualbox-ose-additions && make install clean
....

다음을 [.filename]#/etc/rc.conf#에 추가합니다:

[.programlisting]
....
vboxguest_enable="YES"
vboxservice_enable="YES"
....

man:ntpd[8] 또는 man:ntpdate[8]을 사용하는 경우, 호스트 시간 동기화를 비활성화합니다:

[.programlisting]
....
vboxservice_flags="--disable-timesync"
....

Xorg는 `vboxvideo` 드라이버를 자동으로 인식합니다. 또한 [.filename]#/etc/X11/xorg.conf#에 수동으로 입력할 수도 있습니다:

[.programlisting]
....
Section "Device"
	Identifier "Card0"
	Driver "vboxvideo"
	VendorName "InnoTek Systemberatung GmbH"
	BoardName "VirtualBox Graphics Adapter"
EndSection
....

`vboxmouse` 드라이버를 사용하려면 [.filename]#/etc/X11/xorg.conf#에서 마우스 섹션을 조정합니다:

[.programlisting]
....
Section "InputDevice"
	Identifier "Mouse0"
	Driver "vboxmouse"
EndSection
....

HAL 사용자는 다음 [.filename]#/usr/local/etc/hal/fdi/picy/90-vboxguest.fdi#를 만들거나 [.filename]#/usr/local/share/hal/fdi/picy/10osvendor/90-vboxguest.fdi#에서 복사해야 합니다:

[.programlisting]
....
<?xml version="1.0" encoding="utf-8"?>
<!--
# Sun VirtualBox
# Hal driver description for the vboxmouse driver
# $Id: chapter.xml,v 1.33 2012-03-17 04:53:52 eadler Exp $

	Copyright (C) 2008-2009 Sun Microsystems, Inc.

	This file is part of VirtualBox Open Source Edition (OSE, as
	available from http://www.virtualbox.org. This file is free software;
	you can redistribute it and/or modify it under the terms of the GNU
	General Public License (GPL) as published by the Free Software
	Foundation, in version 2 as it comes in the "COPYING" file of the
	VirtualBox OSE distribution. VirtualBox OSE is distributed in the
	hope that it will be useful, but WITHOUT ANY WARRANTY of any kind.

	Please contact Sun Microsystems, Inc., 4150 Network Circle, Santa
	Clara, CA 95054 USA or visit http://www.sun.com if you need
	additional information or have any questions.
-->
<deviceinfo version="0.2">
  <device>
    <match key="info.subsystem" string="pci">
      <match key="info.product" string="VirtualBox guest Service">
        <append key="info.capabilities" type="strlist">input</append>
	<append key="info.capabilities" type="strlist">input.mouse</append>
        <merge key="input.x11_driver" type="string">vboxmouse</merge>
	<merge key="input.device" type="string">/dev/vboxguest</merge>
      </match>
    </match>
  </device>
</deviceinfo>
....

호스트와 VM 간의 파일 전송을 위한 공유 폴더는 `mount_vboxvfs`를 사용하여 마운트 후 액세스할 수 있습니다. 공유 폴더는 VirtualBox GUI를 사용하거나 `vboxmanage`를 통해 호스트에서 생성할 수 있습니다. 예를 들어, _BSDBox_라는 VM에 대해 [.filename]#/mnt/bsdboxshare# 아래에 _myshare_라는 공유 폴더를 생성하려면 다음을 실행합니다:

[source, shell]
....
# vboxmanage sharedfolder add 'BSDBox' --name myshare --hostpath /mnt/bsdboxshare
....

공유 폴더 이름에 공백이 없어야 한다는 점에 유의하세요. 게스트 시스템 내에서 공유 폴더를 다음과 같이 마운트합니다:

[source, shell]
....
# mount_vboxvfs -w myshare /mnt
....

[[virtualization-host-virtualbox]]
== VirtualBox(TM)를 사용하는 호스트로서의 FreeBSD

VirtualBox(TM)는 활발하게 개발되고 있는 완벽한 가상화 패키지로, Windows(R), macOS(R), Linux(R) 및 FreeBSD를 포함한 대부분의 운영 체제에서 사용할 수 있습니다. Windows(R) 또는 UNIX(R)와 유사한 게스트도 동일하게 실행할 수 있습니다. 오픈 소스 소프트웨어로 출시되지만 별도의 확장 팩으로 제공되는 폐쇄 소스 구성 요소를 사용할도 수 있습니다. 이러한 구성 요소에는 USB 2.0 장치에 대한 지원이 포함됩니다. 자세한 내용은 http://www.virtualbox.org/wiki/Downloads[VirtualBox(TM) 위키의 다운로드 페이지]에서 확인할 수 있습니다. 현재 이러한 확장 기능은 FreeBSD에서 사용할 수 없습니다.

[[virtualization-virtualbox-install]]
=== VirtualBox(TM) 설치하기

VirtualBox(TM)는 FreeBSD 패키지 또는 package:emulators/virtualbox-ose[]의 포트로 제공됩니다. 포트는 다음 명령을 사용하여 설치할 수 있습니다:

[source, shell]
....
# cd /usr/ports/emulators/virtualbox-ose
# make install clean
....

포트의 구성 메뉴에서 유용한 옵션 중 하나는 `GuestAdditions` 프로그램 모음입니다. 이 프로그램은 마우스 포인터 통합(특별한 키보드 단축키를 눌러 전환할 필요 없이 호스트와 게스트 간에 마우스를 공유할 수 있음), 빠른 비디오 렌더링 등 게스트 운영 체제에서 유용한 여러 기능을 제공하며, 특히 Windows(R) 게스트에서 유용합니다. 게스트 추가는 게스트 설치가 완료된 후 메뉴:장치[] 메뉴에서 사용할 수 있습니다.

VirtualBox(TM)를 처음 시작하기 전에 몇 가지 구성 변경이 필요합니다. 이 포트는 실행 중인 커널에 로드해야 하는 커널 모듈을 [.filename]#/boot/modules#에 설치합니다:

[source, shell]
....
# kldload vboxdrv
....

재부팅 후 모듈이 항상 로드되도록 하려면 [.filename]#/boot/loader.conf#에 이 줄을 추가하세요:

[.programlisting]
....
vboxdrv_load="YES"
....

브리지 또는 호스트 전용 네트워킹을 허용하는 커널 모듈을 사용하려면 [.filename]#/etc/rc.conf#에 다음 줄을 추가하고 컴퓨터를 재부팅하세요:

[.programlisting]
....
vboxnet_enable="YES"
....

VirtualBox(TM)를 설치하는 동안 `vboxusers` 그룹이 생성됩니다. VirtualBox(TM)에 액세스해야 하는 모든 사용자는 이 그룹의 구성원으로 추가해야 합니다. `pw`를 사용하여 새 구성원을 추가할 수 있습니다:

[source, shell]
....
# pw groupmod vboxusers -m yourusername
....

[.filename]#/dev/vboxnetctl#에 대한 기본 권한은 제한적이며, 브리지 네트워킹을 위해서는 변경해야 합니다:

[source, shell]
....
# chown root:vboxusers /dev/vboxnetctl
# chmod 0660 /dev/vboxnetctl
....

이 권한 변경을 영구적으로 적용하려면 [.filename]#/etc/devfs.conf#에 다음 줄을 추가하세요:

[.programlisting]
....
own     vboxnetctl root:vboxusers
perm    vboxnetctl 0660
....

VirtualBox(TM)를 시작하려면 Xorg 세션에서:

[source, shell]
....
% VirtualBox
....

VirtualBox(TM)의 구성 및 사용에 대한 자세한 내용은 http://www.virtualbox.org[공식 웹사이트]를 참조하세요. FreeBSD 관련 정보 및 문제 해결 지침은 http://wiki.FreeBSD.org/VirtualBox[FreeBSD 위키의 관련 페이지]를 참조하세요.

[[virtualization-virtualbox-usb-support]]
=== VirtualBox(TM) USB 지원

USB 장치를 게스트 운영 체제로 전달하도록 VirtualBox(TM)를 구성할 수 있습니다. OSE 버전의 호스트 컨트롤러는 USB 2.0 및 3.0 장치를 지원하는 확장 팩이 FreeBSD에서 제공될 때까지 USB 1.1 장치 에뮬레이션으로 제한됩니다.

VirtualBox(TM)가 컴퓨터에 연결된 USB 장치를 인식하려면 사용자가 `operator 그룹의 구성원이어야 합니다.

[source, shell]
....
# pw groupmod operator -m yourusername
....

그런 다음 [.filename]#/etc/devfs.rules#에 다음을 추가하거나 이 파일이 아직 없는 경우 파일을 만듭니다:

[.programlisting]
....
[system=10]
add path 'usb/*' mode 0660 group operator
....

이러한 새 규칙을 로드하려면 [.filename]#/etc/rc.conf#에 다음을 추가하세요:

[.programlisting]
....
devfs_system_ruleset="system"
....

그런 다음 devfs를 다시 시작합니다:

[source, shell]
....
# service devfs restart
....

변경 사항을 적용하려면 로그인 세션과 VirtualBox(TM)를 다시 시작하고 필요에 따라 USB 필터를 만드세요.

[[virtualization-virtualbox-host-dvd-cd-access]]
=== VirtualBox(TM) 호스트 DVD/CD 액세스

게스트가 호스트 DVD/CD 드라이브에 액세스하는 것은 물리적 드라이브 공유를 통해 이루어집니다. 가상 머신의 설정에 있는 스토리지 창에서 이 기능을 설정할 수 있습니다. 필요한 경우 먼저 빈 IDECD/DVD 장치를 생성합니다. 그런 다음 가상 CD/DVD 드라이브 선택 팝업 메뉴에서 호스트 드라이브를 선택합니다. `Passthrough`라고 표시된 확인란이 나타납니다. 이렇게 하면 가상 머신이 하드웨어를 직접 사용할 수 있습니다. 예를 들어, 오디오 CD나 버너는 이 옵션을 선택한 경우에만 작동합니다.

VirtualBox(TM) DVD/CD 기능이 작동하려면 HAL이 실행되어야 하므로 [.filename]#/etc/rc.conf#에서 활성화하고 아직 실행 중이 아니라면 시작하세요:

[.programlisting]
....
hald_enable="YES"
....

[source, shell]
....
# service hald start
....

사용자가 VirtualBox(TM)의 DVD/CD 기능을 사용하려면 [.filename]#/dev/xpt0#, [.filename]#/dev/cdN# 및 [.filename]#/dev/passN#에 액세스할 수 있어야 합니다. 이는 일반적으로 사용자를 `operator`의 멤버로 설정하면 됩니다. 이러한 장치에 대한 권한은 [.filename]#/etc/devfs.conf#에 다음을 추가하여 수정해야 합니다:

[.programlisting]
....
perm cd* 0660
perm xpt0 0660
perm pass* 0660
....

[source, shell]
....
# service devfs restart
....

[[virtualization-host-bhyve]]
== bhyve를 사용하는 호스트로서의 FreeBSD

bhyve BSD 라이선스 하이퍼바이저는 FreeBSD 10.0-RELEASE와 함께 기본 시스템의 일부가 되었습니다. 이 하이퍼바이저는 FreeBSD, OpenBSD 및 많은 Linux(R) 배포판을 포함한 여러 게스트를 지원합니다. 기본적으로 bhyve는 직렬 콘솔에 대한 액세스를 제공하며 그래픽 콘솔을 에뮬레이트하지 않습니다. 최신 CPU의 가상화 오프로드 기능은 명령어를 번역하고 메모리 매핑을 수동으로 관리하는 기존 방법을 피하기 위해 사용됩니다.

bhyve 설계를 사용하려면 인텔(R) 확장 페이지 테이블(Extended Page Tables, EPT) 또는 AMD(R) 빠른 가상화 인덱싱(Rapid Virtualization Indexing, RVI) 또는 네스티드 페이지 테이블(Nested Page Tables, NPT)를 지원하는 프로세서가 필요합니다. 둘 이상의 vCPU가 있는 Linux(R) 게스트 또는 FreeBSD 게스트를 호스팅하려면 VMX 무제한 모드 지원(UG)이 필요합니다. 대부분의 최신 프로세서, 특히 인텔(R) 코어(TM) i3/i5/i7 및 인텔(R) 제온(TM) E3/E5/E7은 이러한 기능을 지원합니다. UG 지원은 인텔의 웨스트미어 마이크로 아키텍처에서 도입되었습니다. EPT를 지원하는 인텔(R) 프로세서의 전체 목록은 https://ark.intel.com/content/www/us/en/ark/search/featurefilter.html?productType=873&0_ExtendedPageTables=True[]를 참조하세요. RVI는 3세대 이상의 AMD 옵테론(TM)(바르셀로나) 프로세서에 탑재되어 있습니다. 프로세서가 bhyve를 지원하는지 확인하는 가장 쉬운 방법은 `dmesg`를 실행하거나 [.filename]#/var/run/dmesg.boot#에서 AMD(R) 프로세서의 경우 `Features2` 줄에 있는 `POPCNT` 프로세서 기능 플래그, Intel(R) 프로세서의 경우 `VT-x` 줄에 있는 `EPT` 및 `UG`를 찾는 것입니다.

[[virtualization-bhyve-prep]]
=== 호스트 준비하기

bhyve에서 가상 머신을 생성하는 첫 번째 단계는 호스트 시스템을 구성하는 것입니다. 먼저 bhyve 커널 모듈을 로드합니다:

[source, shell]
....
# kldload vmm
....

그런 다음 가상 머신의 네트워크 장치가 연결할 [.filename]#tap# 인터페이스를 만듭니다. 네트워크 장치가 네트워크에 참여하려면 [.filename]#tap# 인터페이스와 물리적 인터페이스를 멤버로 포함하는 브리지 인터페이스도 만듭니다. 이 예에서 물리적 인터페이스는 _igb0_입니다:

[source, shell]
....
# ifconfig tap0 create
# sysctl net.link.tap.up_on_open=1
net.link.tap.up_on_open: 0 -> 1
# ifconfig bridge0 create
# ifconfig bridge0 addm igb0 addm tap0
# ifconfig bridge0 up
....

[[virtualization-bhyve-freebsd]]
=== FreeBSD 게스트 생성하기

게스트 머신의 가상 디스크로 사용할 파일을 생성합니다. 가상 디스크의 크기와 이름을 지정합니다:

[source, shell]
....
# truncate -s 16G guest.img
....

설치할 FreeBSD의 설치 이미지를 다운로드합니다:

[source, shell]
....
# fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/12.2/FreeBSD-12.2-RELEASE-amd64-bootonly.iso
FreeBSD-12.2-RELEASE-amd64-bootonly.iso       100% of  230 MB  570 kBps 06m17s
....

FreeBSD는 bhyve에서 가상 머신을 실행하기 위한 예제 스크립트와 함께 제공됩니다. 이 스크립트는 가상 머신을 시작하고 루프로 동작하므로 충돌이 발생하면 자동으로 다시 시작됩니다. 이 스크립트에는 머신 구성을 제어하기 위한 여러 옵션이 있습니다: `-c`는 가상 CPU 수를 제어하고, `-m`은 게스트가 사용할 수 있는 메모리 양을 제한하며, `-t`는 사용할 [.filename]#tap# 장치를 정의하고, `-d`는 사용할 디스크 이미지를 나타내며, `-i`는 디스크 대신 CD 이미지에서 부팅하도록 bhyve에 지시하고, `-I`는 사용할 CD 이미지를 정의합니다. 마지막 매개변수는 실행 중인 머신을 추적하는 데 사용되는 가상 머신의 이름입니다. 이 예는 가상 머신을 설치 모드로 시작합니다:

[source, shell]
....
# sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 1024M -t tap0 -d guest.img -i -I FreeBSD-12.2-RELEASE-amd64-bootonly.iso guestname
....

가상 머신이 부팅되고 인스톨러가 시작됩니다. 가상 머신에 시스템을 설치한 후 설치가 끝날 때 시스템에서 셸에 드롭인할지 묻는 메시지가 표시되면 btn:[Yes]를 선택합니다.

가상 머신을 재부팅합니다. 가상 머신을 재부팅하면 bhyve가 종료되지만, [.filename]#vmrun.sh# 스크립트는 `bhyve`를 루프로 실행하고 자동으로 재시작합니다. 이 경우 부트 로더 메뉴에서 재부팅 옵션을 선택하여 루프를 벗어나십시오. 이제 가상 디스크에서 게스트를 시작할 수 있습니다:

[source, shell]
....
# sh /usr/share/examples/bhyve/vmrun.sh -c 4 -m 1024M -t tap0 -d guest.img guestname
....

[[virtualization-bhyve-linux]]
=== Linux(R) 게스트 생성하기

FreeBSD 이외의 운영 체제를 부팅하려면 package:sysutils/grub2-bhyve[] 포트를 먼저 설치해야 합니다.

다음으로 게스트 머신의 가상 디스크로 사용할 파일을 만듭니다:

[source, shell]
....
# truncate -s 16G linux.img
....

bhyve로 가상 머신을 시작하는 것은 2단계 과정이 필요합니다. 먼저 커널을 로드한 다음 게스트를 시작할 수 있습니다. Linux(R) 커널은 package:sysutils/grub2-bhyve[]로 로드됩니다. grub이 가상 디바이스를 호스트 시스템의 파일에 매핑하는 데 사용할 [.filename]#device.map#을 만듭니다:

[.programlisting]
....
(hd0) ./linux.img
(cd0) ./somelinux.iso
....

package:sysutils/grub2-bhyve[]를 사용하여 ISO 이미지에서 Linux(R) 커널을 로드합니다:

[source, shell]
....
# grub-bhyve -m device.map -r cd0 -M 1024M linuxguest
....

그러면 grub이 시작됩니다. 설치 CD에 [.filename]#grub.cfg#가 포함되어 있으면 메뉴가 표시됩니다. 그렇지 않은 경우 `vmlinuz` 및 `initrd` 파일을 찾아서 수동으로 로드해야 합니다:

[source, shell]
....
grub> ls
(hd0) (cd0) (cd0,msdos1) (host)
grub> ls (cd0)/isolinux
boot.cat boot.msg grub.conf initrd.img isolinux.bin isolinux.cfg memtest
splash.jpg TRANS.TBL vesamenu.c32 vmlinuz
grub> linux (cd0)/isolinux/vmlinuz
grub> initrd (cd0)/isolinux/initrd.img
grub> boot
....

이제 Linux(R) 커널이 로드되었으므로 게스트를 시작할 수 있습니다:

[source, shell]
....
# bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,tap0 -s 3:0,virtio-blk,./linux.img \
    -s 4:0,ahci-cd,./somelinux.iso -l com1,stdio -c 4 -m 1024M linuxguest
....

시스템이 부팅되고 설치 관리자가 시작됩니다. 가상 머신에 시스템을 설치한 후 가상 머신을 재부팅합니다. 그러면 bhyve가 종료됩니다. 가상 머신을 다시 시작하려면 가상 머신의 인스턴스를 파괴해야 합니다:

[source, shell]
....
# bhyvectl --destroy --vm=linuxguest
....

이제 가상 디스크에서 바로 게스트를 시작할 수 있습니다. 커널을 로드합니다:

[source, shell]
....
# grub-bhyve -m device.map -r hd0,msdos1 -M 1024M linuxguest
grub> ls
(hd0) (hd0,msdos2) (hd0,msdos1) (cd0) (cd0,msdos1) (host)
(lvm/VolGroup-lv_swap) (lvm/VolGroup-lv_root)
grub> ls (hd0,msdos1)/
lost+found/ grub/ efi/ System.map-2.6.32-431.el6.x86_64 config-2.6.32-431.el6.x
86_64 symvers-2.6.32-431.el6.x86_64.gz vmlinuz-2.6.32-431.el6.x86_64
initramfs-2.6.32-431.el6.x86_64.img
grub> linux (hd0,msdos1)/vmlinuz-2.6.32-431.el6.x86_64 root=/dev/mapper/VolGroup-lv_root
grub> initrd (hd0,msdos1)/initramfs-2.6.32-431.el6.x86_64.img
grub> boot
....

가상 머신을 부팅합니다:

[source, shell]
....
# bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,tap0 \
    -s 3:0,virtio-blk,./linux.img -l com1,stdio -c 4 -m 1024M linuxguest
....

이제 Linux(R)가 가상 머신에서 부팅되고 로그인 프롬프트가 표시됩니다. 로그인하여 가상 머신을 사용합니다. 완료되면 가상 머신을 재부팅하여 bhyve를 종료합니다. 가상 머신 인스턴스를 삭제합니다:

[source, shell]
....
# bhyvectl --destroy --vm=linuxguest
....

[[virtualization-bhyve-uefi]]
=== UEFI 펌웨어로 bhyve 가상 머신 부팅하기

bhyveload 및 grub-bhyve 외에도 bhyve 하이퍼바이저는 UEFI 사용자 공간 펌웨어를 사용하여 가상 머신을 부팅할 수도 있습니다. 이 옵션은 다른 로더에서 지원하지 않는 게스트 운영 체제를 지원할 수 있습니다.

bhyve에서 UEFI 지원을 사용하려면 먼저 UEFI 펌웨어 이미지를 얻어야 합니다. package:sysutils/bhyve-firmware[] 포트 또는 패키지를 설치하여 이 작업을 수행할 수 있습니다.

펌웨어를 설치한 후, bhyve 명령줄에 `-l bootrom,_/path/to/firmware_` 플래그를 추가합니다. 실제 bhyve 명령은 다음과 같을 수 있습니다:

[source, shell]
....
# bhyve -AHP -s 0:0,hostbridge -s 1:0,lpc \
-s 2:0,virtio-net,tap1 -s 3:0,virtio-blk,./disk.img \
-s 4:0,ahci-cd,./install.iso -c 4 -m 1024M \
-l bootrom,/usr/local/share/uefi-firmware/BHYVE_UEFI.fd \
guest
....

package:sysutils/bhyve-firmware[]에는 레거시 BIOS 모드에서 UEFI를 지원하지 않는 게스트를 부팅할 수 있는 CSM 지원 펌웨어도 포함되어 있습니다:

[source, shell]
....
# bhyve -AHP -s 0:0,hostbridge -s 1:0,lpc \
-s 2:0,virtio-net,tap1 -s 3:0,virtio-blk,./disk.img \
-s 4:0,ahci-cd,./install.iso -c 4 -m 1024M \
-l bootrom,/usr/local/share/uefi-firmware/BHYVE_UEFI_CSM.fd \
guest
....

[[virtualization-bhyve-framebuffer]]
=== bhyve 게스트용 그래픽 UEFI 프레임 버퍼

UEFI 펌웨어 지원은 Microsoft Windows(R)와 같은 그래픽 게스트 운영 체제를 주로 사용하는 경우에 특히 유용합니다.

또한 `-s 29,fbuf,tcp=_0.0.0.0:5900_` 플래그를 사용하여 UEFI-GOP 프레임버퍼 지원을 활성화할 수도 있습니다. 프레임버퍼 해상도는 `w=_800_` 및 `h=_600_`으로 구성할 수 있으며, `wait`을 추가하여 게스트 부팅 전에 VNC 연결을 기다리도록 bhyve에 지시할 수 있습니다. 프레임버퍼는 호스트에서 또는 네트워크를 통해 VNC 프로토콜을 통해 액세스할 수 있습니다. 또한 `-s 30,xhci,tablet`을 추가하여 호스트와 정확한 마우스 커서 동기화를 달성할 수 있습니다.

bhyve 명령의 결과는 다음과 같이 표시됩니다:

[source, shell]
....
# bhyve -AHP -s 0:0,hostbridge -s 31:0,lpc \
-s 2:0,virtio-net,tap1 -s 3:0,virtio-blk,./disk.img \
-s 4:0,ahci-cd,./install.iso -c 4 -m 1024M \
-s 29,fbuf,tcp=0.0.0.0:5900,w=800,h=600,wait \
-s 30,xhci,tablet \
-l bootrom,/usr/local/share/uefi-firmware/BHYVE_UEFI.fd \
guest
....

BIOS 에뮬레이션 모드에서는 제어권이 펌웨어에서 게스트 운영 체제로 넘어가면 프레임버퍼의 업데이트 수신이 중단됩니다.

[[virtualization-bhyve-zfs]]
=== bhyve 게스트에서 ZFS 사용하기

호스트 머신에서 ZFS를 사용할 수 있는 경우 디스크 이미지 파일 대신 ZFS 볼륨을 사용하면 게스트 VM에 상당한 성능 이점을 제공할 수 있습니다. ZFS 볼륨은 다음과 같이 생성할 수 있습니다:

[source, shell]
....
# zfs create -V16G -o volmode=dev zroot/linuxdisk0
....

VM을 시작할 때 ZFS 볼륨을 디스크 드라이브로 지정합니다:

[source, shell]
....
# bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,tap0 -s3:0,virtio-blk,/dev/zvol/zroot/linuxdisk0 \
    -l com1,stdio -c 4 -m 1024M linuxguest
....

[[virtualization-bhyve-nmdm]]
=== 가상 머신 콘솔

콘솔을 분리했다가 다시 연결하기 위해 package:sysutils/tmux[] 또는 package:sysutils/screen[]과 같은 세션 관리 도구로 bhyve 콘솔을 감싸는 것이 유리합니다. bhyve의 콘솔을 `cu`로 액세스할 수 있는 널 모뎀 장치로 만들 수도 있습니다. 이렇게 하려면 [.filename]#nmdm# 커널 모듈을 로드하고 `-l com1,stdio`를 `-l com1,/dev/nmdm0A`로 대체합니다. 필요에 따라 [.filename]#/dev/nmdm# 장치가 자동으로 생성되며, 각 장치는 널 모뎀 케이블의 양쪽 끝에 해당하는 한 쌍입니다([.filename]#/dev/nmdm0A# 및 [.filename]#/dev/nmdm0B#). 자세한 내용은 man:nmdm[4]을 참조하세요.

[source, shell]
....
# kldload nmdm
# bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,tap0 -s 3:0,virtio-blk,./linux.img \
    -l com1,/dev/nmdm0A -c 4 -m 1024M linuxguest
# cu -l /dev/nmdm0B
Connected

Ubuntu 13.10 handbook ttyS0

handbook login:
....

[[virtualization-bhyve-managing]]
=== 가상 머신 관리하기

장치 노드는 각 가상 머신에 대해 [.filename]#/dev/vmm#를 생성합니다. 이를 통해 관리자는 실행 중인 가상 머신의 목록을 쉽게 확인할 수 있습니다:

[source, shell]
....
# ls -al /dev/vmm
total 1
dr-xr-xr-x   2 root  wheel    512 Mar 17 12:19 ./
dr-xr-xr-x  14 root  wheel    512 Mar 17 06:38 ../
crw-------   1 root  wheel  0x1a2 Mar 17 12:20 guestname
crw-------   1 root  wheel  0x19f Mar 17 12:19 linuxguest
crw-------   1 root  wheel  0x1a1 Mar 17 12:19 otherguest
....

지정된 가상 머신은 `bhyvectl`을 사용하여 파괴할 수 있습니다:

[source, shell]
....
# bhyvectl --destroy --vm=guestname
....

[[virtualization-bhyve-onboot]]
=== 영구적 구성

부팅 시 bhyve 게스트가 시작되도록 시스템을 구성하려면 지정된 파일에서 다음 구성을 수행해야 합니다:

[.procedure]
. [.filename]#/etc/sysctl.conf#
+
[.programlisting]
....
net.link.tap.up_on_open=1
....

. [.filename]#/etc/rc.conf#
+
[.programlisting]
....
cloned_interfaces="bridge0 tap0"
ifconfig_bridge0="addm igb0 addm tap0"
kld_list="nmdm vmm"
....

[[virtualization-host-xen]]
== Xen(TM) 호스트로서의 FreeBSD

Xen은 Intel(R) 및 ARM(R) 아키텍처용 GPLv2 라이선스 https://en.wikipedia.org/wiki/Hypervisor#Classification[유형 1 하이퍼바이저]입니다. FreeBSD는 FreeBSD 8.0부터 i386(TM) 및 AMD(R) 64비트 https://wiki.xenproject.org/wiki/DomU[DomU] 및 https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud[Amazon EC2]의 권한 없는 도메인(가상 머신) 지원을 포함했으며, FreeBSD 11.0에는 Dom0 제어 도메인(호스트) 지원이 포함되어 있습니다. 더 나은 성능을 제공하는 하드웨어 가상화(HVM) 도메인을 위해 FreeBSD 11에서는 준가상화(PV) 도메인에 대한 지원이 제거되었습니다.

Xen(TM)은 베어메탈 하이퍼바이저이므로 BIOS 이후에 가장 먼저 로드되는 프로그램입니다. 그런 다음 Domain-0(줄여서 `Dom0`)이라는 특수 권한 게스트가 시작됩니다. Dom0는 특수 권한을 사용하여 기본 물리적 하드웨어에 직접 액세스하는 고성능 솔루션입니다. 디스크 컨트롤러 및 네트워크 어댑터에 직접 액세스할 수 있습니다. Dom0는 Xen(TM) 하이퍼바이저를 관리하고 제어하는 Xen(TM) 관리 도구도 사용하여 VM을 생성, 나열 및 삭제합니다. Dom0는 권한이 없는 도메인을 위한 가상 디스크 및 네트워킹을 제공하며, 종종 `DomU`라고도 합니다. Xen(TM) Dom0는 다른 하이퍼바이저 솔루션의 서비스 콘솔과 비교할 수 있으며, DomU는 개별 게스트 VM이 실행되는 곳입니다.

Xen(TM)은 서로 다른 Xen(TM) 서버 간에 VM을 마이그레이션할 수 있습니다. 두 Xen 호스트가 동일한 기본 스토리지를 공유하는 경우 VM을 먼저 종료할 필요 없이 마이그레이션을 수행할 수 있습니다. 그리고, 마이그레이션은 DomU가 실행되는 동안 실시간으로 수행되므로 다시 시작하거나 다운타임을 계획할 필요가 없습니다. 이 기능은 유지 관리 시나리오 또는 업그레이드 기간에 DomU에서 제공하는 서비스가 계속 제공되도록 하는 데 유용합니다. Xen(TM)의 더 많은 기능은 https://wiki.xenproject.org/wiki/Category:Overview[Xen Wiki 개요 페이지]에 나열되어 있습니다. 아직 FreeBSD에서 모든 기능이 지원되는 것은 아닙니다.

[[virtualization-host-xen-requirements]]
=== Xen(TM) Dom0의 하드웨어 요구 사항

호스트에서 Xen(TM) 하이퍼바이저를 실행하려면 특정 하드웨어 기능이 필요합니다. 하드웨어 가상화된 도메인을 사용하려면 호스트 프로세서에서 EPT(http://en.wikipedia.org/wiki/Extended_Page_Table[http://en.wikipedia.org/wiki/Extended_Page_Table][확장 페이지 테이블]) 및 IOMMU(http://en.wikipedia.org/wiki/List_of_IOMMU-supporting_hardware[입/출력 메모리 관리 장치])를 지원해야 합니다.

[NOTE]
====
FreeBSD Xen(TM) Dom0을 실행하려면 반드시 레거시 부팅(BIOS)을 사용하여 부팅해야 합니다.
====

[[virtualization-host-xen-dom0-setup]]
=== Xen(TM) Dom0 제어 도메인 설정

FreeBSD 11 사용자는 Xen 버전 4.7을 기반으로 하는 package:emulators/xen-kernel47[] 및 package:sysutils/xen-tools47[] 패키지를 설치해야 합니다. FreeBSD-12.0 이상에서 실행되는 시스템은 각각 package:emulators/xen-kernel411[] 및 package:sysutils/xen-tools411[]에서 제공하는 Xen 4.11을 사용할 수 있습니다.

Xen 패키지를 설치한 후 Dom0 통합을 위해 호스트를 준비하려면 구성 파일을 편집해야 합니다. [.filename]#/etc/sysctl.conf#에 항목을 추가하면 연결할 수 있는 메모리 페이지 수에 대한 제한을 해제할 수 있습니다. 그렇지 않으면 더 높은 메모리 요구 사항을 가진 DomU VM이 실행되지 않습니다.

[source, shell]
....
# echo 'vm.max_wired=-1' >> /etc/sysctl.conf
....

또 다른 메모리 관련 설정은 [.filename]#/etc/login.conf#를 변경하고 `memorylocked` 옵션을 `unlimited`으로 설정하는 것입니다. 그렇지 않으면 `Cannot allocate memory` 오류와 함께 DomU 도메인 생성이 실패할 수 있습니다. [.filename]#/etc/login.conf#를 변경한 후 `cap_mkdb`를 실행하여 기능 데이터베이스를 업데이트합니다. 자세한 내용은 crossref:security[security-resourcelimits,"Resource Limits"]을 참조하세요.

[source, shell]
....
# sed -i '' -e 's/memorylocked=64K/memorylocked=unlimited/' /etc/login.conf
# cap_mkdb /etc/login.conf
....

Xen(TM) 콘솔에 대한 항목을 [.filename]#/etc/ttys#에 추가합니다:

[source, shell]
....
# echo 'xc0     "/usr/libexec/getty Pc"         xterm   onifconsole  secure' >> /etc/ttys
....

[.filename]#/boot/loader.conf#에서 Xen(TM) 커널을 선택하면 Dom0가 활성화됩니다. 또한 Xen(TM)은 자체 및 다른 DomU 도메인을 위해 호스트 컴퓨터의 CPU 및 메모리와 같은 리소스를 필요로 합니다. CPU 및 메모리의 양은 개별 요구 사항 및 하드웨어 기능에 따라 다릅니다. 이 예에서는 Dom0에 8GB의 메모리와 4개의 가상 CPU를 사용할 수 있습니다. 직렬 콘솔도 활성화되고 로깅 옵션이 정의됩니다.

다음 명령은 Xen 4.7 패키지에 사용됩니다:

[source, shell]
....
# echo 'hw.pci.mcfg=0' >> /boot/loader.conf
# echo 'if_tap_load="YES"' >> /boot/loader.conf
# echo 'xen_kernel="/boot/xen"' >> /boot/loader.conf
# echo 'xen_cmdline="dom0_mem=8192M dom0_max_vcpus=4 dom0pvh=1 console=com1,vga com1=115200,8n1 guest_loglvl=all loglvl=all"' >> /boot/loader.conf
....

Xen 버전 4.11 이상에서는 다음 명령을 대신 사용해야 합니다:

[source, shell]
....
# echo 'if_tap_load="YES"' >> /boot/loader.conf
# echo 'xen_kernel="/boot/xen"' >> /boot/loader.conf
# echo 'xen_cmdline="dom0_mem=8192M dom0_max_vcpus=4 dom0=pvh console=com1,vga com1=115200,8n1 guest_loglvl=all loglvl=all"' >> /boot/loader.conf
....

[TIP]
====

Xen(TM)이 DomU VM에 대해 생성하는 로그 파일은 [.filename]#/var/log/xen#에 저장됩니다. 문제가 발생하면 해당 디렉터리의 내용을 확인하십시오.
====

시스템 시작 시 xencommons 서비스를 활성화합니다:

[source, shell]
....
# sysrc xencommons_enable=yes
....

이 설정은 Dom0 지원 시스템을 시작하기에 충분합니다. 그러나 DomU 머신을 위한 네트워크 기능이 부족합니다. 이 문제를 해결하려면 DomU VM이 네트워크에 연결하는 데 사용할 수 있는 시스템의 기본 NIC와 브리지 인터페이스를 정의합니다. _em0_을 호스트 네트워크 인터페이스 이름으로 바꿉니다.

[source, shell]
....
# sysrc cloned_interfaces="bridge0"
# sysrc ifconfig_bridge0="addm em0 SYNCDHCP"
# sysrc ifconfig_em0="up"
....

호스트를 다시 시작하여 Xen(TM) 커널을 로드하고 Dom0를 시작합니다.

[source, shell]
....
# reboot
....

Xen(TM) 커널을 성공적으로 부팅하고 시스템에 다시 로그인하면 Xen(TM) 관리 도구 `xl`으로 도메인에 대한 정보를 확인할 수 있습니다.

[source, shell]
....
# xl list
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  8192     4     r-----     962.0
....

출력은 Dom0(`Domain-0`이라고 함)의 ID가 `0`이고 실행 중임을 확인합니다. 또한 앞서 [.filename]#/boot/loader.conf#에 정의된 메모리 및 가상 CPU가 있습니다. 자세한 내용은 https://www.xenproject.org/help/documentation.html[Xen(TM) 설명서]에서 확인할 수 있습니다. 이제 DomU 게스트 VM을 만들 수 있습니다.

[[virtualization-host-xen-domu-setup]]
=== Xen(TM) DomU 게스트 VM 구성

권한이 없는 도메인은 구성 파일과 가상 또는 물리적 하드 디스크로 구성됩니다. DomU의 가상 디스크 저장소는 man:truncate[1]로 만든 파일 또는 crossref:zfs[zfs-zfs-volume,“Creating and Destroying Volumes”]에 설명된 대로 ZFS 볼륨을 사용할 수 있습니다. 이 예에서는 20GB 볼륨이 사용됩니다. ZFS 볼륨, FreeBSD ISO 이미지, 1GB RAM 및 2개의 가상 CPU로 VM이 생성됩니다. ISO 설치 파일은 man:fetch[1]로 검색하여 [.filename]#freebsd.iso#라는 파일에 로컬로 저장됩니다.

[source, shell]
....
# fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/12.0/FreeBSD-12.0-RELEASE-amd64-bootonly.iso -o freebsd.iso
....

[.filename]#xendisk0#이라는 20GB의 ZFS 볼륨이 생성되어 VM의 디스크 공간으로 사용됩니다.

[source, shell]
....
# zfs create -V20G -o volmode=dev zroot/xendisk0
....

새 DomU 게스트 VM은 파일로 정의합니다. 이름, 키맵 및 VNC 연결 세부 정보와 같은 일부 특성도 정의합니다. 다음 [.filename]#freebsd.cfg#에는 이 예제에 대한 최소 DomU 구성이 포함되어 있습니다:

[source, shell]
....
# cat freebsd.cfg
builder = "hvm" <.>
name = "freebsd" <.>
memory = 1024 <.>
vcpus = 2 <.>
vif = [ 'mac=00:16:3E:74:34:32,bridge=bridge0' ] <.>
disk = [
'/dev/zvol/tank/xendisk0,raw,hda,rw', <.>
'/root/freebsd.iso,raw,hdc:cdrom,r' <.>
  ]
vnc = 1 <.>
vnclisten = "0.0.0.0"
serial = "pty"
usbdevice = "tablet"
....

이 명령줄에 대한 자세한 설명입니다:

<.> 사용할 가상화 종류를 정의합니다. `hvm`은 하드웨어 지원 가상화 또는 하드웨어 가상 머신을 의미합니다. 게스트 운영 체제는 가상화 확장이 있는 CPU에서 수정하지 않고 실행할 수 있으며, 물리적 하드웨어에서 실행하는 것과 거의 동일한 성능을 제공합니다. 기본값은 `generic`이며 PV 도메인을 생성합니다.
<.> 동일한 Dom0에서 실행되는 다른 가상 머신과 구별하기 위한 이 가상 머신의 이름입니다. 필수입니다.
<.> VM에서 사용할 수 있는 RAM의 양(MB)입니다. 이 양은 Dom0의 메모리가 아닌 하이퍼바이저의 총 사용 가능한 메모리에서 차감됩니다.
<.> 게스트 VM에서 사용할 수 있는 가상 CPU 수입니다. 최상의 성능을 위해 호스트의 물리적 CPU 수보다 많은 가상 CPU를 가진 게스트를 만들지 마십시오.
<.> 가상 네트워크 어댑터. 호스트의 네트워크 인터페이스에 연결된 브리지입니다. `mac` 매개 변수는 가상 네트워크 인터페이스에 설정된 MAC 주소입니다. 이 매개 변수는 선택 사항이며, MAC이 제공되지 않으면 Xen(TM)에서 임의의 MAC을 생성합니다.
<.> 이 VM에 대한 디스크 스토리지의 디스크, 파일 또는 ZFS 볼륨에 대한 전체 경로입니다. 옵션 및 여러 디스크 정의는 쉼표로 구분됩니다.
<.> 초기 운영 체제가 설치되는 부팅 매체를 정의합니다. 이 예에서는 이전에 다운로드한 ISO 이미지입니다. 설정할 다른 종류의 장치 및 옵션에 대해서는 Xen(TM) 설명서를 참조하십시오.
<.> DomU의 시리얼 콘솔에 대한 VNC 연결을 제어하는 옵션입니다. 순서대로 VNC 지원 활성화, 수신할 IP 주소 정의, 시리얼 콘솔의 장치 노드, 마우스 및 기타 입력 방법의 정확한 위치를 위한 입력 방법 등이 있습니다. `keymap`은 사용할 키맵을 정의하며, 기본값은 `english`입니다.

필요한 모든 옵션이 포함된 파일을 생성한 후 이를 `xl create`에 매개변수로 전달하여 DomU를 생성합니다.

[source, shell]
....
# xl create freebsd.cfg
....

[NOTE]
====
Dom0을 재시작할 때마다 구성 파일을 `xl create`에 다시 전달하여 DomU를 다시 생성해야 합니다. 기본적으로 재부팅 후에는 개별 VM이 아닌 Dom0만 생성됩니다. VM은 가상 디스크에 운영 체제를 저장한 상태에서 중단한 부분을 계속 진행할 수 있습니다. 가상 머신 구성은 시간이 지남에 따라 변경될 수 있습니다(예: 메모리를 더 추가할 때). 필요할 때 게스트 VM을 다시 생성할 수 있도록 가상 머신 구성 파일을 적절히 백업하고 사용 가능한 상태로 유지해야 합니다.
====

'xl list`의 출력은 DomU가 생성되었음을 보여줍니다.

[source, shell]
....
# xl list
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  8192     4     r-----  1653.4
freebsd                                      1  1024     1     -b----   663.9
....

기본 운영 체제 설치를 시작하려면 VNC 클라이언트를 시작하여 호스트의 기본 네트워크 주소 또는 [.filename]#freebsd.cfg#의 `vnclisten` 줄에 정의된 IP 주소로 안내합니다. 운영 체제를 설치한 후 DomU를 종료하고 VNC 뷰어의 연결을 끊습니다. [.filename]#freebsd.cfg#를 편집하여 `cdrom` 정의가 있는 줄을 제거하거나 줄의 시작 부분에 `+#+` 문자를 삽입하여 주석을 달면 됩니다. 이 새 구성을 로드하려면 이름 또는 아이디를 매개변수로 전달하여 `xl destroy`를 사용하여 이전 DomU를 제거해야 합니다. 그런 다음 수정된 [.filename]*freebsd.cfg*를 사용하여 다시 생성합니다.

[source, shell]
....
# xl destroy freebsd
# xl create freebsd.cfg
....

그런 다음 VNC 뷰어를 사용하여 머신에 다시 액세스할 수 있습니다. 이번에는 운영 체제가 설치된 가상 디스크에서 부팅되어 가상 머신으로 사용할 수 있습니다.

[[virtualization-host-xen-troubleshooting]]
=== 문제 해결

이 섹션에는 FreeBSD를 Xen(TM) 호스트 또는 게스트로 사용할 때 발생하는 문제를 해결하는 데 도움이 되는 기본 정보가 포함되어 있습니다.

[[virtualization-host-xen-troubleshooting-host]]
==== 호스트 부팅 문제 해결

다음 문제 해결 팁은 Xen(TM) 4.11 이상을 위한 것입니다. Xen(TM) 4.7을 계속 사용 중인데 문제가 발생하는 경우 최신 버전의 Xen(TM)으로 마이그레이션하는 것을 고려하십시오.

호스트 부팅 문제를 해결하려면 직렬 케이블 또는 디버그 USB 케이블이 필요할 수 있습니다. 자세한 Xen(TM) 부팅 출력은 [.filename]#loader.conf#에 있는 `xen_cmdline` 옵션에 옵션을 추가하여 얻을 수 있습니다. 몇 가지 관련 디버그 옵션은 다음과 같습니다:

* `iommu=debug`: iommu에 대한 추가 진단 정보를 인쇄하는 데 사용할 수 있습니다.
* `dom0=verbose`: dom0 빌드 프로세스에 대한 추가 진단 정보를 인쇄하는 데 사용할 수 있습니다.
* `sync_console`: 콘솔 출력을 강제하는 플래그. 속도 제한으로 인한 메시지 손실을 방지하기 위한 디버깅에 유용합니다. 이 옵션을 사용하면 악의적인 게스트가 콘솔을 사용하여 Xen(TM)에 대한 DoS 공격을 수행할 수 있으므로 프로덕션 환경에서는 이 옵션을 사용하지 마십시오.

또한 문제를 식별하기 위해 FreeBSD를 자세한 정보 모드(verbose mode)로 부팅해야 합니다. 자세한 정보 부팅을 활성화하려면 다음 명령을 실행하세요:

[source, shell]
....
# echo 'boot_verbose="YES"' >> /boot/loader.conf
....

이러한 옵션 중 어느 것도 문제 해결에 도움이 되지 않는다면 mailto:freebsd-xen@FreeBSD.org[freebsd-xen@FreeBSD.org] 및 mailto:xen-devel@lists.xenproject.org[xen-devel@lists.xenproject.org]으로 시리얼 부팅 로그를 보내 자세한 분석을 요청하세요.

[[virtualization-host-xen-troubleshooting-guest]]
==== 게스트 생성 문제 해결

게스트 생성 시에도 문제가 발생할 수 있으며, 다음은 게스트 생성 문제를 진단하는 데 도움이 될 수 있는 몇 가지 방법입니다.

게스트 생성 실패의 가장 일반적인 원인은 `xl` 명령이 일부 오류를 뱉어내고 반환 코드가 0과 다른 상태로 종료되는 것입니다. 제공된 오류가 문제를 식별하는 데 충분하지 않은 경우 `v` 옵션을 반복해서 사용하여 `xl`에서 더 자세한 출력을 얻을 수도 있습니다.

[source, shell]
....
# xl -vvv create freebsd.cfg
Parsing config from freebsd.cfg
libxl: debug: libxl_create.c:1693:do_domain_create: Domain 0:ao 0x800d750a0: create: how=0x0 callback=0x0 poller=0x800d6f0f0
libxl: debug: libxl_device.c:397:libxl__device_disk_set_backend: Disk vdev=xvda spec.backend=unknown
libxl: debug: libxl_device.c:432:libxl__device_disk_set_backend: Disk vdev=xvda, using backend phy
libxl: debug: libxl_create.c:1018:initiate_domain_create: Domain 1:running bootloader
libxl: debug: libxl_bootloader.c:328:libxl__bootloader_run: Domain 1:not a PV/PVH domain, skipping bootloader
libxl: debug: libxl_event.c:689:libxl__ev_xswatch_deregister: watch w=0x800d96b98: deregister unregistered
domainbuilder: detail: xc_dom_allocate: cmdline="", features=""
domainbuilder: detail: xc_dom_kernel_file: filename="/usr/local/lib/xen/boot/hvmloader"
domainbuilder: detail: xc_dom_malloc_filemap    : 326 kB
libxl: debug: libxl_dom.c:988:libxl__load_hvm_firmware_module: Loading BIOS: /usr/local/share/seabios/bios.bin
...
....

자세한 정보 출력으로 문제를 진단하는 데 도움이 되지 않는 경우 [.filename]#/var/log/xen#에 있는 QEMU 및 Xen(TM) 도구 스택 로그도 있습니다. 도메인 이름이 로그 이름에 추가되므로 도메인 이름이 `freebsd`인 경우 [.filename]#/var/log/xen/xl-freebsd.log#, [.filename]#/var/log/xen/qemu-dm-freebsd.log#를 찾을 수 있습니다. 두 로그 파일 모두 디버깅에 유용한 정보를 포함할 수 있습니다. 이 방법으로 문제를 해결하는 데 도움이 되지 않는 경우 mailto:freebsd-xen@FreeBSD.org[freebsd-xen@FreeBSD.org] 및 mailto:xen-devel@lists.xenproject.org[xen-devel@lists.xenproject.org]으로 발생한 문제에 대한 설명과 가능한 한 많은 정보를 보내 도움을 받으시기 바랍니다.
