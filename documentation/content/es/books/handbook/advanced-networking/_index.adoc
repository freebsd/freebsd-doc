---
title: Capítulo 29. Networking avanzado
part: Parte IV. Comunicaciones en red
prev: books/handbook/network-servers
next: books/handbook/partv
---

[[advanced-networking]]
= Networking avanzado
:doctype: book
:toc: macro
:toclevels: 1
:icons: font
:sectnums:
:sectnumlevels: 6
:source-highlighter: rouge
:experimental:
:skip-front-matter:
:toc-title: Tabla de contenidos
:table-caption: Tabla
:figure-caption: Figura
:example-caption: Ejemplo
:xrefstyle: basic
:relfileprefix: ../
:outfilesuffix:
:sectnumoffset: 29

ifeval::["{backend}" == "html5"]
:imagesdir: ../../../../images/books/handbook/advanced-networking/
endif::[]

ifeval::["{backend}" == "pdf"]
:imagesdir: ../../../../static/images/books/handbook/advanced-networking/
endif::[]

ifeval::["{backend}" == "epub3"]
:imagesdir: ../../../../static/images/books/handbook/advanced-networking/
endif::[]

include::shared/authors.adoc[]
include::shared/releases.adoc[]
include::shared/es/mailing-lists.adoc[]
include::shared/es/teams.adoc[]
include::shared/es/urls.adoc[]

toc::[]

[[advanced-networking-synopsis]]
== Resumen

Este capítulo cubre algunos de los servicios de red que se usan con más frecuencia en sistemas UNIX(R). Para ser más concretos este capítulo explica cómo definir, ejecutar, probar y mantener todos los servicios de red que utiliza FreeBSD. Se muestran además ejemplos de ficheros de configuración que podrá utilizar para sus propios quehaceres.

Después de leer este capítulo habremos aprendido:

* Los conceptos básicos de pasarelas y "routers".
* Cómo poner en funcionamiento dispositivos IEEE 802.11 y Bluetooth(R).
* Cómo configurar FreeBSD para que actúe como un "bridge".
* Cómo poner en funcionamiento un sistema de ficheros en red con NFS.
* Cómo realizar un arranque del sistema por red en máquinas sin disco duro.
* Cómo ejecutar un servidor de información en red para compartir cuentas de usuario mediante NIS.
* Cómo especificar parámetros de configuración automática de red utilizando DHCP.
* Cómo ejecutar un servidor de nombres de dominio.
* Cómo sincronizar la hora y la fecha y ejecutar un servidor horario utilizando el protocolo NTP.
* Cómo ejecutar un servicio de traducción de direcciones de red.
* Cómo gestionar el dæmon inetd.
* Cómo conectar dos computadoras a través de PLIP.
* Cómo habilitar IPv6 en una máquina FreeBSD.
* Cómo configurar ATM sobre FreeBSD 5.X.

Antes de leer este capítulo debería usted:

* Intentar comprender los conceptos básicos de los scripts de [.filename]#/etc/rc#.
* Familiarizarse con la terminología básica de redes.

[[network-routing]]
== Pasarelas y "routers"

Para que una máquina sea capaz de encontrar otra máquina remota a través de la red debe existir mecanismo que describa cómo llegar del origen al destino. Este mecanismo se demonina _routing_ o _encaminamiento_. Una "ruta" es un par definido de direcciones: una dirección de "destino" y una dirección de "pasarela". Éste par indica que para llegar a dicho _destino_ debe efectuarse una comunicación previa con dicha _pasarela_. Exiten tres tipos distintos de destinos: máquinas individuales, subredes y "por defecto". La "ruta por defecto" se utiliza sólamente cuando no se puede aplicar ninguna de las otras rutas existentes. El tema de las rutas por defecto se tratará más adelante con más detalle. También existen tres tipos de pasarelas distintas: máquinas individuales, interfaces (también llamados "enlaces") y direcciones hardware de ethernet (direcciones MAC).

=== Ejemplo

Para ilustrar diferentes aspectos del sistema de encaminamiento veamos el siguiente ejemplo obtenido mediante `netstat`.

[source,bash]
....
% netstat -r
Routing tables

Destination      Gateway            Flags     Refs     Use     Netif Expire

default          outside-gw         UGSc       37      418      ppp0
localhost        localhost          UH          0      181       lo0
test0            0:e0:b5:36:cf:4f   UHLW        5    63288       ed0     77
10.20.30.255     link#1             UHLW        1     2421
ejemplo.com      link#1             UC          0        0
host1            0:e0:a8:37:8:1e    UHLW        3     4601       lo0
host2            0:e0:a8:37:8:1e    UHLW        0        5       lo0 =>
host2.ejemplo.com link#1            UC          0        0
224              link#1             UC          0        0
....

Las primeras dos líneas especifican la ruta por defecto (la cual se comenta en la <<network-routing-default,siguiente sección>>) y la ruta de `máquina local`.

La interfaz (columna `Netif`) que especifica esta tabla de rutas para el destino `localhost` se denomina [.filename]#lo0#, y también se conoce como el dispositivo de " loopback" a de bucle de retorno. Esto viene a decir que el tráfico no debe entregarse a la red puesto que dicho tráfico va destinado a la misma máquina que lo originó.

Lo siguiente que podemos observar son las direcciones que comienzan por `0:e0:`. Son direcciones hardware de Ethernet, llamadas también se direcciones MAC. FreeBSD identifica automáticamente cualquier máquina (`test0` en el ejemplo anterior) que se encuentre en la red local y crea una ruta del estilo que estamos comentando, para entregar el tráfico directamente a través del correspondiente interfaz Ethernet, en este caso [.filename]#ed0#. Existe también un contador (`Expire`) asociado con este tipo de rutas que se usa para borrarlas cuando dicho contador expira. Las rutas para las máquinas de nuestra propia red de de área local se crean dinámicamente utilizando el protocolo ARP (Address Resolution Protocol o Protocolo de Resolución de Direcciones), que se encarga de averiguar la dirección MAC que se corresponde con la dirección IP de la máquina destino.

FreeBSD tambíen utiliza rutas de subred para direccionar la subred local (`10.20.30.255` es la dirección de broadcast para la subred `10.20.30`, y `ejemplo.com` es el nombre de dominio asociado con dicha subred.) La notación `link#1` se refiere a la primera tarjeta Ethernet de la máquina. En este tipo de redes no se especifica ningún interfaz en el campo de `Netif`.

Las rutas de subredes aparecen cuando se asigna una dirección IP a una interfaz, utilizando una máscara de red. También se pueden aprender dinámicamente utilizando demonios de encaminamiento, como routed. Por último estas rutas pueden crearse manualmente de forma explícita; es lo que se conoce con el nombre de rutas estáticas.

La línea de `host1` se refiere a nuestra máquina, que el sistema identifica por la correspondiente dirección Ethernet de la tarjeta de red. FreeBSD sabe que debe utilizar la interfaz de loopback ([.filename]#lo0#) en vez de enviar los paquetes a a través de red.

Las dos líneas que comienzan por `host2` son ejemplos del uso de alias de man:ifconfig[8] alias (consultar la sección sobre Ethernet para averiguar por qué nos podría interesar hacer esto.) El símbolo `=>` después de la interfaz [.filename]#lo0# especifica que no sólo estamos utilizando la interfaz de loopback, si no que además especifica que se trata de un alias. Estas rutas sólo aparecen en las máquinas que implementan el alias, el resto de las máquinas de la subred local solamente poseerán una línea `link#1` para dichas rutas.

La última línea (destino de subred `224`) trata sobre encaminamiento multicast, que cubriremos en otra sección.

Finalmente, se pueden observar varios atributos relacionados con las rutas en la columna de `Flags`. A continuación se muestra una pequeña tabla con el significado de algunos de esos de los atributos o "flags".

[.informaltable]
[cols="1,1", frame="none"]
|===

|U
|Up: La ruta está activa.

|H
|Host: El destino de la ruta es una única máquina.

|G
|Gateway: Envía cualquier cosa para éste destino a través de la pasarela especificada, la cual decidirá cómo encaminar el paquete hasta que eventualmente se alcance el destino.

|S
|Static: Esta ruta se configuró manualmente, y no se ha generado de forma automática por el sistema.

|C
|Clone: Genera una nueva ruta para la máquina a la que nos queremos conectar basándose en la ruta actual. Este tipo de ruta se utiliza normalmente en redes locales.

|W
|WasCloned: Indica una ruta que se auto-configuró basándose en una ruta de red de área local con etiqueta Clone.

|L
|Link: Esta ruta posée referencias a hardware de Ethernet.
|===

[[network-routing-default]]
=== Rutas por defecto

Cuando el sistema local necesita realizar una conexión con una máquina remota se examina la tabla de rutas para determinar si se conoce algún camino para llegar al destino. Si la máquina remota pertenece a una subred que sabemos cómo alcanzar (rutas clonadas) entonces el sistema comprueba si se puede conectar utilizando dicho camino.

Si todos los caminos conocidos fallan al sistema le queda una única opción: la "ruta por defecto". Esta ruta está constituída por un tipo especial de pasarela (normalmente el único "router" presente en la red área local) y siempre posée el "flag" `c` en el campo de "flags". En una LAN, la pasarela es la máquina que posée conectividad con el resto de las redes (sea a través de un enlace PPP, DSL, cable modem, T1 u otra interfaz de red.)

Si se configura la ruta por defecto en una máquina que está actuando como pasarela hacia el mundo exterior la ruta por defecto será el "router" que se encuentre en posesión del proveedor de servicios de internet (ISP).

Vamos a examinar un ejemplo que utiliza rutas por defecto. A continuación se muestra una configuración bastante común:

image::net-routing.png[]

Las máquinas `Local1` y `Local2` se encuentran en nuestro sitio u organización. `Local1` se conecta con un ISP a través de una conexión de modem PPP. El servidor PPP del ISP se conecta a través de una red de área local a otra pasarela utilizando una interfaz externa.

Las rutas por defecto para cada una de las máquinas son las siguientes:

[.informaltable]
[cols="1,1,1", frame="none", options="header"]
|===
| Host
| Default Gateway
| Interface

|Local2
|Local1
|Ethernet

|Local1
|T1-GW
|PPP
|===

Una pregunta bastante frecuente es "?Por qué (o cómo) hacer que la máquina `T1-GW` sea el "router" por defecto para `Local1` en vez de que sea el servidor del ISP al cual se está conectando?".

Recordemos que, como la interfaz PPP está utilizando una dirección de la red local del ISP en nuestro lado de la las rutas para cualquier otra máquina en la red local del proveedor se generarán de forma automática. De este ya sabemos el modo de alcanzar la máquina `T1-GW`, de tal forma que no se necesita un paso intermedio para enviar tráfico al servidor del ISP.

Es frecuente utilizar la dirección `X.X.X.1` como la dirección de la pasarela en la red local. Siguiendo con el ejemplo anterior, si nuestro espacio de direccionamiento local fuera la clase C `10.20.30` y nuestro ISP estuviera utilizando `10.9.9` las rutas por defecto serían:

[.informaltable]
[cols="1,1", frame="none", options="header"]
|===
| Host
| Default Route

|Local2 (10.20.30.2)
|Local1 (10.20.30.1)

|Local1 (10.20.30.1, 10.9.9.30)
|T1-GW (10.9.9.1)
|===

Se puede especificar fácilmente la entrada de la ruta por defecto utilizando el fichero [.filename]#/etc/rc.conf#. En nuestro ejemplo en la máquina `Local2`, se añadió la siguiente línea en dicho fichero:

[.programlisting]
....
defaultrouter="10.20.30.1"
....

También se puede hacer directamente desde la línea de órdenes mediante man:route[8]:

[source,bash]
....
# route add default 10.20.30.1
....

Para obtener más información sobre la manipulación de tablas de rutas se ruega consultar la página de manual man:route[8].

=== Máquinas con doble pertenencia (Dual Homed Hosts)

Existe otro tipo de configuración que debemos describir y que se produce cuando una máquina se sitúa en dos redes distintas al mismo tiempo. Técnicamente hablando cualquier máquina que actúa como pasarela (en el caso anterior utilizando un enlace de PPP) pertenece al tipo de máquinas con doble pertenencia, pero normalmente el término sólo se aplica para describir máquinas que se encuentran directamente conectadas con dos redes de área local.

En un caso la máquina posée dos tarjetas de red Ethernet, cada una de ellas con una dirección de red independiente. En otro caso la máquina puede tener sólo una tarjeta de red, pero utilizar " aliasing" (man:ifconfig[8]). El primer caso se utiliza cuando se necesita usar dos redes Ethernet al mismo tiempo mientras que el segundo caso se utiliza cuando se dispone de un único segmento de red físico pero se han definido dos redes lógicas distintas

En cualquier caso la tabla de rutas se construye de tal forma que cada subred sepa que la máquina es la pasarela definida definida ("inbound route") para la otra subred. Ésta configuración en la que la máquina actúa como "router" entre las dos subredes se usa a menudo cuando queremos implementar filtrado de paquetes o cortafuegos seguridad en un sentido o en ambos.

Si queremos que dicha máquina encamine paquetes entre las dos interfaces es necesario decirle a FreeBSD que active dicha funcionalidad. En la siguiente sección se explica cómo hacerlo.

[[network-dedicated-router]]
=== Construcción de un "route"

Un "router" de red, también llamado pasarela o "route", es simplemente un sistema que reenvía paquetes desde un interfaz hacia otro interfaz. Los estándares Internet y el sentido común aplicado a la ingeniería de redes impiden que FreeBSD incluya por defecto ésta característica. Se puede activar cambiando a `YES` el valor de la siguiente variable en el fichero man:rc.conf[5]:

[.programlisting]
....
gateway_enable=YES          # Set to YES if this host will be a gateway
....

Esta opción modificará la variable de man:sysctl[8] `net.inet.ip.forwarding` al valor `1`. Si en algún momento se necesita detener el "router" de forma temporal basta con asignar a dicha variable el valor `0`. Consulte man:sysctl[8] para más detalles.

Nuestro recién activado "router" necesita rutas para saber a dónde debe enviar el tráfico recibido. Si nuestra red es ña se pueden definir rutas estáticas. FreeBSD incluye por defecto el dæmon de encaminamiento BSD, man:routed[8], que admite RIP (versión 1 y versión 2) e IRDP. El paquete package:net/zebra[] le permitirá usar otros protocolos de encaminamiento dinámico como BGP v4, OSPF v2 y muchos otros. En caso de necesitar características avanzadas de gestión puede usted recurrir a productos comerciales como GateD(R).

Incluso cuando FreeBSD se configura del modo descrito no se cumple completamente con los estándares de Internet respecto a los "routers". Bastará no obstante para poder usarse.

=== Configuración de rutas estáticas

==== Configuración manual

Vamos a suponer que tenemos la siguiente topología de red:

....

    INTERNET
      | (10.0.0.1/24) Router por defecto para Internet
      |
      |Interfaz xl0
      |10.0.0.10/24
   +------+
   |      | RouterA
   |      | (pasarela FreeBSD)
   +------+
      | Interfaz xl1
      | 192.168.1.1/24
      |
  +--------------------------------+
   Red Interna 1       | 192.168.1.2/24
                       |
                   +------+
                   |      | RouterB
                   |      |
                   +------+
                       | 192.168.2.1/24
                       |
                     Red Interna 2
....

En este escenario `RouterA` es nuestra máquina FreeBSD que actúa como pasarela para acceder al resto de internet. Tiene una ruta por defecto que apunta a `10.0.0.1` que le permite conectarse con el mundo exterior. Vamos a suponer también que `RouterB` se encuentra configurado de forma adecuada que sabe cómo llegar a cualquier sitio que necesite. Esto es sencillo viendo nuestra topología de red, basta con añadir una ruta por defecto en la máquina `RouterB` utilizando `192.168.1.1` como "router".

Si observamos la tabla de rutas de `RouterA` veremos algo como lo siguiente:

[source,bash]
....
% netstat -nr
Routing tables

Internet:
Destination        Gateway            Flags    Refs      Use  Netif  Expire
default            10.0.0.1           UGS         0    49378    xl0
127.0.0.1          127.0.0.1          UH          0        6    lo0
10.0.0/24          link#1             UC          0        0    xl0
192.168.1/24       link#2             UC          0        0    xl1
....

Con la tabla de rutas actual `RouterA` no es capaz de alcanzar la red interna 2. Esto es así porque no posee ninguna ruta para la red `192.168.2.0/24`. Una forma de mitigar esto es añadir de forma manual la ruta que falta. La siguiente orden añade la red interna 2 a la tabla de rutas de la máquina `RouterA` utilizando `192.168.1.2` como siguiente salto:

[source,bash]
....
# route add -net 192.168.2.0/24 192.168.1.2
....

Ahora `RouterA` puede alcanzar cualquier máquina en la red `192.168.2.0/24`.

==== Cómo hacer la configuración persistente

El ejemplo anterior es perfecto en tanto que resuelve el problema de encaminamiento entre redes pero existe un problema. La información de encaminamiento desaparecerá si se reinicia la máquina. La forma de evitarlo es añadir las rutas estáticas a [.filename]#/etc/rc.conf#:

[.programlisting]
....
# Añade la red interna 2 como una ruta estática
static_routes="redinterna2"
route_internalnet2="-net 192.168.2.0/24 192.168.1.2"
....

La variable de configuración `static_routes` es una lista de cadenas separadas por espacios. Cada cadena identifica un nombre para la ruta que se desea definir. En el ejemplo anterior sólamente se dispone de una cadena dentro de la variable `static_routes`. Esta cadena es _redinterna2_. A continuación se añade otra variable de configuración denominada `route_redinterna2` donde se escriben todos los parámetros de configuración que normalmente utilizaríamos normalmente utilizaríamos con man:route[8]. En el ejemplo que estamos comentando se utilizaría la siguiente orden:

[source,bash]
....
# route add -net 192.168.2.0/24 192.168.1.2
....

De tal forma que la variable debería contener `"-net 192.168.2.0/24 192.168.1.2"`.

Como ya se ha comentado anteriormente podemos especificar más de una cadena en la variable `static_routes`. Esto nos permite crear varias rutas estáticas. Las siguientes línas muestran un ejemplo donde se añaden rutas estáticas para las redes `192.168.0.0/24` y `192.168.1.0/24` en un "router"imaginario:

[.programlisting]
....
static_routes="red1 red2"
route_red1="-net 192.168.0.0/24 192.168.0.1"
route_red2="-net 192.168.1.0/24 192.168.1.1"
....

=== Propagación de rutas

Ya hemos comentado cómo se definen las rutas para el mundo exterior pero no hemos comentado nada sobre cómo haremos que el mundo exterior nos encuentre a nosotros.

También hemos aprendido que las tablas de rutas se pueden construír de tal forma que un grupo de tráfico (perteneciente a un espacio de direcciones determinado) se reenvíe a una máquina específica de la red, que se encargará de reenviar los paquetes hacia adentro.

Cuando se obtiene un espacio de direcciones para la organización el proveedor de servicios modifica sus tablas de rutas para que todo el tráfico para nuestra subred se encamine a través del enlace PPP hasta alcanzarnos. Pero ?cómo conocen las organizaciones dispersas a través del país que deben enviar los paquetes dirigidos a nosotros hacia nuestro ISP?

Existe un sistema (muy similar al sistema de nombres de dominio, DNS) que se encarga de controlar todos los espacios de direcciones que se encuentran actualmente repartidos y que además define sus puntos de conexión con el "backbone" de internet. El "backbone" está formado por las principales líneas de de comunicacion que se encargan de transportar el tráfico de internet a través del país y del mundo entero. Cada máquina del "backbone" dispone de una copia de un conjunto maestro de tablas de rutas gracias a las cuales pueden dirigir el tráfico para una red particular hacia una determinada red de transporte de dicho "backbone". Una vez en la red de transporte adecuada el tráfico se encamina a través de un número indeterminado de redes de proveedores de servicio hasta que se alcanza la red de destino final.

Una de las tareas que debe realizar el proveedor de servicio servicio consiste en anunciarse a las organizaciones del consiste en anunciarse a las organizaciones del "backbone" como el punto de conexión principal (y por tanto como el camino de entrada) para alcanzar las redes de sus clientes. Este proceso se denomina propagación de rutas.

=== Solución de problemas

En algunas ocasiones surgen problemas con la propagación de las rutas y algunas organizaciones son incapaces de conectarse con nuestra subred. Quizá la orden más útil para averiguar dónde se está interrumpiendo el sistema de encaminamiento sea man:traceroute[8]. Se puede usar también cuando somos nosotros los que no podemos alcanzar alguna red externa (por ejemplo cuando man:ping[8] falla).

man:traceroute[8] se ejecuta pasandole como parámetro el nombre de la máquina remota a la que nos queremos conectar. Esta orden muestra por pantalla lás máquinas que actúan de pasarela a lo largo del camino. El proceso termina bien porque se alcanza el destino o bien porque algún "router" intermedio no puede conectarse con el siguiente salto, o lo desconoce.

Si quiere saber más sobre esto consulte la página man de man:traceroute[8].

=== Rutas multicast

FreeBSD soporta tanto aplicaciones multicast como encaminamiento multicast de forma nativa. Las aplicaciones multicast no necesitan ninguna configuración especial en FreeBSD; estas aplicaciones se ejecutan tal cual. El encaminamiento multicast necesita para ser usado que se compile dicho soporte en el núcleo de FreeBSD:

[.programlisting]
....
options MROUTING
....

Se debe configurar además el dæmon de encaminamiento multicast, man:mrouted[8], para establecer túneles y ejecutar DVMRP utilizando [.filename]#/etc/mrouted.conf#. Se pueden encontrar más detalles sobre cómo realizar una configuración de multicast en man:mrouted[8].

[[network-wireless]]
== Redes sin cables ("wireless")

=== Introducción

Puede resultar muy útil el ser capaz de utilizar una computadora sin la molestia de tener un cable de red colgando de la máquina en todo momento. FreeBSD puede utilizarse como un cliente de "wireless" e incluso como un "punto de acceso".

=== Modos de operación Wireless

Existen dos formas diferentes de configurar dispositivos wireless 802.11: BSS e IBSS.

==== Modo BSS

El modo BSS es el que se utiliza normalmente. Este modo también se denomina modo infraestructura. En esta configuración se conectan un determinado número de puntos de acceso a una red cableada. Cada red Cada red "wireless" posée su propio nombre. Este nombre es el SSID de la red.

Los clientes "wireless" se conectan a estos puntos de acceso. El estándar IEEE 802.11 define el protocolo que se utiliza para realizar esta conexión. Un cliente "wireless" puede asociarse con una determinada red "wireless" especificando el SSID. Un cliente "wireless" también puede asociarse a cualquier red que se encuentre disponible; basta con no especificar ningún SSID.

==== Modo IBSS

El modo IBSS, también conocido como modo ad-hoc, se ha diseñado para facilitar las conexiones punto a punto. En realidad existen dos tipos distintos de modos ad-hoc. Uno es el modo IBSS, también conocido como modo ad-hoc o modo ad-hoc del IEEE. Este modo se encuentra especificado en el estándar IEEE 802.11. El segundo tipo se denomina modo ad-hoc de demostración o modo ad-hoc de Lucent (y algunas veces, también se le llama simplemente modo ad-hoc, lo cual es bastante confuso). Este es el modo de funcionamiento antíguo, anterior al estándar 802.11, del modo ad-hoc debería utilizarse sólo en instalaciones propietarias. No profundizaremos más sobre estos modos de funcionamiento.

=== Modo infraestructura

==== Puntos de acceso

Los puntos de acceso son dispositivos de red "wireless" que funcionan de forma equivalente a los "hubs" o concentradores, permitiendo que varios clientes " wireless" se comuniquen entre sí. A menudo se utilizan varios puntos de acceso para cubrir un área determinada como una casa, una oficina u otro tipo de localización delimitada.

Los puntos de acceso poseen típicamente varias conexiones de red: la tarjeta "wireless" y una o más tarjetas Ethernet que se utilizan para comunicarse con el resto de la red.

Los puntos de acceso se pueden comprar como tales pero también se puede configurar un sistema FreeBSD para crear nuestro propio punto de acceso "wireless" utilizando un determinado tipo de tarjetas "wireless" que poseen tales capacidades de configuración. Existe una gran cantidad de fabricantes de hardware que distribuyen puntos de acceso y tarjetas de red "wireless", aunque las capacidades de unos y otras varín.

==== Construcción de un punto de acceso basado en FreeBSD

===== Requisitos

Para crear nuestro propio punto de acceso con FreeBSD debemos utilizar un determinado tipo de tarjeta "wireless". Por el momento, sólo las tarjetas con el chip Prism nos permiten hacer un punto de acceso. También vamos a necesitar una tarjeta para red cableada que sea soportada por el sistema (esto no es muy complicado dada la ingente cantidad de dispositivos de este tipo que funcionan en FreeBSD). Para este ejemplo vamos a suponer que queremos puentear (man:bridge[4]) todo el tráfico entre la red cableada y la red inalámbrica.

El uso como punto de acceso "wireless" (también denominado _hostap_) funciona mejor con determinadas versiones del " firmware". Las tarjetas con chip Prism2 deben disponer de la versión 1.3.4 (o superior) del " firmware". Los chips Prism2.5 y Prism3 deben disponer de la versión 1.4.9 o superior del "firmware". Las versiones más antíguas de estos " firmwares" pueden no funcionar correctamente. A día de hoy la única forma de actualizar el " firmware" de las tarjetas es usando las herramientas que proporciona el fabricante para Windows(R).

===== Puesta en marcha del sistema

Primero debemos asegurarnos de que el sistema reconoce la tarjeta "wireless":

[source,bash]
....
# ifconfig -a
wi0: flags=8843<UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST> mtu 1500
        inet6 fe80::202:2dff:fe2d:c938%wi0 prefixlen 64 scopeid 0x7
        inet 0.0.0.0 netmask 0xff000000 broadcast 255.255.255.255
        ether 00:09:2d:2d:c9:50
        media: IEEE 802.11 Wireless Ethernet autoselect (DS/2Mbps)
        status: no carrier
        ssid ""
        stationname "nodo Wireless FreeBSD"
        channel 10 authmode OPEN powersavemode OFF powersavesleep 100
        wepmode OFF weptxkey 1
....

No se preocupe si no entiende algo de la configuración anterior, lo importante es asegurarse de que el sistema muestra algo parecido, lo cual nosindicará que la tarjeta "wireless" ha sido correctamente reconocida por FreeBSD. Si el interfaz inalámbrico no es reconocido correctamente y se está utilizando una tarjeta PC Card consulte man:pccardc[8] y man:pccardd[8], en las que tiene mucha información al respecto.

A continuación, para que podamos disponer de un "bridge" deberá cargar el módulo del kernel man:bridge[4] por el sencillo procedimiento de ejecutar la siguiente orden:

[source,bash]
....
# kldload bridge
....

No debería aparecer mensaje de error alguno al ejecutar dicha orden. Si apareciera alguno quizás deba compilar el kernel del sistema con man:bridge[4] incluído. La sección <<network-bridging,Bridging>> de éste manual incluye información abundante para llevar a buen puerto esa tarea.

Una vez que tenemos el soporte de "bridging" cargado debemos indicar a FreeBSD qué interfaces se desean puentear. Para ello emplearemos man:sysctl[8]:

[source,bash]
....
# sysctl net.link.ether.bridge=1
# sysctl net.link.ether.bridge_cfg="wi0,xl0"
# sysctl net.inet.ip.forwarding=1
....

En FreeBSD 5.2-RELEASE y posteriores se deben emplear las siguientes opciones en lugar de las anteriormente expuestas:

[source,bash]
....
# sysctl net.link.ether.bridge.enable=1
# sysctl net.link.ether.bridge.config="wi0,xl0"
# sysctl net.inet.ip.forwarding=1
....

Ahora es el momento de configurar la tarjeta de red inalámbrica. La siguiente orden convierte la tarjeta en un punto de acceso:

[source,bash]
....
# ifconfig wi0 ssid mi_red channel 11 media DS/11Mbps mediaopt hostap up stationname "PA FreeBSD"

....

La línea de man:ifconfig[8] levanta el interfaz [.filename]#wi0#, configura el SSID con el valor de _mi_red_ y también el nombre de la estación como _FreeBSD_. La opción `media DS/11Mbps` configura la tarjeta a 11Mbps. Ésto es necesario para que cualquier valor que se necesite asignar a `mediaopt` surta efecto. La opción `mediaopt hostap` sitúa el interfaz en modo punto de acceso. La opción `channel 11` configura la tarjeta para que use el canal de radio número 11. En man:wicontrol[8] encontraráa rangos de canales válidos para varios dominios regulatorios. Por favor, tenga en cuenta que no todos los canales son legales en todos los países.

Despues de esto deberíamos disponer de un punto de acceso completamente funcional y en ejecución. Le animamos a consultar man:wicontrol[8], man:ifconfig[8] y man:wi[4] para máss información.

También le recomemdamos leer la sección sobre cifrado que econtrará más adelante.

===== Información de estado

Una vez que el punto de acceso estáconfigurado resulta interesante poder obtener información acerca de los clientes que estén asociados. La persona encargada de la administración del punto de acceso puede ejecutar cuando estime oportuno lo siguiente:

[source,bash]
....
# wicontrol -l
1 station:
00:09:b7:7b:9d:16  asid=04c0, flags=3<ASSOC,AUTH>, caps=1<ESS>, rates=f<1M,2M,5.5M,11M>, sig=38/15
....

Lo que aquí se muestra indica que hay una única estación asociada y nos suministra sus parámetros. Los valores de señal que se muestran se deben tomar sólo como indicaciones aproximadas de la fuerza de dicha señal. Su traducción a dBm u otras unidades varía según la versión del " firmware" de la tarjeta que se use.

==== Clientes

Un cliente "wireless" es un sistema que se comunica con un punto de acceso o directamente con otro cliente "wireless".

Generalmente los clientes "wireless" sólo poseen un dispositivo de red: la tarjeta de red inalámbrica.

Existen varias formas de configurar un cliente " wireless" basadas en los distintos modos inalámbricos, normalmente reducidos a BSS (o modo infraestructura, que requiere de un punto de acceso) y el modo IBSS (modo ad-hoc, o modo punto a punto). En nuestro ejemplo usaremos el más famoso de ambos, el BSS, para comunicarnos con un punto de acceso.

===== Requisitos

Sólamente existe un requisito real para configurar un sistema FreeBSD como cliente inalámbrico: usar una tarjeta de red inalámbrica soportada por el sistema.

===== Ejecución de un cliente inalámbrico FreeBSD

Para utilizar una red inalámbrica se necesitan conocer algunos conceptos básicos de redes de redes wireless. En nuestro ejemplo queremos conectarnos a la red inalámbrica _mi_red_ y queremos hacerlo con el soporte de cifrado desactivado.

[NOTE]
====
En este ejemplo no se utiliza cifrado, lo cual resulta ser bastante peligroso. En la próxima sección aprenderemos cómo activar el sistema de cifrado común el los dispositivos inalámbricos, por qué resulta importante hacerlo y por qué algunas tecnologías de cifrado no son suficientes para protegernos completamente.
====

Asegúrese de que FreeBSD reconoce su tarjeta de red inalámbrica:

[source,bash]
....
# ifconfig -a
wi0: flags=8843<UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST> mtu 1500
        inet6 fe80::202:2dff:fe2d:c938%wi0 prefixlen 64 scopeid 0x7
        inet 0.0.0.0 netmask 0xff000000 broadcast 255.255.255.255
        ether 00:09:2d:2d:c9:50
        media: IEEE 802.11 Wireless Ethernet autoselect (DS/2Mbps)
        status: no carrier
        ssid ""
        stationname "FreeBSD Wireless node"
        channel 10 authmode OPEN powersavemode OFF powersavesleep 100
        wepmode OFF weptxkey 1
....

A continuación debemos especificar los parámetros correctos para nuestra red:

[source,bash]
....
# ifconfig wi0 inet 192.168.0.20 netmask 255.255.255.0 ssid mi_red
....

Sustituya `192.168.0.20` y `255.255.255.0` con una dirección IP y máscara de red que se adecúen con el espacio de direccionamiento de la red cableada. Recordemos que nuestro punto de acceso está puenteando la red inalámbrica y la red de cable, de modo que para el resto de dispositivos de la red el cliente inalábrico se muestra como un elemento más de la red cableada.

Llegados a este punto deberíamos poder hacer ping a las máquinas de la red cableada como si estuviéramos compartiendo el mismo enlace físico cableado.

Si se presentan problemas con la conexión inalámbrica se puede comprobar si la tarjeta " wireless" se encuentra correctamente asociada (conectada) con el punto de acceso:

[source,bash]
....
# ifconfig wi0
....

ebería devolver algún tipo de información entre la que deberíamos observar la siguiente línea:

[source,bash]
....
status: associated
....

Si no aparece la palabra `associated` puede ser que nos encontremos fuera de la cobertura proporcionada por el punto de acceso o puede ser que necesitemos activar el cifrado, aunque éstos no son los únicos problemas con los que nos podemos encontrar.

==== Cifrado

El cifrado, también llamado codificación, de una red inalámbrica es un proceso importante porque, a diferencia de lo que ocurre con las redes cableadas convencionales, las redes inalámbricas no se pueden restringir a un espacio físico determinado. Los datos que viajan a través de ondas de radio se difunden a través de las paredes y alcanzan a los vecinos más cercanos. Aquí es donde entra en en juego el sistema de cifrado. El cifrado se emplea para evitar que cualquiera pueda examinar los datos enviados a través del aire.

Los dos métodos más comunes para realizar el cifrado de datos entre el cliente y el punto de acceso son WEP e man:ipsec[4].

===== WEP

WEP son las siglas de Wired Equivalency Protocol. WEP es un un intento de crear redes inalámbricas al menos tan seguras omo las redes cableadas o al menos de seguridad equivalente a dichas redes. Por desgracia el sistema WEP es débil y resulta bastante sencillo de romper. Esto significa que cuando se transmite información de carácter crítico no se debe confiar únicamente en este sistema de cifrado.

No obstante es mejor que no utilizar nada; puede activar WEP en el sistema que hace de punto de acceso mediante:

[source,bash]
....
# ifconfig wi0 inet up ssid
              mi_red wepmode on wepkey
              0x1234567890 media DS/11Mbps
              mediaopt hostap
....

y en un cliente inalámbrico mediante la siguiente orden:

[source,bash]
....
# ifconfig wi0 inet 192.168.0.20 netmask 255.255.255.0 ssid mi_red wepmode on wepkey 0x1234567890
....

Por favor, tenga un poco de sentido común y reemplace la clave _0x1234567890_ por otra clave menos obvia.

===== IPsec

man:ipsec[4] es una herramienta más robusta y potente para cifrar datos que se mueven a través de una red. Es el mecanismo más conveniente para asegurar los datos de una red inalámbrica. Tiene más información sobre el protocolo man:ipsec[4] y cómo utilizarlo en la sección crossref:security[ipsec,IPsec] de este manual.

==== Herramientas

No hay muchas herramientas disponibles si se quiere depurar y monitorizar redes inalámbricas pero en el siguiente apartado mostraremos cómo utilizar algunas de ellas.

===== El paquete bsd-airtools

El paquete bsd-airtools es un conjunto muy completo de herramientas "wireless" que se pueden utilizar para multitud de tareas, entre las cuales podemos citar citar el desciframiento de claves WEP, detección de puntos de de acceso, monitorización de la señal de radio, etc.

El paquete bsd-airtools se puede instalar como "port" desde package:net/bsd-airtools[]. La información relacionada con los "ports" puede encontrarse en la sección crossref:ports[ports,Instalación de aplicaciones: «packages» y ports] de este manual.

El programa `dstumbler` es una herramienta que permite descubrir puntos de acceso y entre otras cosas muestra de forma gráfica la relación señal / ruido del enlace. Si se experimentan problemas para acceder a un determinado punto de acceso `dstumbler` puede ser muy útil.

Para probar la seguridad de la red inalámbrica se puede usar "dweputils", concretamente las órdenes `dwepcrack`, `dwepdump` y `dwepkeygen`. Estas órdenes permiten determinar hasta qué punto la seguridad que ofrece WEP es suficiente para nuestras necesidades.

===== Las utilidades `wicontrol`, `ancontrol` y `raycontrol`

Mediante estas herramientas se puede controlar el comportamiento de la tarjeta de red inalámbrica. En los ejemplos anteriores se ha utilizado man:wicontrol[8] debido a que la tarjeta de red del ejemplo utiliza el interfaz [.filename]#wi0#. Si se posée una tarjeta "wireless" de Cisco dicha tarjeta se mostrará en el sistema mediante el interfaz [.filename]#an0# y por lo tanto la orden equivalente que se debe usar será man:ancontrol[8].

===== `ifconfig`

Con man:ifconfig[8] se puede utilizar unas cuantas de las opciones que se pueden usar con man:wicontrol[8], pero no obstante no posée todas las funcionalidades que proporciona man:wicontrol[8]. Se recomienda leer man:ifconfig[8] para conocer los detalles de los parámetros y opciones que admite.

==== Tarjetas de Red inalámbricas soportadas

===== Puntos de acceso

Las únicas tarjetas que soportan el modo de funcionamiento funcionamiento BSS (pueden funcionar como puntos de acceso) son los dispositivos basados en el chip Prism 2, 2.5 ó 3. Consulte man:wi[4] para ver una lista completa de ellos.

===== Clientes

FreeBSD soporta casi todas las tarjetas inalámbricas 802.11b 802.11b que se encuentran actualmente en el mercado. La mayoría de las tarjetas basadas en los chips Prism, Spectrum24, Spectrum24, Hermes, Aironet y Raylink tambíen funcionan en modo IBSS (modos ad-hoc, punto a punto y BSS).

[[network-bluetooth]]
== Bluetooth

=== Introducción

Bluetooth es una tecnología inalámbrica que opera en banda de 2.4 GHz (donde no se necesita licencia). Se trata de una tecnología pensada para la creación de redes de ámbito personal (de cobertura reducida, normalmente de unos 10 metros). Las redes se suelen construir en modo "ad-hoc" utilizando dispositivos heterogéneos como teléfonos móviles, dispositivos manuales ("handhelds") y computadoras portátiles. A diferencia de otras tecnologías inalámbricas como Wi-Fi, Bluetooth ofrece perfiles de servicio más detallados; por ejemplo un perfil para actuar como un servidor de ficheros basado en FTP, para la difusión de ficheros ("file pushing"), para el transporte de voz, para la emulación de línea serie y muchos más.

La pila de Bluetooth en FreeBSD se implementa utilizando el entorno de Netgraph (véase man:netgraph[4]). La mayoría de los dispositivos USB Bluetooth se pueden utilizar mediante el controlador man:ng_ubt[4]. Los dispositivos Bluetooth basados en el chip Broadcom BCM2033 están soportados mediante los controladores man:ubtbcmfw[4] y man:ng_bt3c[4]. Los dispositivos Bluetooth basados en la interfaz serie o de Rayos Infrarrojos (UART) se controlan mediante man:sio[4], man:ng_h4[4] y man:hcseriald[8]. Este capítulo describe el uso de dispositivos Bluetooth USB. El soporte para Bluetooth se encuentra en las versiones de FreeBSD 5.0 y posteriores.

=== Instalación del dispositivo

Por defecto los controladores de los dispositivos Bluetooth se encuentran disponibles como módulos del kernel. Antes de enchufar el dispositivo Bluetooth se debe cargar el módulo correspondiente dentro del núcleo.

[source,bash]
....
# kldload ng_ubt
....

Si el dispositivo Bluetooth se encuentra conectado cuando el sistema arranca se debe cargar el módulo modificando a tal efecto el fichero [.filename]#/boot/loader.conf#.

[.programlisting]
....
ng_ubt_load="YES"
....

Al conectar el dispositivo Bluetooth aparecerá en la consola (o en syslog) la siguiente información:

[source,bash]
....
ubt0: vendor 0x0a12 product 0x0001, rev 1.10/5.25, addr 2
ubt0: Interface 0 endpoints: interrupt=0x81, bulk-in=0x82, bulk-out=0x2
ubt0: Interface 1 (alt.config 5) endpoints: isoc-in=0x83, isoc-out=0x3,
      wMaxPacketSize=49, nframes=6, buffer size=294
....

Se debe copiar [.filename]#/usr/shared/examples/netgraph/bluetooth/rc.bluetooth# a algún lugar más conveniente, por ejemplo [.filename]#/etc/rc.bluetooth#. Este script se usa para ejecutar y detener la pila Bluetooth del sistema. Se suele recomendar quitar la pila antes de desenchufar el dispositivo pero si no se hace no debería producirse ningún desastre. Cuando se arranca la pila aparece un mensaje similar a este:

[source,bash]
....
# /etc/rc.bluetooth start ubt0
BD_ADDR: 00:02:72:00:d4:1a
Features: 0xff 0xff 0xf 00 00 00 00 00
<3-Slot> <5-Slot> <Encryption> <Slot offset>
<Timing accuracy> <Switch> <Hold mode> <Sniff mode>
<Park mode> <RSSI> <Channel quality> <SCO link>
<HV2 packets> <HV3 packets> <u-law log> <A-law log> <CVSD>
<Paging scheme> <Power control> <Transparent SCO data>
Max. ACL packet size: 192 bytes
Number of ACL packets: 8
Max. SCO packet size: 64 bytes
Number of SCO packets: 8
....

=== Interfaz de la controladora de la máquina (HCI)

La interfaz de la Controladora de la Máquina (Host Controller Interface) proporciona una interfaz de órdenes para la controladora de banda base y para el gestor de enlace, y permite acceder al estado del hardware y a los registros de control. Esta interfaz proporciona una capa de acceso homogénea para todos los dispositivos Bluetooth de banda base. La capa HCI de la máquina intercambia órdenes y datos con el firmware del HCI presente en el dispositivo Bluetooth. El driver de la capa de transporte de la controladora de la máquina (es decir, el driver del bus físico) proporciona ambas capas de HCI la posibilidad de intercambiar información entre ellas.

Se crea un nodo Netgraph de tipo _HCI_ para cada dispositivo Bluetooth. El nodo Netgraph HCI se conecta normalmente con el nodo que representa el controlador del dispositivo Bluetooth de la máquina (sentido de bajada) y con el nodo Netgraph L2CAP en el sentido de subida. Todas las operaciones HCI se realizan sobre el nodo Netgraph HCI y no sobre el el nodo que representa al dispositivo. El nombre por defecto para el nodo HCI es "devicehci". Para obtener más detalles, por favor consulte la página del manual de man:ng_hci[4].

Una de las tareas más importantes que se deben realizar es el descubrimiento automático de otros dispositivos Bluetooth que se encuentren dentro del radio de cobertura. Esta operación se denomina en inglés _inquiry_ (consulta). Esta operación o otras operaciones HCI relacionadas se realizan mediante la utilidad man:hccontrol[8]. El siguiente ejemplo muestra cómo descubrir dispositivos en pocos segundos. Tenga siempre presente que un dispositivo remoto sólo contesta a la consulta si se encuentra configurado en modo descubrimiento (_discoverable mode_).

[source,bash]
....
% hccontrol -n ubt0hci inquiry
Inquiry result, num_responses=1
Inquiry result #0
       BD_ADDR: 00:80:37:29:19:a4
       Page Scan Rep. Mode: 0x1
       Page Scan Period Mode: 00
       Page Scan Mode: 00
       Class: 52:02:04
       Clock offset: 0x78ef
Inquiry complete. Status: No error [00]
....

`BD_ADDR` es la dirección identificativa única del dispositivo Bluetooth, similar a las direcciones MAC de las tarjetas Ethernet. Esta dirección se necesita para transmitir otro tipo de información a otros dispositivos. Se puede asignar un nombre más significativo para los humanos en la variable BD_ADDR. El fichero [.filename]#/etc/bluetooth/hosts# contiene información relativa a los dispositivos Bluetooth conocidos. El siguiente ejemplo muestra cómo obtener un nombre significativo para los humanos que fué asignado a un dispositivo remoto:

[source,bash]
....
% hccontrol -n ubt0hci remote_name_request 00:80:37:29:19:a4
BD_ADDR: 00:80:37:29:19:a4
Name: Pav's T39
....

Si se realiza una consulta (inquiry) sobre el dispositivo Bluetooth remoto, dicho dispositivo identificará nuestro computador como "nombre.de.su.sistema (ubt0)". El nombre asignado al dispositivo local se puede modificar en cualquier momento.

El sistema Bluetooth proporciona una conexión punto a punto (con sólo dos unidades Bluetooth involucradas) o también una conexión punto multipunto. En el último caso, la conexión se comparte entre varios dispositivos Bluetooth. El siguiente ejemplo muestra como obtener una lista de las conexiones de banda base activas en el dispositivo local:

[source,bash]
....
% hccontrol -n ubt0hci read_connection_list
Remote BD_ADDR    Handle Type Mode Role Encrypt Pending Queue State
00:80:37:29:19:a4     41  ACL    0 MAST    NONE       0     0 OPEN
....

Resulta útil disponer de un _manejador de la conexión_ cuando se necesita terminar la conexión de banda base. Es importante recalcar que normalmente no es necesario realizar esta terminación de forma manual. La pila Bluetooth puede concluír automáticamente las conexiones de banda base que se encuentren inactivas.

[source,bash]
....
# hccontrol -n ubt0hci disconnect 41
Connection handle: 41
Reason: Connection terminated by local host [0x16]
....

Se ruega consultar la salida de la orden `hccontrol help` para obtener un listado completo de las órdenes HCI disponibles. La mayoría de estas órdenes no requiren privilegios de superusuario.

=== Protocolo de adaptación y de control de enlace a nivel lógico (L2CAP)

El protocolo L2CAP (Logical Link Control and Adaptation Protocol) proporciona servicios de datos tanto orientados a conexión como no orientados a conexión a los protocolos de las capas superiores, junto con facilidades de multiplexación y de segmentacion y reensamblaje. L2CAP permite que los protocolos de capas superiores puedan transmitir y recibir paquetes de datos L2CAP de hasta 64 kilobytes de longitud.

L2CAP se basa en el concepto de _canales_. Un canal es una conexión lógica que se sitúa sobre la conexión de banda base. Cada canal se asocia a un único protocolo. Cada paquete L2CAP que se recibe a un canal se redirige al protocolo superior correspondiente. Varios canales pueden operar sobre la misma conexión de banda base, pero un canal no puede tener asociados más de un protocolo de alto nivel.

Para cada dispositivo Bluetooth se cre un único nodo Netgraph de tipo _l2cap_. El nodo L2CAP se conecta normalmente conectado al nodo Netgraph HCI (hacia abajo) y con nodos Bluetooth tipo "sockets" hacia arriba. El nombre por defecto para el nodo Netgraph L2CAP es "devicel2cap". Para obtener más detalles se ruega consultar la página del manual man:ng_l2cap[4].

man:l2ping[8] le será muy útil para hacer ping a otros dispositivos. Algunas implementaciones de Bluetooth no devuelven todos los datos que se envían, de tal forma que el valor _0 bytes_ que se observa a continuación es normal:

[source,bash]
....
# l2ping -a 00:80:37:29:19:a4
0 bytes from 0:80:37:29:19:a4 seq_no=0 time=48.633 ms result=0
0 bytes from 0:80:37:29:19:a4 seq_no=1 time=37.551 ms result=0
0 bytes from 0:80:37:29:19:a4 seq_no=2 time=28.324 ms result=0
0 bytes from 0:80:37:29:19:a4 seq_no=3 time=46.150 ms result=0
....

La herramienta man:l2control[8] se utiliza para realizar varias operaciones sobre los nodos L2CAP. Este ejemplo muestra cómo obtener la lista de conexiones lógicas (canales) y la lista de conexiones de banda base (física) que mantiene el dispositivo local:

[source,bash]
....
% l2control -a 00:02:72:00:d4:1a read_channel_list
L2CAP channels:
Remote BD_ADDR     SCID/ DCID   PSM  IMTU/ OMTU State
00:07:e0:00:0b:ca    66/   64     3   132/  672 OPEN
% l2control -a 00:02:72:00:d4:1a read_connection_list
L2CAP connections:
Remote BD_ADDR    Handle Flags Pending State
00:07:e0:00:0b:ca     41 O           0 OPEN
....

Otra herramienta de diagnóstico interesante es man:btsockstat[1]. Realiza un trabajo similar a la orden man:netstat[1], pero en este caso para las estructuras de datos relacionadas con el sistema Bluetooth. A continuación se muestra la información relativa a la misma conexión lógica del ejemplo anterior.

[source,bash]
....
% btsockstat
Active L2CAP sockets
PCB      Recv-Q Send-Q Local address/PSM       Foreign address   CID   State
c2afe900      0      0 00:02:72:00:d4:1a/3     00:07:e0:00:0b:ca 66    OPEN
Active RFCOMM sessions
L2PCB    PCB      Flag MTU   Out-Q DLCs State
c2afe900 c2b53380 1    127   0     Yes  OPEN
Active RFCOMM sockets
PCB      Recv-Q Send-Q Local address     Foreign address   Chan DLCI State
c2e8bc80      0    250 00:02:72:00:d4:1a 00:07:e0:00:0b:ca 3    6    OPEN
....

=== Protocolo RFCOMM

El protocolo RFCOMM proporciona emulación de puertos serie a través del protocolo L2CAP. Este protocolo se basa en el estándar de la ETSI denominado TS 07.10. RFCOMM es un protoclo de transporte sencillo, con soporte para hasta 9 puertos serie RS-232 (EIATIA-232-E). El protocolo RFCOMM permite hasta 60 conexiones simultaneas (canales RFCOMM) entre dos dispositivos Bluetooth.

Para los propósitos de RFCOMM, un camino de comunicación involucra siempre a dos aplicaciones que se ejecutan en dos dispositivos distintos (los extremos de la comunicación). Entre ellos existe un segmento que los comunica. RFCOMM pretende cubrir aquellas aplicaciones que utilizan los puertos serie de las máquinas donde se ejecutan. El segmento de comunicación es un enlace Bluetooth desde un dispositivo al otro (conexión directa).

RFCOMM trata únicamente con la conexión de dispositivos directamente, y también con conexiones entre el dispositivo y el modem para realizar conexiones de red. RFCOMM puede soportar otras configuraciones, tales como módulos que se comunican via Bluetooth por un lado y que proporcionan una interfaz de red cableada por el otro.

En FreeBSD el protocolo RFCOMM se implementa utilizando la capa de "sockets" de Bluetooth.

=== Enparejamiento de dispositivos

Por defecto, la comunicación Bluetooth no se valida, por lo que cualquier dispositivo puede en principio hablar con cualquier otro. Un dispositivo Bluetooth (por ejemplo un teléfono celular) puede solicitar autenticación para realizar un determinado servicio (por ejemplo para el servicio de marcación por modem). La autenticación de Bluetooth normalmente se realiza utilizando _códigos PIN_. Un código PIN es una cadena ASCII de hasta 16 caracteres de longitud. Los usuarios deben introducir el mismo código PIN en ambos dispositivos. Una vez que el usuario ha introducido el PIN adecuado ambos dispositivos generan una _clave de enlace_. Una vez generada, la clave se puede almacenar en el propio dispositivo o en un dispositivo de almacenamiento externo. La siguiente vez que se comuniquen ambos dispositivos se reutilizará la misma clave. El procedimiento descrito hasta este punto se denomina _emparejamiento (pairing)_. Es importante recordar que si la clave de enlace se pierde en alguno de los dispositivos involucrados se debe volver a ejecutar el procedimiento de emparejamiento.

El dæmon man:hcsecd[8] se encarga de gestionar todas las peticiones de autenticación Bluetooth. El archivo de configuración predeterminado se denomina [.filename]#/etc/bluetooth/hcsecd.conf#. A continuación se muestra una sección de ejemplo de un teléfono celular con el código PIN arbitrariamente fijado al valor "1234": 

[.programlisting]
....
device {
        bdaddr  00:80:37:29:19:a4;
        name    "Pav's T39";
        key     nokey;
        pin     "1234";
      }
....

No existe ninguna limitación en los códigos PIN a excepción de su longitud. Algunos dispositivos (por ejemplo los dispositivos de mano Bluetooth) pueden obligar a escribir un número predeterminado de caracteres para el código PIN. La opción `-d` fuerza al dæmon man:hcsecd[8] a permanecer ejecutádose en primer plano, de tal forma que se puede observar fácilmente lo que ocurre. Si se configura el dispositivo Bluetooth remoto para aceptar el procedimiento de emparejamiento y se inicia la conexión con dicho dispositivo, el dispositivo remoto debería decir que el procedimiento de emparejamiento se ha aceptado y debería solicitar el código PIN. Si se introduce el mismo código PIN que se escribió en su momento en el fichero [.filename]#hcsecd.conf# el procedimiento de emparejamiento y de generación de la clave de enlace debería terminar satisfactoriamente. Por otra parte el procedimiento de emparejamiento se puede iniciar en el dispositivo remoto. A continuación se muestra un ejemplo de la salida del dæmon `hcsecd`.

[.programlisting]
....
hcsecd[16484]: Got Link_Key_Request event from 'ubt0hci', remote bdaddr 0:80:37:29:19:a4
hcsecd[16484]: Found matching entry, remote bdaddr 0:80:37:29:19:a4, name 'Pav's T39', link key doesn't exist
hcsecd[16484]: Sending Link_Key_Negative_Reply to 'ubt0hci' for remote bdaddr 0:80:37:29:19:a4
hcsecd[16484]: Got PIN_Code_Request event from 'ubt0hci', remote bdaddr 0:80:37:29:19:a4
hcsecd[16484]: Found matching entry, remote bdaddr 0:80:37:29:19:a4, name 'Pav's T39', PIN code exists
hcsecd[16484]: Sending PIN_Code_Reply to 'ubt0hci' for remote bdaddr 0:80:37:29:19:a4
....

=== Protocolo de descubrimiento de servicios (SDP)

El Protocolo de Descubrimiento de Servicios (Service Discovery Protocol o SDP) permite a las aplicaciones cliente descubrir la existencia de diversos servicios proporcionados por uno o varios servidores de aplicaciones, junto con los atributos y propiedades de los servicios que se ofrecen. Estos atributos de servicio incluyen el tipo o clase de servicio ofrecido y el mecanismo o la información necesaria para utilizar dichos servicios.

SDP se basa en una determinada comunicación entre un servidor SDP y un cliente SDP. El servidor mantiene una lista de registros de servicios, los cuales describen las características de los servicios ofrecidos. Cada registro contiene información sobre un determinado servicio. Un cliente puede recuperar la información de un registro de servicio almacenado en un servidor SDP lanzando una petición SDP. Si el cliente o la aplicación asociada con el cliente decide utilizar un determinado servicio, debe establecer una conexión independiente con el servicio en cuestión. SDP proporciona un mecanismo para el descubrimiento de servicios y sus atributos asociados, pero no proporciona ningún mecanismo ni protocolo para utilizar dichos servicios.

Normalmente, un cliente SDP realiza una búsqueda de servicios acotada por determinadas características. No obstante hay momentos en los que resulta deseable descubrir todos los servicios ofrecidos por un servidor SDP sin que pueda existir ningún conocimiento previo sobre los registros que pueda contener. Este proceso de búsqueda de cualquier servicio ofrecido se denomina _navegación_ o _browsing_.

El servidor Bluetooth SDP denominado man:sdpd[8] y el cliente de línea de órdenes man:sdpcontrol[8] se incluyen en la instalación estándar de FreeBSD. El siguiente ejemplo muestra cómo realizar una consulta de navegación una consulta de navegación SDP.

[source,bash]
....
% sdpcontrol -a 00:01:03:fc:6e:ec browse
Record Handle: 00000000
Service Class ID List:
        Service Discovery Server (0x1000)
Protocol Descriptor List:
        L2CAP (0x0100)
                Protocol specific parameter #1: u/int/uuid16 1
                Protocol specific parameter #2: u/int/uuid16 1

Record Handle: 0x00000001
Service Class ID List:
        Browse Group Descriptor (0x1001)

Record Handle: 0x00000002
Service Class ID List:
        LAN Access Using PPP (0x1102)
Protocol Descriptor List:
        L2CAP (0x0100)
        RFCOMM (0x0003)
                Protocol specific parameter #1: u/int8/bool 1
Bluetooth Profile Descriptor List:
        LAN Access Using PPP (0x1102) ver. 1.0
....

... y así sucesivamente. Resulta importante resaltar una vez más que cada servicio posee una lista de atributos (por ejemplo en el canal RFCOMM). Dependiendo de los servicios que se quieran utilizar puede resultar necesario anotar algunos de los atributos. Algunas implementaciones de Bluetooth no soportan navegación de servicios y pueden devolver una lista vacía. En este caso se puede intentar buscar algún servicio determinado. El ejemplo siguiente muestra cómo buscar el servicio OBEX Object Push (OPUSH):

[source,bash]
....
% sdpcontrol -a 00:01:03:fc:6e:ec search OPUSH
....

En FreeBSD los servicios a clientes Bluetooth se suministran mediante el servidor man:sdpd[8].

[source,bash]
....
# sdpd
....

La aplicación local servidora que quiere proporcionar servicio Bluetooth a los clientes remotos puede registrar su servicio con el dæmon SDP local. Un ejemplo de dicha aplicación Un ejemplo de dicha aplicación lo constituye el dæmon man:rfcomm_pppd[8]. Una vez ejecutado el dæmon registra un servicio LAN de Bluetooth en el dæmon SDP local.

Se puede obtener la lista de servicios registrados con el servidor SDP local lanzando una consulta de navegación SDP utilizando el canal de control local.

[source,bash]
....
# sdpcontrol -l browse
....

=== Acceso telefónico a redes (DUN) y acceso a redes mediante perfiles PPP (LAN)

El perfil de Acceso Telefónico a Redes (Dial-Up Networking o DUN) se utiliza mayoritariamente con modems y teléfonos celulares. Los escenarios cubiertos por este perfil se describen a continuación:

* Utilización de un teléfono celular o un modem por una computadora para simular un modem sin cables que se conecte a un servidor de acceso telefónico a redes o para otros servicios de acceso telefónico relacionados; 
* Utilización de un teléfono celular o un modem por un computador para recibir llamadas de datos. 

El Acceso a Redes con perfiles PPP (LAN) se puede utilizar en las siguientes situaciones:

* Acceso LAN para un único dispositivo Bluetooth; 
* Acceso LAN para múltiples dispositivos Bluetooth; 
* Conexión de PC a PC (utilizando emulación de PPP sobre una línea serie). 

En FreeBSD ambos perfiles se implementan bajo las órdenes man:ppp[8] y man:rfcomm_pppd[8], un encapsulador que convierte la conexión RFCOMM de Bluetooth en algo que puede ser utilizado por PPP. Antes de que se puedan utilizar los perfiles se debe definir una nueva etiqueta PPP en el fichero de configuración [.filename]#/etc/ppp/ppp.conf#. Consulte man:rfcomm_pppd[8] para ver algunos ejemplos.

En el siguiente ejemplo se va a utilizar man:rfcomm_pppd[8] para abrir una conexión RFCOMM con un dispositivo remoto con BD_ADDR 00:80:37:29:19:a4 sobre un canal RFCOMM basado en DUN (Dial-Up Networking). El número de canal RFCOMM se obtiene a partir del dispositivo remoto a través de SDP. Es posible especificar el canal RFCOMM a mano, en cuyo caso man:rfcomm_pppd[8] no realizará ninguna consulta SDP. Se puede utilizar la orden man:sdpcontrol[8] para descubrir el canal RFCOMM utilizado en el dispositivo remoto.

[source,bash]
....
# rfcomm_pppd -a 00:80:37:29:19:a4 -c -C dun -l rfcomm-dialup
....

Para proporcionar el servicio de Acceso a Redes a través de PPP (LAN) se debe ejecutar el servidor man:sdpd[8]. Se debe crear una nueva entrada en [.filename]#/etc/ppp/ppp.conf#. Le rogamos que consulte man:rfcomm_pppd[8] y observe los ejemplos que se facilitan. Por último se debe ejecutar el servidor PPP RFCOMM sobre un número de canal RFCOMM adecuado. El servidor PPP RFCOMM registrará automáticamente el servicio LAN de Bluetooth con el servidor SDP local. El ejemplo que se muestra a continuación describe cómo ejecutar el servidor PPP RFCOMM.

[source,bash]
....
# rfcomm_pppd -s -C 7 -l rfcomm-server
....

=== Perfil OBEX Object Push (OPUSH)

OBEX es un protocolo muy utilizado para transferencias de ficheros sencillas entre dispositivos móviles. Su uso más importante se produce en comuncaciones por infrarrojos, donde se utiliza para transferencia de ficheros genéricos entre portátiles o dispositivos Palm y para enviar tarjetas de visita o entradas de la agenda entre teléfonos celulares y otros dispositivos con aplicaciones PIM.

El cliente y el servidor de OBEX se implementan como un paquete denominado obexapp disponible como " port" en package:comms/obexapp[].

El cliente OBEX se utiliza para introducir y para recuperar recuperar objetos del servidor OBEX. Un objeto puede por ejemplo ser una tarjeta de visita o una cita. El cliente OBEX puede obtener un número de canal RFCOMM del dispositivo remoto utilizando SDP. Esto se hace especificando el nombre del servicio en lugar del número de canal RFCOMM. Los nombres de servicios soportados son: IrMC, FTRN y OPUSH. Es posible especificar el canal RFCOMM como un número. A continuación se muestra un ejemplo de una sesión OBEX donde el objeto que posee la información del dispositivo se recupera del teléfono celular y un nuevo objeto (la tarjeta de visita) se introduce en el directorio de dicho teléfono.

[source,bash]
....
% obexapp -a 00:80:37:29:19:a4 -C IrMC
obex> get
get: remote file> telecom/devinfo.txt
get: local file> devinfo-t39.txt
Success, response: OK, Success (0x20)
obex> put
put: local file> new.vcf
put: remote file> new.vcf
Success, response: OK, Success (0x20)
obex> di
Success, response: OK, Success (0x20)
....

Para proporcionar servicio de OBEX el servidor man:sdpd[8] debe estar en funcionamiento. Además se debe crear un directorio raíz donde todos los objetos van a ser almacenados. La ruta por defecto para el directorio raíz es [.filename]#/var/spool/obex#. Por último se debe ejecutar el servidor OBEX en un número de canal RFCOMM válido. El servidor OBEX registra automáticamente el servicio de Object Push con el dæmon SDP local. El ejemplo que se muestra a local. El ejemplo que se muestra a continuación continuación describe cómo ejecutar el servidor OBEX.

[source,bash]
....
# obexapp -s -C 10
....

=== Perfil de puerto serie (SP)

El perfil de puerto serie (Serial Port o SP) permite que dispositivos Bluetooth realicen emulación de RS232 (o similar). El escenario cubierto por este perfil trata con con aplicaciones comerciales que utilizan Bluetooth como un sustituto sustituto del cable, utilizando una capa de abstracción que representa un puerto serie virtual.

La aplicación man:rfcomm_sppd[1] implementa el perfil Puerto Serie. Usa una pseudo tty como abstracción de puerto serie virtual. El ejemplo de más abajo muestra cómo conectarse a un servicio de dispositivo remoto de Puerto Serie. Observe que no necesita especificarse el canal RFCOMM: man:rfcomm_sppd[1] puede obtenerlo del dispotivo remoto via SDP. Si necesita especificarlo por alguna razón hágalo en la propia línea de órdenes.

[source,bash]
....
# rfcomm_sppd -a 00:07:E0:00:0B:CA -t /dev/ttyp6
rfcomm_sppd[94692]: Starting on /dev/ttyp6...
....

Una vez conectado el pseudo tty se puede utilizar como un puerto serie.

[source,bash]
....
# cu -l ttyp6
....

=== Solución de problemas

==== Un dispositivo remoto no puede conectarse

Algunos dispositivos Bluetooh antiguos no soportan el cambio de cambio de roles. Por defecto, roles. Cuando FreeBSD acepta una nueva conexión por defecto intenta realizar un cambio de rol y convertirse en maestro. Dispositivos que no son capaces de realizar este cambio no pueden conectarse. Es interesante resaltar que el cambio de roles se realiza cuando se está estableciendo una nueva conexión de tal forma que no es posible preguntar al dispositivo si soporta intercambio de roles. Existe una opción HCI para desactivar el intercambio de roles en la parte local.

[source,bash]
....
# hccontrol -n ubt0hci write_node_role_switch 0
....

==== Algo va mal ?puedo ver exactamente qué está ocurriendo?

Sí, se puede. Utilice el paquete hcidump-1.5, que se puede descargar de http://www.geocities.com/m_evmenkin/[aquí]. La herramienta hcidump es similar a la herramienta man:tcpdump[1]. Se puede utilizar para mostrar el contenido de los paquetes Bluetooth sobre el terminal y para volcar los paquetes Bluetooth a un fichero.

[[network-bridging]]
== Puenteado

=== Introducción

Algunas veces resulta útil dividir una red física (como por ejemplo un segmento Ethernet) en dos segmentos de red separados, sin tener que crear subredes IP y sin utilizar una pasarela para comunicar ambos segmentos. El dispositivo que realiza esta función se denomina "bridge". Un sistema FreeBSD con dos interfaces de red puede actuar como un "bridge" o puente entre ambas.

El "bridge" funciona de tal forma que aprende las direcciones de la capa MAC (direcciones Ethernet) de los nodos que se encuentran conectados a cada interfaz de red de tal forma que sólo se reenvía tráfico entre los segmentos de red cuando las direcciones fuente y destino se encuentran separadas en segmentos distintos.

En varios aspectos se puede comparar un "bridge" con un "switch" de pocos puertos.

=== Situaciones donde el puenteado resulta adecuado

Existen al menos dos situaciones típicas donde se puede utilizar la funcionalidad proporcionada por los " bridges".

==== Tráfico de gran volumen en un segmentos de red

La primera situación surge cuando nos encontramos con un segmento de red congestionado pero por las razones que sean no queremos subdividir la red e interconectar las nuevas subredes mediante un "route".

Vamos a considerar un ejemplo de un periódico donde los departamentos editoriales y de producción utilizan la misma subred. Los usuarios de la editorial utilizan el servidor `A` como servidor de ficheros y los de producción utilizan el servidor `B`. Se Se utiliza una red Ethernet para conectar ambos departamentos y se ha detectado que la alta utilización del enlace está ralentizando el funcionamiento de la red.

Si los usuarios de la editorial pudieran agregarse en un segmento de red mientras que los usuarios de producción se localizaran en otro se podrían conectar ambos segmentos mediante un "bridge". Sólo se utilizará el "bridge" para encaminar tráfico de red destinado a interfaces que se encuentren en el _otro_ lado del "bridge", reduciendo de esta forma la congestión en cada nuevo segmento.

==== Cortafuegos de filtrado/conformación de tráfico

La segunda situación típica se produce cuando se necesita un cortafuegos pero no la Traducción de Direcciones de Red (NAT).

A continuación se muestra un ejemplo. Una pequeña compañía se comunica con su ISP utilizando DSL o ISDN. Dicha compañía posee 13 13 direcciones IP globalmente accesibles delegadas por su ISP y tiene 10 ordenadores en funcionamiento. En esta situación un un cortafuegos basado en un "router" resulta difícil debido a la distribución del espacio de direccionamiento disponible (subnetting).

Un cortafuegos implementado sobre un "bridge" se puede utilizar en el camino de bajado desde el ISP hasta las oficinas de la compañía sin necesidad de tener en cuenta ningún aspecto relacionado con la distribución de las direcciones IP.

=== Configuración de un "bridge"

==== Selección de la interfaz de red

Un "bridge" necesita al menos dos tarjetas de red situadas en dos segmentos de red para su funcionamiento. Por desgracia no todas las interfaces de red pueden usarse para el puenteo. Consulte man:bridge[4], ahín encontrará más información sobre qué tarjetas puede usar.

Por favor, instale y pruebe las dos tarjetas de red antes de continuar.

==== Cambios en la configuración del núcleo

Para activar el soporte de "bridging" en el núcleo añada

[.programlisting]
....
options BRIDGE
....

al fichero de configuración del núcleo y recompile el kernel.

==== Soporte de cortafuegos

Si se desea utilizar el "bridge" como un cortafuegos, se debe añadir además la opción `IPFIREWALL`. Lea el capílo de firewalls  para obtener información general sobre cómo configurar el bridge para que actúe además como cortafuegos.

Si además queremos que los paquetes que no sean IP (por ejemplo paquetes ARP) puedan atravesar el "bridge" deberemos añadir la opción `IPFIREWALL_DEFAULT_TO_ACCEPT`. Tenga en cuenta opción modifica el comportamiento del cortafuegos de tal forma que por defecto aceptará cualquier paquete. Hay que tener cuidado para asegurarse de que el comportamiento esperado del cortafuegos, que reside en el conjunto de reglas que se hayan definido, no se vea afectado por este cambio.

==== Soporte de conformado de tráfico

Si se quiere utilizar el "bridge" como un conformador de tráfico, es decir, como un elemento capaz de adaptar los distintos flujos según determinados patrones, se debe añadir la opción `DUMMYNET` a la configuración del núcleo. Se ruega consultar man:dummynet[4] para obtener más información al respecto.

=== Cómo activar el "bridge"

Añadir la línea:

[.programlisting]
....
net.link.ether.bridge=1
....

en [.filename]#/etc/sysctl.conf# para habilitar el soporte de "bridging" en tiempo de ejecución y la línea:

[.programlisting]
....
net.link.ether.bridge_cfg=if1,if2
....

Para activar el "bridging" en las interfaces especificadas (sustituya _if1_ y _if2_ con los nombres de sus interfaces de red). Si deseamos filtrar los paquetes puenteados utilizando man:ipfw[8], debemos añadir también:

[.programlisting]
....
net.link.ether.bridge_ipfw=1
....

En FreeBSD 5.2-RELEASE y posteriores, se debe utilizar las siguientes líneas en lugar de las anteriores:

[.programlisting]
....
net.link.ether.bridge.enable=1
net.link.ether.bridge.config=if1,if2
net.link.ether.bridge.ipfw=1
....

=== Información adicional

Si queremos ser capaces de conectarnos al "bridge" mediante man:telnet[1] se puede asignar una dirección IP a una de las tarjetas de red del "bridge". Por amplio consenso se considera una mala idea asignar más de una dirección IP al "bridge".

Si poseemos varios "bridges" en nuestra red sólamente puede existir un único camino entre cualesquiera dos máquinas de nuestra red. Técnicamente hablando esto significa que no existe soporte para gestión de enlace mediante mecanismos basados en árboles de recubrimiento mínimos ("spanning tree").

Un "bridge" puede añadir latencia a los tiempos de respuesta de la orden man:ping[8], especialmente cuando el tráfico tiene que viajar de un segmento de red al otro.

[[network-nfs]]
== NFS

FreeBSD soporta diversos sistemas de ficheros, uno de los cuales es el Sistema de Ficheros en Red, tambíen conocido por su acrónimo en inglés NFS. NFS permite compartir directorios y ficheros a través de la red. Los usuarios del sistema  NFS pueden acceder a ficheros que se encuentran físicamente en máquinas remotas de una forma transparente, como si se tratara de ficheros locales.

He aquí algunos los beneficios más destacados que NFS proporciona:

* Las estaciones de trabajo locales utilizan menos espacio de disco debido a que los datos se encuentran centralizados en un único lugar pero pueden ser accedidos y modificados por varios usuarios, de tal forma que no es necesario replicar la información.
* Los usuarios no necesitan disponer de un directorio " home" en cada una de las máquinas de la organización. Los directorios "home" pueden crearse en el servidor de NFS para posteriormente poder acceder a ellos desde cualquier máquina a través de la infraestrutura de red.
* También se pueden compartir a través de la red dispositivos de almacenamiento como disqueteras, CDROM y unidades ZIP. Esto puede reducir la inversión en dichos dispositivos y mejorar el aprovechamiento del hardware existente en la organización.

=== Cómo funciona NFS

El sistema NFS está dividido al menos en dos partes principales: un servidor y uno o más clientes. Los clientes acceden de forma remota a los datos que se encuentran almacenados en el servidor. Para que el sistema funcione correctamente se deben configurar y ejecutar unos cuantos procesos.

[NOTE]
====
En FreeBSD 5.X se ha reemplazado  portmap por rpcbind. de esta forma para los ejemplos que vamos a comentar a continuación se recuerda que en FreeBSD 5.X se debe reemplazar cualquier instancia de portmap por rpcbind.
====

El servidor de NFS debe ejecutar los siguientes dæmones:

[.informaltable]
[cols="1,1", frame="none", options="header"]
|===
| Dæmon
| Descripción

|nfsd
|El dæmonNFS, que atiende peticiones de clientes NFS.

|mountd
|El dæmon de montaje de NFS, que transporta las peticiones que man:nfsd[8] realiza.

|portmap
| El dæmon portmapper permite que los clientes NFS puedan descubrir qué puerto está utilizando el servidor de NFS.
|===

El cliente también puede ejecutar un dæmon conocido , como nfsiod. El dæmon nfsiod atiende las peticiones provinientes del servidor NFS. Este dæmon es opcional y sirve para mejorar el rendimiento pero no es necesario para el funcionamiento correcto del sistema. Se recomienda consultar man:nfsiod[8] para obtener más información.

[[network-configuring-nfs]]
=== Configuración de NFS

La configuración de NFS es un proceso relativamente sencillo. Para que los procesos anteriormente descritos se ejecuten en tiempo de arranque del sistema, basta con realizar paqueñas modificaciones en [.filename]#/etc/rc.conf#. 

En [.filename]#/etc/rc.conf# del servidor de NFS se deben configurar las siguientes opciones:

[.programlisting]
....
portmap_enable="YES"
nfs_server_enable="YES"
mountd_flags="-r"
....

mountd se ejecuta automáticamente cuando se activa el servidor NFS.

En el cliente debemos asegurarnos de que se encuentra activada la activada la siguiente opción dentro de [.filename]#/etc/rc.conf#:

[.programlisting]
....
nfs_client_enable="YES"
....

El archivo [.filename]#/etc/exports# especifica los directorios o sistemas de ficheros que NFS exporta al exterior. Cada línea dentro de [.filename]#/etc/exports/# especifia un sistema de ficheros y qué máquinas tienen derechos de acceso sobre dicho sistema. Además de los derechos de acceso se pueden definir otras opciones de acceso, tales como solo lectura o lectura y escritura. Existen multitud de opciones que pueden definirse sobre un directorio exportable pero en este manual sólo se van a comentar unas pocas. Consulte man:exports[5] para obtener una descripción más detallada.

Aquí se muestran algunos ejemplos de entradas para [.filename]#/etc/exports#:

El siguiente ejemplo proporciona una idea de cómo exportar sistemas de ficheros, aunque los parámetros pueden diferir dependiendo de su entorno y su configuración de red. En dicho ejemplo, se exporta el directorio [.filename]#/cdromm# a tres máquinas que se encuentran en el mismo dominio que el servidor (de ahí que no se especifique ningún nombre de dominio para cada máquina) o que pueden estar dadas de alta en [.filename]#/etc/hosts#. En cualquier caso la opción `-ro` configura el sistema de ficheros de red como "sólo lectura" ("read-only"). Con esta opción los sistemas remotos no serán capaces de realizar cambios sobre el sistema de ficheros exportados.

[.programlisting]
....
/cdrom -ro host1 host2 host3
....

La siguiente línea exporta el directorio [.filename]#/home# a tres máquinas utilizando direcciones IP. Esto resulta útil cuando disponemos de una red privada pero no disponemos de ningún servidor de DNS configurado. También se podría configurar [.filename]#/etc/hosts# para que resolviera nombres de máquinas internos; consulte man:hosts[5] para obtener más información al respecto. La opción `-alldirs` permite que los subdirectorios del directorio [.filename]#/home# tambíen se puedan utilizar como puntos de montaje. En otras palabras, esto permite que los clientes puedan trabajar sobre los subdirectorios en los que estén realmente interesados.

[.programlisting]
....
/home  -alldirs  10.0.0.2 10.0.0.3 10.0.0.4
....

La siguiente línea exporta el directorio [.filename]#/a# de tal forma que puedan acceder a dicho directorio dos máquinas situadas en distintos dominios. La opción `-maproot=root` permite que el usuario `root` de la máquina cliente modifique los datos del sistema de ficheros en red como si fuera el usuario `root` del servidor. Si no se especifica la opción `-maproot=root` el usuario `root` del cliente puede no poseer los permisos necesarios para realizar modificaciones en el sistema de ficheros.

[.programlisting]
....
/a  -maproot=root  host.example.com box.example.org
....

Para que un cliente pueda acceder al sistema de ficheros exportado debe poseer permisos para ello. Debemos asegurarnos de que el cliente se encuentra listado en [.filename]#/etc/exports#.

Dentro de [.filename]#/etc/exports# cada línea representa información de exportación de un sistema de ficheros para un determinado conjunto de máquinas. Una máquina sólo puede aparecer una vez dentro de un sistema de ficheros exportable y el archivo sólo puede tener una única entrada por defecto. Por ejemplo, si suponemos que [.filename]#/usr# es un único sistema de ficheros la siguiente configuración de [.filename]#/etc/exports# sería incorrecta:

[.programlisting]
....
/usr/src   client
/usr/ports client
....

Existe un sistema de ficheros, concretamente [.filename]#/usr#, que posee dos líneas con reglas de exportación para la misma máquina, `client`. El formato correcto para esta situación sería el siguiente:

[.programlisting]
....
/usr/src /usr/ports  client
....

Las propiedades de un sistemas de ficheros que se exporta al exterior deben aparecer agrupadas bajo la misma línea. Líneas que no poseen ningún cliente se tratan como si tuvieran una única máquina. Esto limita la forma en que pueden configurarse la exportaciones de sistemas de ficheros pero para la mayoría de la gente no suele ser un problema.

El ejemplo que se muestra a continuación es una muestra de una lista de exportación correcta, donde [.filename]#/usr# y [.filename]#/exports# son sistemas de ficheros locales:

[.programlisting]
....
# Exportar src y ports a cliente01 y cliente02, pero
# solo el cliente01 tiene acceso root
/usr/src /usr/ports -maproot=root    cliente01
/usr/src /usr/ports               cliente02
# Las maquinas cliente tienen acceso root y pueden montar todo lo que aparezca
# en /exports.  Cualquier sistema puede montar /exports/obj en modo
# solo lectura
/exports -alldirs -maproot=root      cliente01 cliente02
/exports/obj -ro
....

Se debe reiniciar el dæmon mountd siempre que se modifique el contenido del archivo [.filename]#/etc/exports# para que los cambios surtan efecto. Esto se realiza enviando la señal HUP al proceso `mountd`:

[source,bash]
....
# kill -HUP `cat /var/run/mountd.pid`
....

También se puede reiniciar FreeBSD para que se cargue la nueva configuración pero este mecanismo no resulta necesario si se ejecutan las órdenes como `root`, que ponen el servidor de NFS de nuevo en funcionamiento.

En el servidor de NFS:

[source,bash]
....
# portmap
# nfsd -u -t -n 4
# mountd -r
....

En el cliente de NFS:

[source,bash]
....
# nfsiod -n 4
....

En este punto todo debería estar preparado para poder anclar el sistema de ficheros remoto en la máquina cliente. En los siguientes ejemplos el nombre del servidor es `server` y el punto de montaje temporal utilizado por el cliente es `client`. Si se desea montar el sistema de ficheros de forma temporal o simplemente comprobar que la configuración funciona sin problemas se puede ejecutar una orden como la que se muestra a continuación con permisos de `root` en la máquina cliente:

[source,bash]
....
# mount server:/home /mnt
....

Esta orden ancla el directorio [.filename]#/home# del servidor en el directorio [.filename]#/mnt# del cliente. Si todo funciona correctamente debería poder entrar en el directorio [.filename]#/mnt# del cliente y ver todos los ficheros que se encuentran en el directorio [.filename]#/home# del servidor. 

Si queremos anclar automáticamente un sistema de ficheros remoto cuando la máquina está arrancando se puede añadir una línea como la siguiente dentro de [.filename]#/etc/fstab#:

[.programlisting]
....
servidor:/home	/mnt	nfs	rw	0	0
....

man:fstab[5] comenta todas las opciones disponibles.

=== Usos prácticos

El protocolo NFS tiene múltiples usos prácticos. Los más típicos se enumeran a continuación:

* Compartición de la unidad de CDROM entre varias máquinas. Esto resulta ser más barato y una forma más conveniente para instalar software en varias máquinas.
* En grandes redes puede ser más adecuado configurar un servidor central de NFS en el cual se almacenen todos los "homes" de los distintos usuarios. Estos directorios se pueden exportar a través de la red de tal forma que los usuarios pueden trabajar con el mismo directorio independientemente de la máquina que utilicen. 
* Varias máquinas pueden poseer el directorio [.filename]#/usr/ports/distfiles# compartido. De este modo cuando necesitemos instalar un port en varias máquinas, se puede acceder rápidamente a las fuentes sin necesidad de bajarlas una vez para cada máquina.

[[network-amd]]
=== Anclajes automáticos usando amd

El dæmon man:amd[8] ("the automatic mounter daemon", o dæmon de montaje automático) automáticamente ancla un sistema de ficheros remoto cuando se tiene que acceder a un fichero perteneciente a dicho sistema. Los sistemas de ficheros que permanecen inactivos durante un determinado periodo de tiempo son automáticamente desmontados por el mismo dæmon. Este dæmon proporciona una alternativa sencilla a la utilización de los montajes permanentes que normalmente se especifican a través del fichero [.filename]#/etc/fstab#.

amd trabaja actuando como un servidor servidor de NFS para los directorios [.filename]#/host# y [.filename]#/net#. Cuando se accede a algún fichero ubicado bajo estos directorios amd busca el punto de montaje remoto y automáticamente lo monta. El directorio [.filename]#/net# se utiliza para anclar sistemas de ficheros remotos especificados mediante direcciones IP, mientras que el directorio [.filename]#/host# almacena aquellos sistemas de ficheros remotos que han sido especificados mediante un nombre de máquina.

amd detecta cualquier intento de acceder a un fichero dentro del directorio [.filename]#/host/foobar/usr# y se encarga de montar el sistema de ficheros remoto ([.filename]#/usr#) en la máquina, en caso de que no estuviera ya anclado.

.Anclaje de una exportación utilizando amd
[example]
====
`showmount` muestra los puntos de montaje que posee una máquina remota. Por ejemplo para conocer los montajes de un máquina llamada `foobar`, se puede utilizar:

[source,bash]
....
% showmount -e foobar
Exports list on foobar:
/usr                               10.10.10.0
/a                                 10.10.10.0
% cd /host/foobar/usr
....

====

Como se observa en el ejemplo, `showmount` muestra el directorio [.filename]#/usr# como una exportación. Cuando se cambia el directorio actual al directorio [.filename]#/host/foobar/usr# el dæmon amd intenta resolver el nombre `foobar` y automáticamente ancla el sistema de ficheros remoto.

El dæmon amd se puede ejecutar a partir de los scripts de inicio, utilizando la siguiente línea del archivo de configuración [.filename]#/etc/rc.conf#:

[.programlisting]
....
amd_enable="YES"
....

Además, amd soporta opciones adicionales que pueden definirse mediante la variable `amd_flags`. Por defecto, la variable `amd_flags` posee las siguientes opciones:

[.programlisting]
....
amd_flags="-a /.amd_mnt -l syslog /host /etc/amd.map /net /etc/amd.map"
....

El archivo [.filename]#/etc/amd.map# define las opciones por defecto con las cuales se anclan los sistemas de ficheros remotos. El archivo [.filename]#/etc/amd.conf# define algunas características avanzadas para el dæmon amd.

Se ruega consultar las páginas del manual de man:amd[8] y de man:amd.conf[8] para obtener más información.

[[network-nfs-integration]]
=== Problemas de integración con otras plataformas

Determinados adaptadores Ethernet para sistemas basados en el bus ISA poseen restricciones que pueden producir serios problemas de red, en particular con el protocolo NFS. Estos problemas no son específicos de FreeBSD, pero los sistemas FreeBSD se ven afectados por ellos.

El problema surge casi siempre cuando el sistema (FreeBSD) está empotrado dentro de una red compuesta por estaciones de trabajo de alto rendimiento, como por ejemplo estaciones de Silicon Graphics y de Sun Microsystems. El montaje del sistema de ficheros remoto suele funcionar perfectamente y algunas operaciones sobre el el sistema de ficheros pueden tener éxito pero de repente el el servidor que no responde a las peticiones del cliente, aunque peticiones y respuestas de otros clientes funcionan con normalidad y se continúan procesando. Esto sucede en los sistemas clientes, tanto en sistemas FreeBSD como en otras estaciones de trabajo. En muchos sistemas, lo único que se puede hacer es resetear la máquina de forma abrupta, ya que el bloqueo producido por el protocolo NFS no se puede solucionar.

Aunque la solución "correcta" consiste en obtener un adaptador Ethernet con mayor rendimiento y capacidad, todavía se puede aplicar un parche sencillo que puede llegar a permitir un funcionamiento sin problemas. Si el sistema FreeBSD actúa como servidor de _NFS_ se puede incluír la opción `w=1024` cuando el ejecute una petición de montaje sobre dicho servidor. Si FreeBSD dicho servidor. Si FreeBSD actúa como cliente de _NFS_, se puede ejecutar man:mount[8] con el parámetro `-r=1024`. Estas opciones se pueden especificar en el [.filename]#/etc/fstab# del cliente para que entren en funcionamiento cuando se realicen montajes automáticos y también se puede utilizar el parámetro `-o` de man:mount[8] cuando se realicen montajes manuales.

Resulta apropiado resaltar que existe un problema totalmente distinto que algunas veces se confunde con el que acabamos de describir, que aparece cuando el servidor y los clientes se encuentran en redes diferentes. Si nos encontramos en esta situación debemos _asegurarnos_ de que nuestros " routers" están encaminando correctamente los paquetes UDP que genera el protocolo NFS pues en caso contrario el sistema no funcionará, independientemente de los ajustes que se realicen en el cliente o en el servidor.

En los siguientes ejemplos `fastws` es el nombre de una estación de trabajo de altas prestaciones y `freebox` es el nombre de un sistema FreeBSD con un adaptador Ethernet de bajas prestaciones. Se pretende además exportar el directorio [.filename]#/sfcompartido# (ver man:exports[5]) y el directorio [.filename]#/projecto#. Tenga en cuenta que en cualquier caso puede resultar útil definir opciones adicionales a las que que se muestran en el siguiente ejemplo, como pueden ser `hard`, `soft` o `bg`. Esto dependerá de la aplicación que utilice el sistema de ficheros remoto.

Ejemplos de configuración para el sistema FreeBSD (`freebox`) que actúa como cliente. Configuración del archivo [.filename]#/etc/fstab# de `freebox`:

[.programlisting]
....
fastws:/sfcompartido /projecto nfs rw,-r=1024 0 0
....

Orden de ejecución manual para `freebox`:

[source,bash]
....
# mount -t nfs -o -r=1024 fastws:/sfcompartido /projecto
....

Ejemplos de configuración para el sistema FreeBSD que actúa como servidor. Configuración de [.filename]#/etc/fstab# de `fastws`:

[.programlisting]
....
freebox:/sfcompartido /projecto nfs rw,-w=1024 0 0
....

Orden de ejecución manual para `fastws`:

[source,bash]
....
# mount -t nfs -o -w=1024 freebox:/sfcompartido /projecto
....

Casi cualquier adaptador Ethernet de 16 bits permite operar sin operar sin las restricciones anteriores sobre el tamaño de lectura o escritura especificado por defecto.

Por si alguien estuviera interesado a continuación se muestra el error que aparece en estos casos, lo cual explica por qué decimos que el error resulta irrecuperable. NFS trabaja típicamente con un tamaño de "bloque" de 8 K (aunque se pueden producir fragmentos de menor tamaño). Debido a que el máximo tamaño de los paquetes Ethernet se encuentra alrededor de los 1500 bytes el "bloque" de NFS se trocea en varios paquetes Ethernet aunque desde el punto de vista del protocolo NFS se trata como si fuese un único paquete. Los trozos deben reensamblarse en el destino y se debe enviar una _confirmación_ para el bloque recibido. Las estaciones de trabajo de altas prestaciones pueden soltar paquetes NFS de forma contínua uno después de otro, lo más juntos posible. Por otro lado en las tarjetas de red más pequeñas y de menor capacidad puede ocurrir que un paquete recien llegado a la tarjeta sobreescriba información perteneciente a un paquete anterior antes de que llegue a ser transmitido completamente, de tal forma que al recibirse el bloque NFS no puede ser ni reconstruido ni ni reconocido. Como resultado de este proceso la máquina tratará de enviar el mismo paquete transcurridos unos instantes de espera, pero se tratarán de enviar de nuevo los 8 K que constituyen un bloque NFS, y de esta forma se repetirá el el proceso, así hasta el infinito.

Si se mantiene el tamaño del bloque por debajo del tamaño de paquete máximo de Ethernet, podemos asegurar que cualquier paquete Ethernet transporta un bloque NFS, el cual puede asentirse individualmente, evitando así la explosión de paquetes y el eventual bloqueo del sistema.

Desbordamientos circulares del "buffer" (" overruns") pueden producirse si nos encontramos con una estación de trabajo de altas prestaciones que envía contínuamente mucho tráfico a un sistema convencional, pero con tarjetas Ethernet de buena calidad, estos desbordamientos resultan altamente improbables para el caso de los tamaños de bloque por defecto generados por el sistema NFS. Cuando se produce un desbordamiento, las unidades afectadas se retransmiten, y existe una gran probabilidad de que se reciban, se reensamblen y se confirmen.

[[network-diskless]]
== Ejecución sin disco duro

Una máquina FreeBSD se puede arrancar a través de la red y operar sin que necesite poseer ningún disco, utilizando sistemas de ficheros de un servidor de NFS. No se necesita realizar ninguna modificación al sistema, salvo configurar determinados ficheros. Este tipo de sistemas se pueden configurar fácilmente puesto que FreeBSD dispone de todos los elementos necesarios:

* Existen al menos dos formas de cargar el núcleo del sistema operativo a través de la red:

** PXE: El sistema de Intel(R) conocido como Preboot Execution Environment. Se trata de una especie de arranque inteligente a partir de una memoria de sólo lectura (ROM) que se encuentra en algunas placas bases y tarjetas de red. Se puede obtener más información en man:pxeboot[8].
** El port etherboot (package:net/etherboot[]) genera código de sólo lectura (código ROM) que se puede utilizar para arrancar máquinas a través de la red. Dicho código se puede instalar en una memoria de arranque tipo PROM en algunas tarjetas de red o se puede cargar en una disquetera (o disco duro), y también en un sistema de ficheros MS-DOS(R) que esté en ejecución. Varias tarjetas de red soportan este mecanismo.

* Existe un script de ejemplo ([.filename]#/usr/shared/examples/diskless/clone_root#) que facilita la creación y el mantenimiento del sistema de ficheros raíz de la estación de trabajo en el servidor. La configuración de este "script" se debe retocar ligeramente pero sirve como punto de partida para comenzar rápidamente.
* Existen ficheros estándar de arranque bajo [.filename]#/etc# que dan soporte al arranque de máquinas sin disco.
* El "swapping", en caso de ser necesario, se puede realizar usando NFS y tambíen usando un disco duro local.

Existen varias formas de ejecutar una estación de trabajo sin discos. En el proceso se involucran distintos elementos y la mayoría se pueden adaptar a las necesidades del usuario. A continuación se describen variaciones sobre la configuración de un sistema sin discos, haciendo incapié en la simplicidad y compatibilidad con los "scripts" de arranque de FreeBSD. El sistema que vamos a describir tiene las siguientes características.

* Las estaciones de trabajo sin disco utilizan un sistema de ficheros [.filename]#raíz# de sólo lectura y un sistema de ficheros compartido, también de sólo lectura, bajo [.filename]#/usr#.
+ 
El sistema de ficheros [.filename]#raíz# es una copia del sistema raíz estandar de FreeBSD (normalmente del sistema raíz del servidor), donde se sobreescriben algunos archivos de configuración necesarios para la ejecución sin discos y para la configuración local específica de la máquina objetivo.
+ 
Las partes del sistema de ficheros [.filename]#raíz# que tiene que tener permisos de lectura y escritura se superponen con los sistemas de ficheros man:mfs[8] (FreeBSD 4.X) o man:md[4]. Cualquier cambio que se produzca en dichas partes se perderá cuando se reinicie el sistema.
* El núcleo se transmite y se carga utilizando etherboot o bien PXE, dependiendo del hardware y los mecanismos que se soporten.

[CAUTION]
====

Como se ha comentado con anterioridad estos sistemas son inseguros. Se debe confinar dentro de una red protegida y el resto de las máquinas por defecto no deben confiar en estos métodos.
====

Toda la información que se presenta en esta sección se ha probado utilizando FreeBSD 4.9-RELEASE y 5.2.1-RELEASE. El texto se encuentra estructurado principalmente para utilización en sistemas 4.X. Se insertan notas para indicar cambios producidos en las versiones 5.X.

=== Conocimientos previos

Configurar estaciones de trabajo sin discos es una operación relativamente sencilla pero en la que pueden cometerse errores. Estos errores resultan algunas veces difíciles de diagnosticar debido a razones que vamos a exponer a continuación. Por ejemplo:

* Diferentes opciones de tiempo de compilación pueden determinar comportamientos distintos en tiempo de ejecución.
* Los mensajes de error a menudo resultan crípticos o incluso no existen.

Se se quieren resolver los posibles problemas que puedan surgir resulta muy útil conocer el funcionamiento conceptual del mecanismo.

Para que el arranque se produzca exitosamente se deben realizar varias operaciones:

* La máquina necesita obtener algunos parámetros iniciales, tales como su dirección IP, el fichero ejecutable, el nombre del servidor y la ruta raíz. Esto se realiza utilizando los protocolos DHCP o BOOTP. DHCP es una extensión compatible del protocolo BOOTP y utiliza los mismos números de puertos y los mismos formatos de paquete básicos.
+ 
Es posible configurar un sistema de tal forma que utilice sólamente BOOTP. En el sistema base de FreeBSD se incluye el programa servidor man:bootpd[8].
+ 
No obstante DHCP posee varias ventajas sobre BOOTP (archivos de configuración más limpios, posibilidad de ejecutar PXE, junto con otras características que no se relacionan directamente con el tema que estamos tratando tratando) por lo que principalmente se va a describir la configuración de DHCP, proporcionando ejemplos equivalentes en man:bootpd[8] siempre que sea posible. La configuración de ejemplo se basa en el paquete software de ISC DHCP (en el servidor de prueba se instaló la versión 3.0.1.r12).
* La máquina sin disco necesita transferir uno o varios programas a la memoria local. Para ello se usa TFTP o bien NFS. La elección entre ambos se produce mediante la configuración de la compilación que se produce en varios lugares. Una fuente de error típica aparece cuando se especifican ficheros con el protocolo incorrecto: TFTP normalmente transfiere todos los ficheros desde un único directorio del servidor, de modo que espera nombres de ficheros relativos a dicho directorio. Por otro lado NFS necesita recibir rutas de fichero absolutas.
* El kernel y los programas de arranque intermedios deben ser inicializados y ejecutados. Existen diferencias importantes en este área:

** PXE carga man:pxeboot[8], una versión modificada de la tercera fase del cargador de arranque de FreeBSD. man:loader[8] obtiene la mayoría de los parámetros necesarios para arrancar el sistema y los deposita en variables de entorno del kernel antes de tranferir el control. En este caso es posible utilizar un un núcleo [.filename]#GENERIC# .
** etherboot carga directamente el directamente el núcleo con menos trabajo previo que el método anterior. Para ello se debe compilar un núcleo con ciertas opciones.
+
PXE y etherboot funcionan muy bien en los sistemas 4.X. Dado que los núcleos de los sistemas 5.X permiten que el man:loader[8] realice más tareas, se prefiere usar  PXE.
+ 
Si su BIOS y su tarjeta de red soportan PXE lo normal es utilizarlo. No obstante se puede arrancar un sistema 5.X utilizando  etherboot.
* Para acabar la tarea la máquina necesita acceder al sistema de ficheros. En todos los casos se utiliza NFS.

No olvide consultar man:diskless[8].

=== Instrucciones de configuración

==== Configuración utilizando ISC DHCP

El servidor ISC DHCP puede responder tanto a peticiones de BOOTP como a peticiones de DHCP.

ISC DHCP no forma parte de la versión 4.9 de FreeBSD por lo que se debe instalar el port package:net/isc-dhcp3-server[] o el paquete correspondiente. Por favor, consulte crossref:ports[ports,Instalación de aplicaciones: «packages» y ports] para obtener más información sobre los ports y los paquetes.

Una vez que ISC DHCP se encuentra instalado necesita un fichero de configuración para poder ejecutarse [.filename]#/usr/local/etc/dhcpd.conf#). A continuación se muestra un ejemplo comentado, donde la máquina `margaux` utiliza etherboot y la máquina `corbieres` utiliza PXE:

[.programlisting]
....

default-lease-time 600;
max-lease-time 7200;
authoritative;

option domain-name "example.com";
option domain-name-servers 192.168.4.1;
option routers 192.168.4.1;

subnet 192.168.4.0 netmask 255.255.255.0 {
  use-host-decl-names on; <.>
  option subnet-mask 255.255.255.0;
  option broadcast-address 192.168.4.255;

  host margaux {
    hardware ethernet 01:23:45:67:89:ab;
    fixed-address margaux.example.com;
    next-server 192.168.4.4; <.>
    filename "/data/misc/kernel.diskless"; <.>
    option root-path "192.168.4.4:/data/misc/diskless"; <.>
  }
  host corbieres {
    hardware ethernet 00:02:b3:27:62:df;
    fixed-address corbieres.example.com;
    next-server 192.168.4.4;
    filename "pxeboot";
    option root-path "192.168.4.4:/data/misc/diskless";
  }
}
....

<.> Esta opción indica a dhcpd que envíe el valor que se encuentra en las declaraciones de `host` como el nombre de máquina para la máquina sin disco. Otra forma de hacer esto sería añadiendo una opción `option host-name margaux` dentro de las declaraciones de máquina.

<.> La directiva `next-server` selecciona el servidor de TFTP o de NFS que se debe utilizar para cargar el núcleo o el fichero cargador del núcleo (por defecto se utiliza la misma máquina que actúa como servidor de DHCP).

<.> La directiva `filename` define el archivo que etherboot o PXE cargará en el siguiente paso de ejecución. Debe especificarse de acuerdo con el método de transferencia seleccionado. Etherboot se puede compilar para que use NFS o TFTP. El sistema FreeBSD se configura por defecto para  NFS. PXE utiliza TFTP por lo que se utiliza una ruta relativa para especificar el nombre del fichero (esto puede depender de la configuración del servidor de TFTP pero suele ser lo normal). Además PXE no carga el núcleo, lo hace [.filename]#pxeboot#. Existen otras posibilidades interesantes, como cargar [.filename]#pxeboot# desde el directorio [.filename]#/boot# de una unidad de CD-ROM de FreeBSD (ya que man:pxeboot[8] puede cargar un núcleo [.filename]#GENERIC# surge la posibilidad de utilizar PXE para arrancar desde una unidad de CD-ROM remota).
<.> La opción `root-path` define la ruta para el sistema de ficheros raíz utilizando la notación típica de NFS. Cuando se utiliza PXE, es posible dejar la dirección IP siempre y cuando no se active la opción del núcleo de BOOTP. El servidor NFS será en este caso el mismo que el servidor de TFTP.

==== Configuración utilizando BOOTP

A continuación se muestra la configuración equivalente utilizando bootpd (reducida a un único cliente). Esta configuración se debe situar en [.filename]#/etc/bootptab#.

Por favor, recuerde que etherboot se debe compilar con la opción específica de `NO_DHCP_SUPPORT` para que pueda utilizar BOOTP y que PXE _requiere_ DHCP. La única ventaja obvia de bootpd es que se encuentra disponible en el sistema base.

[.programlisting]
....

.def100:\
  :hn:ht=1:sa=192.168.4.4:vm=rfc1048:\
  :sm=255.255.255.0:\
  :ds=192.168.4.1:\
  :gw=192.168.4.1:\
  :hd="/tftpboot":\
  :bf="/kernel.diskless":\
  :rp="192.168.4.4:/data/misc/diskless":

margaux:ha=0123456789ab:tc=.def100
....

==== Preparación de un programa de arranque con Etherboot

http://etherboot.sourceforge.net[La página web de Etherboot ] contiene http://etherboot.sourceforge.net/doc/html/userman/t1.html[ una amplia documentación] enfocada principalmente a los sistemas Linux pero en cualquier caso contiene información que puede resultar útil. En los siguientes párrafos se describe brevemente como se puede utilizar etherboot en un sistema FreeBSD.

Lo primero es instalar el port o paquete package:net/etherboot[]. El port de etherboot está en [.filename]#/usr/ports/net/etherboot#. Si el árbol de ports está instalado en el sistema basta con ejecutar `make` en dicho directorio. Por favor, lea crossref:ports[ports,Instalación de aplicaciones: «packages» y ports] para saber más sobre los ports y los paquetes.

Se puede modificar la configuración de etherboot (por ejemplo, para que use TFTP en lugar de NFS) editando el fichero [.filename]#Config# que se encuentra en el directorio fuente de etherboot.

Para nuestros propósitos se utilizará un disquete de arranque. Para utilizar otros métodos (PROM o un programa MS-DOS(R)) por favor consulte la documentación de etherboot.

Para crear un disco de arranque se debe insertar un disco en la unidad de disquetes de la máquina donde se ha instalado etherboot, cambiar al directorio [.filename]#src# dentro del árbol de directorios de etherboot y teclear:

[source,bash]
....
# gmake bin32/tipo_de_dispositivo.fd0

....

_tipo_de_dispositivo_ depende del tipo de tarjeta Ethernet que se encuentre instalada en la estación de trabajo sin disco. Consulte el fichero [.filename]#NIC# en el mismo directorio para determinar cúal es el _tipo_de_dispositivo_ que debe usted usar.

==== Arranque con PXE

Por defecto el cargador man:pxeboot[8] carga, valga la redundancia, el kernel vía NFS. El El cargador se puede compilar para que utilice  TFTP en lugar de NFS especificando la opción `LOADER_TFTP_SUPPORT` dentro de [.filename]#/etc/make.conf#. Observe los comentarios de [.filename]#/etc/defaults/make.conf# (o de [.filename]#/usr/shared/examples/etc/make.conf# para sistemas 5.X) para saber más detalles.

Existen otras dos opciones de [.filename]#make.conf# no documentadas que pueden ser útiles para arrancar una máquina sin disco a través del puerto serie: `BOOT_PXELDR_PROBE_KEYBOARD` y `BOOT_PXELDR_ALWAYS_SERIAL` (esta última sólo existe en FreeBSD 5.X).

Para utilizar PXE cuando arranca la máquina normalmente el usuario tiene que seleccionar la opción `Boot from network` dentro del menú de opciones de la BIOS o pulsar un tecla de función cuando la máquina se está inicializando.

==== Configuración de servidores de TFTP y de NFS

Si PXE o etherboot se encuentran configurados para utilizar TFTP se necesita activar tftpd en el servidor de ficheros: 

[.procedure]
====

. Crear un directorio desde el cual el dæmon tftpd servirá los ficheros, por ejemplo [.filename]#/tftpboot#.
. Añadir la siguiente línea a [.filename]#/etc/inetd.conf#:
+
[.programlisting]
....
tftp	dgram	udp	wait	root	/usr/libexec/tftpd	tftpd -l -s /tftpboot
....
+
[NOTE]
======
Parece que al menos algunas versiones de PXE utilizan la versión TCP de TFTP. En este caso se puede añadir una segunda línea, donde se reemplace `dgram udp` por `stream tcp`.
======

. Indicar a inetd que vuelva a leer su fichero de configuración:
+
[source,bash]
....
# kill -HUP `cat
                /var/run/inetd.pid`
....
====

Se puede situar el directorio [.filename]#tftpboot# en cualquier parte del servidor. Debe asegurarse de que la localización se encuentra correctamente configurada tanto en [.filename]#inetd.conf# como en [.filename]#dhcpd.conf#.

En todos los casos también resulta necesario activar el sistema de NFS y exportar los sistemas de ficheros adecuados, todo ello en el servidor de NFS.

[.procedure]
====

. Añadir lo siguiente a [.filename]#/etc/rc.conf#:
+
[.programlisting]
....
nfs_server_enable="YES"
....

. Exportar el sistema de ficheros donde el directorio raíz sin disco se encuentra localizado añadiendo lo siguiente a [.filename]#/etc/exports# (ajuste el punto de montaje de la unidad y sustituya _margaux corbieres_ por el nombre de las estaciones de trabajo sin disco, según corresponda):
+
[.programlisting]
....
/data/misc -alldirs -ro margaux corbieres
....

. Indicar a mountd que vuelva a leer su archivo de configuración. Si en un primer paso se ha configurado la activación automática del sistema de NFS en [.filename]#/etc/rc.conf# lo mejor es reiniciar para que los cambios surtan efecto.
+
[source,bash]
....
# kill -HUP `cat
                /var/run/mountd.pid`
....
====

==== Construcción de un kernel sin disco

Si se utiliza etherboot, se necesita crear un archivo de configuración para el kernel de la máquina sin disco que posea las siguientes opciones (además de las opciones del núcleo habituales):

[.programlisting]
....

options     BOOTP          # Use BOOTP to obtain IP address/hostname
options     BOOTP_NFSROOT  # NFS mount root filesystem using BOOTP info
....

Puede resultar interesante utilizar además `BOOTP_NFSV3`, `BOOT_COMPAT` y `BOOTP_WIRED_TO` (consultar [.filename]#LINT# en 4.X o [.filename]#NOTES# en sistemas 5.X).

Los nombres de estas opciones son nombres históricos y ligeramente confusos ya que permiten un uso indistinto tanto de DHCP como de BOOTP dentro del núcleo (también resulta posible forzar la utilización única de o bien BOOTP o bien de DHCP).

Contruir el núcleo (vea crossref:kernelconfig[kernelconfig,Configuración del kernel de FreeBSD]) y copiarlo al lugar especificado en el archivo [.filename]#dhcpd.conf#.

[NOTE]
====
Cuando se utiliza PXE, la construcción del núcleo con las opciones anteriores no resulta ser algo estrictamente necesario (aunque se recomienda). Activar dichas opciones provoca un mayor tráfico de peticiones de DHCP durante el arranque del núcleo, lo que puede dar lugar a pequeñas inconsistencias entre los nuevos valores y los los valores recuperados por man:pxeboot[8] en casos muy específicos. La ventaja de utilizarlas consiste en que como un efecto colateral se configurará el nombre de la máquina. De otro modo tendríamos que configurar dicho nombre mediante otro método por ejemplo mediante la configuración específica de la máquina cliente a través del archivo [.filename]#rc.conf#.
====

[NOTE]
====
Para que el núcleo se pueda cargar sin problemas con etherboot en sistemas 5.X dicho núcleo tiene que tener compilado el soporte para _device hints_. Para ello normalmente se especifica la siguiente opción dentro del fichero de configuración del núcleo (consulte los comentarios del fichero [.filename]#NOTES#):

[.programlisting]
....
hints		"GENERIC.hints"
....

====

==== Preparación del sistema de ficheros raíz

Se debe crear un sistema de ficheros raíz en las estaciones de trabajo sin disco, concretamente en la localización especificada por `root-path` dentro de [.filename]#dhcpd.conf#. Las siguientes secciones describen dos formas de hacer esto.

===== Utilización del "script" [.filename]#clone_root#

Este es el modo más rápido de crear un sistema de ficheros raíz, pero actulamente sólo se encuentra soportado en FreeBSD 4.X. El "script" de shell se encuentra en [.filename]#/usr/shared/examples/diskless/clone_root# y debe ser configurado al menos para ajustar el lugar donde se construirá el sistema de ficheros (concretamente la variable `DEST`).

Consulte los comentarios que se encuentran al comienzo del "script" para conocer cuales son las instrucciones que debe seguir. Allí se explica cómo se construye el sistema de ficheros base y como determinados ficheros se pueden sobreescribir de manera selectiva por versiones específicas para funcionar sin discos, para toda una subred o para una máquina individual. También allí se muestran ejemplos de los ficheros [.filename]#/etc/fstab# y [.filename]#/etc/rc.conf# para máquinas sin disco.

Los archivos [.filename]#README# que se encuentran dentro de [.filename]#/usr/shared/examples/diskless# contienen mucha información de base, que junto con el resto de ejemplos dentro del directorio [.filename]#diskless# sirven para documentar un método de configuración distinto del que se utiliza en [.filename]#clone_root# y en los " scripts" del sistema de [.filename]#/etc#, que resultan ser un tanto confusos. No obstante se pueden utilizar a modo de referencia, excepto si se prefiere utilizar el método que se describe en ellos, en cuyo caso se necesitará modificar y adaptar los "scripts" de forma adecuada.

===== Utilización del procedimiento estándar de `make world`

Este método se puede utilizar tanto en FreeBSD 4.X o 5.X y se instalará un sistema completamente nuevo (no sólo el sistema de ficheros raíz) dentro de `DESTDIR`. Basta con ejecutar el siguiente " script":

[.programlisting]
....
#!/bin/sh
export DESTDIR=/data/misc/diskless
mkdir -p ${DESTDIR}
cd /usr/src; make world && make kernel
cd /usr/src/etc; make distribution
....

Una vez ejecutado puede ser necesario ajustar los ficheros [.filename]#/etc/rc.conf# y [.filename]#/etc/fstab# que se encuentran en `DESTDIR` de acuerdo con nuestras necesidades.

==== Configuración de la partición de intercambio

En caso de ser necesario se puede acceder a un fichero de intercambio (swap) a través del sistema NFS. Uno de los métodos típicamente utilizados para realizar esta tarea ha sido retirado de la distribución 5.X.

===== NFS swap en sistemas FreeBSD 4.X

La ubicación del fichero de intercambio y su tamaño se puede especificar con las opciones FreeBSD-specific 128 y 129 de BOOTP/DHCP. A continuación se muestran varios ejemplos de ficheros de de configuración para  ISC DHCP 3.0 o bootpd:

[.procedure]
====

. Añadir las siguientes líneas al fichero [.filename]#dhcpd.conf#:
+
[.programlisting]
....

# Global section
option swap-path code 128 = string;
option swap-size code 129 = integer 32;

host margaux {
  ... # Standard lines, see above
  option swap-path "192.168.4.4:/netswapvolume/netswap";
  option swap-size 64000;
}
....
+ 
`swap-path` es la ruta al directorio donde se instalarán los archivos de intercambio. Cada Cada fichero se denomina [.filename]#swap.direccion-ip-del-cliente#.
+ 
Versiones más antiguas de dhcpd usaban una sintáxis del estilo de `option option-128 "...`, lo cual ya no está soportado.
+ 
[.filename]#/etc/bootptab# normalmente utiliza la siguiente sintaxis:
+
[.programlisting]
....
T128="192.168.4.4:/netswapvolume/netswap":T129=0000fa00
....
+
[NOTE]
======
El tamaño del fichero dedicado a intercambio se debe expresar en [.filename]#/etc/bootptab# en formato hexadecimal.
======

. En el servidor de ficheros NFS donde va a residir el fichero de "swap" se debe(n) crear dicho(s) fichero(s)
+
[source,bash]
....
# mkdir /volumenintercambiored/intercambiored
# cd /volumenintercambiored/intercambiored
# dd if=/dev/zero bs=1024 count=64000 of=swap.192.168.4.6
# chmod 0600 swap.192.168.4.6

....
+ 
_192.168.4.6_ es la dirección IP del cliente sin disco.
. En el servidor NFS añadir a [.filename]#/etc/exports# la siguiente línea: 
+
[.programlisting]
....

/volumenintercambiored  -maproot=0:10 -alldirs margaux corbieres
....
+ 
A continuación indicar a mountd que vuelva a leer el fichero [.filename]#/etc/exports# como se ha indicado anteriormente.

====

===== NFS swap en FreeBSD 5.X

El núcleo no soporta la activación del intercambio a través de NFS en tiempo de arranque. De esta forma la "swap" se debe activar mediante los "scripts" montando un sistema de ficheros de lectura-escritura y creando y activando el fichero de intercambio. Para crear un fichero de intercambio de un determinado tamaño se puede ejecutar lo siguiente:

[source,bash]
....
# dd if=/dev/zero of=/ruta/al/fichero/de/intercambio bs=1k count=1 oseek=100000
....

Para activar el intercambio se tiene que añadir la siguiente línea al fichero de configuración [.filename]#rc.conf#:

[.programlisting]
....
swapfile=/ruta/al/fichero/de/intercambio
....

==== Varios

===== Ejecución con un [.filename]#/usr# de sólo lectura

Si la estación de trabajo sin disco se configura para utilizar el sistema X-Window se tiene que ajustar el fichero de configuración de xdm debido a que dicho fichero sitúa por defecto el fichero de "logs" de errores en el directorio [.filename]#/usr#.

===== Uso de un servidor no-FreeBSD

Cuando el servidor del sistema de ficheros raíz no ejecuta FreeBSD se tiene que crear un sistema de ficheros raíz sobre una máquina FreeBSD para después copiarlo al servidor original mediante las órdenes `tar` o `cpio`.

En esta situación algunas veces surgen varios problemas relacionados con los dispositivos especiales que se encuentran en el directorio [.filename]#/dev# debido a los diferentes tamaños de los enteros mayor/menor. Una solución para este problema consiste en exportar un directorio del servidor no-FreeBSD, montar este directorio en la máquina FreeBSD anterior y ejecutar `MAKEDEV` en dicha máquina para crear las entradas de dispositivo correctas (FreeBSD 5.0 y posteriores utilizan man:devfs[5] para ubicar nodos de dispositivos de forma transparente para el usuario de tal modo que la ejecución de `MAKEDEV` en estos sistemas no sirve para nada).

[[network-isdn]]
== RDSI

http://www.alumni.caltech.edu/~dank/isdn/[la página de RDSI de Dan Kegel] constituye un recurso de información bastante bueno sobre la tecnología RDSI (ISDN en inglés) y sobre el hardware relacionado.

A continuación se comenta un esquema rápido sobre RDSI:

* Si usted vive en Europa le puede resultar útil leer la sección sobre tarjetas RDSI.
* Si se prevee utilizar RDSI principalmente para conectarse a Internet a través de un Proveedor de Servicios utilizando un mecanismo de marcación automática no dedicado (dial-up), se puede echar un vistazo a los Adaptadores de Terminal. Dichos adaptadores proporciona la mayor flexibilidad y garantiza los mínimos problemas en caso de cambio de cambio de proveedor.
* Si estamos conectados a dos LAN o conectando a Internet con una conexión RDSI dedicada puede ser interesante considerar la opción de usar un "router/bridge" único.

El coste es un factor importante a la hora de determinar qué solución se debe escoger. Las siguientes opciones se encuentran ordenadas desde las más baratas hasta las más caras.

[[network-isdn-cards]]
=== Tarjetas RDSI

La implementación de RDSI que posee FreeBSD soporta sólamente el estandar DSS1/Q.931 (también conocido como Euro-RDSI) utilizando tarjetas pasivas. A partir de FreeBSD 4.4 se soportan también algunas tarjetas activas usando firmware que además soporta otros protocolos de señalización; esto también sucede con la primera tarjeta RDSI de acceso primario (PRI) soportada.

El software isdn4bsd permite conectar con otras pasarelas RDSI utilizando IP sobre HDLC o bien PPP PPP síncrono: ambos mediante el uso del PPP del núcleo con `isppp`, una versión modificada del controlador man:sppp[4] o mediante la utilización del PPP de entorno de usuario, man:ppp[8]. Si se utiliza man:ppp[8] de entorno de usuario se pueden agrupar dos o mas canales B de RDSI (channel bonding). Existe también software que permite a una máquina responder a llamadas de teléfono y algunas cosas más como un modem de 300 baudios.

Cada vez se soportan más tarjetas RDSI bajo FreeBSD y los informes existentes muestra que FreeBSD se utiliza con dichas tarjetas de forma satisfactoria en toda Europa y también en otras partes del mundo.

Las tarjetas RDSI pasivas soportadas en FreeBSD son principalmente las que poseen el chip Infineon (antiguamente Siemens) ISAC/HSCX/IPAC. También las tarjetas RDSI con los chips de Cologne (en bus ISA exclusivamente), tarjetas PCI con el chip Winbond W6692, algunas tarjetas con combinaciones de los chips Tiger 300/320/ISAC y también algunas tarjetas basadas en chips propietarios como las AVM Fritz! PCI V.1.0 y AVM Fritz! PnP.

Actualmente las tarjetas RDSI activas soportadas son las AVM B1 (ISA y PCI) BRI, y las AVM T1 PCI PRI.

Se puede consultar [.filename]#/usr/shared/examples/isdn/# para obtener documentación sobre isdn4bsd y también en http://www.freebsd-support.de/i4b/[la página principal de isdn4bsd], donde hay enlaces de ayuda, erratas y mucha más información útil, como por ejemplo el http://people.FreeBSD.org/~hm/[manual de isdn4bsd].

Si se quiere añadir soporte para un protoclo RDSI distinto para una tarjeta RDSI que no se encuentra soportada o para mejorar isdn4bsd en algún aspecto por favor póngase en contacto con {hm}.

Para realizar consultas referentes a la instalación, configuración y depuración de problemas relacionados con isdn4bsd le recomendamos recurrir a la lista de correo link:{freebsd-isdn-url}[freebsd-isdn].

=== Adaptadores de terminal RDSI

Los Adaptadores de Terminal (TA), son para RDSI lo que los modems son para las líneas de teléfono convencionales.

La mayor parte de los TAs utilizan el conjunto de instrucciones AT de los modem Hayes y se pueden utilizar en lugar del modem.

Un TA opera básicamente de igual forma que un modem, diferenciándose en que las velocidades de conexión y "throughput" son mucho más grandes. La configuración de crossref:ppp-and-slip[ppp,PPP] se realiza exactamente igual que para una configuración de modem convencional.

La ventaja principal de utilizar un TA para conectarse a un proveedor de servicios de internet consiste en que se puede usar PPP dinámico. Ya que el espacio de direcciones de IP se está direcciones de IP se está convirtiendo cada vez convirtiendo en un recurso cada dí más limitado y escaso los proveedores ya no desean proporcionar direcciones IP estáticas a sus clientes. No obstante la mayoría de los "routers standalone" no son capaces de adquirir direcciones IP dinámicas.

Los TAs confían completamente en el dæmon de PPP que se está ejecutando para proporcionar fiabilidad y estabilidad en la conexión. De esta forma si se tiene configurado PPP se puede migrar fácilmente de la utilización de modems analógicos al uso de RDSI. No obstante si existía algún problema con PPP antes de efectuar la migración dichos problemas persistirán en RDSI.

Si se desea máxima estabilidad se puede utilizar la opción crossref:ppp-and-slip[ppp,PPP], no el crossref:ppp-and-slip[userppp,PPP a nivel de usuario].

Se sabe que los siguientes TAs funcionan con FreeBSD:

* Motorola BitSurfer y Bitsurfer Pro
* Adtran

La mayoría de los demás TAs probablemente también funcionen puesto que los fabricantes siempre tratan de que sus productos puedan aceptar la mayoría de las órdenes AT.

El problema que existe con los TAs es que, como sucede con los modems, se necesita tener una buena tarjeta serie instalada en el sistema.

Se recomienda consultar el tutorial link:{serial-uart}[FreeBSD Serial Hardware] para obtener una comprensión detallada del funcionamiento de los dispositivos serie en FreeBSD y para comprender las diferencias entre puertos serie síncronos y asíncronos.

Un TA que se ejecuta a través de un puerto serie (asíncrono) está limitado a 115.2 Kbs, aunque la conexión RDSI sea de 128 Kbs. Para utilizar completamente el ancho de banda que RDSI proporciona, se debe conectar el TA a una tarjeta serie síncrona.

No se engañe creyendo que comprando un TA interno hará desaparecer los problemas síncronos/asíncronos. Los TA internos simplemente disponen de un chip de puerto serie instalado de fábrica. Lo único que se consigue con estos dispositivos es no tener que enchufarlos a la red elétrica ahorrando así un enchufe y no tener que comprar un cable serie, pero los problemas dichos anteriormente permanecen.

Una tarjeta asíncrona con un TA resulta ser al menos tan rápida como un "router standalone" y si FreeBSD controla dicha tarjeta se puede adaptar más fácilmente.

La elección de una tarjeta síncrona/TA versus un "router standalone" se trata en la mayoría de los casos de una cuestión cuasi-religiosa. Han existido diversas discusiones sobre este tema en varias listas de correo. Nosotros recomendamos que busque información en los http://www.freebsd.org/search/[históricos] para para poder sopesar los pros y los contras que se han esgrimido en tales discusiones.

=== "bridges/routers" RDSI "Stand-alone"

Los "bridges" o "routers" RDSI no son específicos de FreeBSD o de cualquier otro sistema operativo. Para una descripción completa de la tecnología de "bridge" y de pasarela de red por favor consulte cualquier libro sobre redes.

En el contexto de esta sección los términos "router", pasarela y "bridge" se utilizarán indistintamente.

Según va bajando el coste de los " routers/bridges" RDSI su utilización entre el público en general va en aumento. Un "router" RDSI es una pequeña caja que se conecta directamente a la red Ethernet local y que gestiona sus propias conexiones con el "bridge/router" remoto. Posee un software preconfigurado para comunicarse vía PPP y tambíen utilizando otros protocolos de uso común.

Un router sopota una mayor tasa de paquetes (throughput) que un "standalone TA", ya que utiliza una conexión RDSI síncrona de forma completa.

El problema principal que surge con los "routers" y los "bridges" RDSI es que la interoperatibilidad entre fabricantes muchas veces causa problemas. Si se está planificando conectarse a un proveedor de servicios resulta conveniente discutir previamente con ellos las necesidades y requisitos.

Si se tiene en mente conectar dos segmentos LAN tales como su LAN de casa y la LAN de su oficina RDSI proporciona la solución más simple y menos costosa de gestionar. Esto es así porque al comprar usted mismo el equipamiento necesario para ambos extremos de la conexión tiene usted el control sobre el enlace y puede asegurar su correcto funcionamiento.

Por ejemplo, si queremos conecar una computadora casera o una sucursal de la red de oficinas con la oficinal central, se puede utilizar una configuración como la que se muestra a continuación.

.Sucursal o red doméstica
[example]
====
La red utiliza una topología basada en bus con Ethernet tipo 10 base 2 ("thinnet"). Se conecta, en caso de ser necesario, el "router" a la red cableada mediante un "transceiver" AUI/10BT.

image::isdn-bus.png[10 Base 2 Ethernet]

Si nuestra sucursal o red hogar está compuesta únicamente por una computadora se puede utilizar un cable cruzado de par trenzado para conectar con el "router standalone" de forma directa.
====

.Oficina central u otra LAN
[example]
====
La red utiliza una topología en estrella basada en Ethernet de 10 base T ("Par Trenzado").

image::isdn-twisted-pair.png[ISDN Network Diagram]

====

Una gran ventaja que poseen la mayoría de los "routers/bridges" es que pueden gestionar al mismo tiempo dos conexiones PPP _independientes_ destinadas a dos organizaciones distintas. Esta funcionalidad no se proporciona en la mayoría de los TAs, excepto para determinados modelos (normalmente más caros) que se fabrican con dos puertos serie. No confunda esto con la agrupación de canales, MPP, etc.

Esta característica puede resultar muy útil si, por ejemplo, se dispone de una conexión RDSI dedicada con la oficina y queremos introducirnos en ella pero no queremos utilizar otra línea RDSI en el trabajo. Un "router" situado en las instalaciones de la oficina puede gestionar una conexión de canal B dedicada (64 Kpbs) hacia internet y utilizar el otro canal B como una conexión de datos independiente. El segundo canal B se puede utilizar para marcación remota ("dial-in" y " dial-out") o para agrupación dinámica de canales (MPP, etc) en conjunción con el primer canal B con el objetivo de obtener un mayor ancho de banda.

Un "bridge" Ethernet permite transmitir más tráfico aparte del tráfico IP. Se puede transmitir IPX/SPX o cualquier otro protocolo que se esté utilizando.

[[network-nis]]
== NIS/YP

=== ?Qué es esto?

NIS, siglas de Network Information Services (Servicios de Información de Red), fué un servicio desarrollado por Sun Microsystems para centralizar la administración de sistemas UNIX(R) (originalmente SunOS(TM)). Actualmente se considera como un estándar de la industria; los principales sistemas tipo UNIX(R) (Solaris(TM), HP-UX, AIX(R), Linux, NetBSD, OpenBSD, FreeBSD, etc) implementan NIS.

NIS también se conocía como el servicio de páginas amarillas pero debido a problemas legales debidos a la propiedad de marcas comerciales, Sun tuvo que cambiar el nombre. El antíguo término ("Yellow Pages" o yp) todavía se ve y se utiliza con frecuencia.

Se trata de un sistema cliente servidor basado en llamadas RPC que permite a un grupo de máquinas que se encuentran definidas dentro de un dominio administrativo NIS compartir un conjunto de ficheros de configuración. Esto permite al administrador de sistemas por un lado configurar clientes NIS de forma minimalista y por otro lado centralizar la gestión de los ficheros de configuración en una única ubicación (una sola máquina).

Se trata de algo similar al sistema de dominio de Windows NT(R) aunque la implementación interna no se puede comparar, la funcionalidad y el servicio obtenido son similares.

=== Términos/procesos que debe usted conocer

Existen varios conceptos y varios procesos de usuario que el usuario no versado en estos temas suele encontrarse la primera vez que se intenta implantar un servicio de NIS en FreeBSD, tanto si se intenta configurar un servidor como si se intenta configurar un cliente:

[.informaltable]
[cols="1,1", options="header"]
|===
| Term
| Description

|NIS domainname
|Un servidor maestro de NIS y todos sus clientes (incluyendo a sus servidores esclavos) poseen el mismo nombre dominio NIS. Al igual que ocurre con el nombre de dominio de Windows NT(R), el nombre de dominio de NIS no tiene nada que ver con el nombre de dominio de DNS.

|portmap
|Debe ejecutarse para que se activen las llamadas a procedimientos remotos (Remote Procedure Call o RPC) que son utilizadas por NIS. Si portmap no se está ejecutando no se podrá ejecutar ni clientes ni servidores de NIS.

|ypbind
|"Asocia" un cliente con un servidor NIS. Primeramente se lee el nombre de dominio NIS del sistema y utilizando RPC se conecta con el servidor. ypbind es la parte central de la comunicación cliente servidor del sistema NIS; si ypbind muere en una máquina cliente, dicha máquina no podrá acceder al servidor NIS.

|ypserv
|Debe ejecutarse sólamente en los servidores NIS; se trata del proceso servidor de NIS. Si man:ypserv[8] muere, el servidor no será capaz de responder a peticiones NIS (no obstante, si se definen servidores NIS esclavos la situación puede recuperarse). Existen algunas implementaciones de NIS (no es el caso de FreeBSD) que no intentan conectarse con otro servidor si el servidor con otro servidor si el servidor que se estaba que se estaba utilizando muere. A menudo lo único que se puede hacer en estos casos es reiniciar el servidor (el proceso o la propia máquina) o el proceso ypbind del cliente.

|rpc.yppasswdd
|Otro proceso que sólo debe ejecutarse en el servidor maestro de NIS; se trata de un dæmon que permite a los clientes de NIS modificar las contraseñas de los usuarios. Si no se ejecuta este dæmon los usuarios tendrán que entrar en el servidor maestro de NIS para cambiar sus contraseñas allí.
|===

=== ?Cómo funciona?

Existen tres tipos de máquinas dentro del entorno NIS: los servidores maestros, los servidores esclavos y los clientes de NIS. Los servidores actúan como repositorios centrales para almacenamiento de información de configuración. Los servidores maestros mantienen una copia maestra de dicha información, mientras que los servidores esclavos mantienen copias de la información maestra por motivos de redundancia. Los servidores se encargan de transmitir la información necesaria a los clientes a petición de estos últimos.

De esta forma se pueden compatir mucha información contenida en varios archivos. Los ficheros [.filename]#master.passwd#, [.filename]#group# y [.filename]#hosts# normalmente se comparten a través de NIS. Siempre que un proceso en un cliente necesita información que, en caso de no utilizar NIS, se podría recuperar de ficheros locales, en este caso se envía una solicitud al servidor NIS con el que nos encontramos asociados.

==== Clases de máquinas

* _Servidor de NIS maestro_. Este servidor, semejante a un controlador de dominio primario de Windows NT(R) mantiene todos los archivos que utilizan los clientes. Los ficheros [.filename]#passwd#, [.filename]#group# y algunos otros se encuentran ubicados en el servidor maestro.
+
[NOTE]
====
Resulta posible configurar una máquina para que actúe como servidor NIS maestro para más de un dominio NIS. No obstante esta configuración no se va a tratar en esta introducción, en la cual asumimos un entorno NIS de tamaño relativamente pequeño.
====

* _Servidores de NIS esclavos_. Semejantes a los controladores de backup de Windows NT(R), los servidores NIS esclavos se utilizan para proporcionar redundancia en entornos de trabajo donde la disponibilidad del servicio resulta muy importante. Además se utilizan para distribuir la carga que normalmente soporta un servidor maestro: los clientes de NIS siempre se asocian con el servidor de NIS que posee mejor tiempo de respuesta, y esto y esto también incluye a los servidores de NIS esclavos.

* _Clientes NIS_. Los clientes NIS, de forma semejante a las estaciones de trabajo de Windows NT(R), se validan contra un servidor NIS (en el caso de Windows NT(R) se validan contra un controlador de dominio) para acceder al sistema.

=== Uso de NIS/YP

Esta sección trata sobre cómo configurar y poner en funcionamiento un entorno de NIS sencillo.

[NOTE]
====
Esta sección supone que se está utilizando utilizando FreeBSD 3.3 o posteriores. Las instrucciones dadas aquí _probablemente_ funcionen también en cualquier versión de FreeBSD superior a la 3.0 pero no podemos garantizar que esto sea así.
====

==== Planificación

Vamos a suponer que somos el administrador de un pequeño laboratorio de una universidad. En este laboratorio, compuesto por 15 máquinas FreeBSD, actualmente no existe ningún punto de administración centralizada; cada máquina posee sus sus propios [.filename]#/etc/passwd# y [.filename]#/etc/master.passwd#. Estos ficheros se encuentran sincronizados el uno con el otro mediante intervención manual; por tanto, cuando queramos añadir un usuario a nuestro laboratorio tendremos que ejecutar `adduser` en todas las máquinas. Claramente esta situación tiene que cambiar, de tal forma que hemos decidido crear un dominio NIS en el laboratorio usando dos máquinas como servidores NIS.

La configuración de nuestro laboratorio debería ser algo parecido a lo siguiente:

[.informaltable]
[cols="1,1,1", options="header"]
|===
| Nombre de máquina
| Dirección IP
| Papel

|`ellington`
|`10.0.0.2`
|servidor NIS maestro

|`coltrane`
|`10.0.0.3`
|Servidor NIS esclavo

|`basie`
|`10.0.0.4`
|Estación de trabajo del profesorado

|`bird`
|`10.0.0.5`
|máquina cliente

|`cli[1-11]`
|`10.0.0.[6-17]`
|Resto de máquinas clientes
|===

Si se está configurando un esquema de NIS por primera vez es una buena idea detenerse a pensar cómo queremos implantar el sistema. Existen varias decisiones que se deben tomar independientemente del tamaño de nuestra red.

===== Elección del nombre de dominio NIS

Este nombre puede no ser el "nombre de dominio" al que estamos acostumbrados. Resulta más preciso llamarlo "nombre de dominio NIS". Cuando un cliente genera peticiones de NIS que llegan a todas las máquinas (broadcast) solicitando información se incluye el nombre de dominio NIS que tiene configurado. De esta forma, varios servidores de dominios distintos situados en la misma red pueden discriminar las peticiones recibidas. Se puede pensar en el nombre de dominio NIS como un identificador de grupos de máquinas que se encuentran relacionados administrativamente de alguna forma.

Algunas organizaciones eligen utilizar su nombre de dominio de Internet como nombre de dominio NIS. Esto no se recomienda ya que puede causar confusión cuando se intentan depurar problemas de red. El nombre de dominio NIS debería ser un nombre único dentro de nuestra red y resulta más útil aún si el nombre elegido puede describir de alguna forma al conjunto de máquinas que representa. Por ejemplo el departamento de arte de la empresa Acme puede utilizar como nombre de dominio "acme-art". En nuestro ejemplo hemos utilizado el nombre `test-domain`.

No obstante algunos sistemas operativos (de forma notable SunOS(TM)) utilizan como nombres de dominio nombres de Internet. Si se poseen máquinas con esta restricción no queda más remedio que _utilizar_ los nombres de dominio de Internet como nombres de dominio NIS.

===== Requisitos físicos de los servidores

Existen varias cosas que debemos tener en cuenta cuando se selecciona una máquina para actuar como servidor NIS. Una de las características desafortunadas del servicio de páginas amarillas es el alto nivel de dependencia que llegan a tener los clientes respecto del servidor de NIS. Si el cliente no puede contactar con el servidor NIS normalmente la máquina se queda en un estado totalmente inutilizable. La carencia de información de usuarios y grupos provoca que las máquinas se bloqueen. Con esto en mente debemos debemos asegurarnos de escoger un servidor de NIS que no se reinicie de forma habitual o uno que no se utilice para para desarrollar. Si se dispone de una red con poca carga puede resultar aceptable colocar el servidor de NIS en una máquina donde se ejecuten otros servicios pero en todo momento se debe tener presente que si por cualquier motivo el servidor de NIS quedara inutilizable afectaría a _todas_ las máquinas de forma negativa.

==== Servidores NIS

Las copias canónicas de toda la información que mantiene el sistema de páginas amarillas se almacenan en una única máquina denominada servidor maestro de NIS. Las bases de datos utilizadas para almacenar la información se denominan mapeos NIS. En FreeBSD estas asociaciones o mapeos se almacenan en el directorio [.filename]#/var/yp/[nombrededominio]# donde [.filename]#[nombrededominio]# es el nombre del dominio de NIS que el servidor gestiona. Un único servidor NIS puede gestionar varios dominios al mismo tiempo de forma que resulta posible tener varios directorios como el anterior, uno por cada dominio soportado. Cada dominio posee su conjunto de mapeos independiente y propio.

Los servidores maestro y esclavos manejan todas las peticiones de a través del dæmon `ypserv`. `ypserv` se responsabiliza de recibir peticiones de los clientes NIS. Estas peticiones se traducen a una ruta dentro del servidor. Esta ruta localiza un fichero de base de datos determinado del servidor de NIS, y finalmente `ypserv` se encarga de transmitir la información de dicha base de datos de vuelta al cliente que la solicitó.

===== Configuración de un servidor de NIS maestro

La configuración de un servidor de NIS maestro puede resultar relativamente sencilla dependiendo de las necesidades que se tengan. FreeBSD viene preconfigurado por defecto con un servicio NIS. Todo lo que necesitamos es añadir la siguiente línea en [.filename]#/etc/rc.conf# y FreeBSD se encarga del resto.

[.procedure]
====
[.programlisting]
....
nisdomainname="test-domain"
....
. Esta línea establece el nombre de dominio NIS como `test-domain`, cuando se realiza la configuración de la red (por ejemplo, después de un reinicio).
+
[.programlisting]
....
nis_server_enable="YES"
....
. Esta variable indica a FreeBSD que ejecute los procesos necesarios para actuar como un servidor de NIS la próxima vez que se configure el subsistema de red.
+
[.programlisting]
....
nis_yppasswdd_enable="YES"
....
. Esto permite activar el dæmon `rpc.yppasswdd` el cual, como se ha mencionado anteriormente, permite a los usuarios realizar cambios de contraseña desde las máquinas clientes de NIS.
====

[NOTE]
====
Dependiendo de la configuración de NIS podemos necesitar añadir algunas entradas más. Consulte la <<network-nis-server-is-client,sección sobre servidores NIS que también actúan como clientes>>, más adelante en el texto, para saber más sobre esto.
====

Una vez hecho esto todo lo que tenemos que hacer es ejecutar `/etc/netstart` como superusuario. Esta orden realiza los pasos de configuración necesarios utilizando los valores de las variables definidas en [.filename]#/etc/rc.conf#.

===== Inicialización de los mapeos de NIS

Las _asociaciones o mapeos de NIS_ no son más que ficheros de base de datos. Estos ficheros se generan a partir de los ficheros de configuración contenidos en el directorio [.filename]#etc/# excepto para el caso del fichero [.filename]#etc/master.passwd#. Esto es así por una buena razón ya que no suele ser buena idea propagar las contraseñas de `root` y de otras cuentas de administración a todos los servidores NIS del dominio. servidores NIS del dominio. Así, antes de inicializar los mapeos se debe ejecutar:

[source,bash]
....
# cp /etc/master.passwd /var/yp/master.passwd
# cd /var/yp
# vi master.passwd
....

Se deben borrar todas las entradas que hagan referencia a cuentas del sistema (`bin`, `tty`, `kmem`, `games`, etc), junto con cualquier cuenta que no deseemos que se transmita a los clientes NIS (por ejemplo la cuenta de `root` y cualquier otra cuenta con UID 0 (el del superusuario)).

[NOTE]
====
Asegúrese de que [.filename]#/var/yp/master.passwd# no se puede leer ni por grupos ni por el resto de usuarios (modo 600). Utilice `chmod` en caso de necesidad.
====

Una vez hecho esto es hora de inicializar las asociaciones de NIS. FreeBSD incluye un "script" denominado `ypinit` para realizar esta tarea (consulte su página del manual para obtener más información). Recuerde que este "script" se encuentra disponible en la mayoría de los sistemas UNIX(R), pero no en todos. En sistemas Digital UNIX/Compaq Tru64 UNIX se denomina `ypsetup`. Debido a que se pretende generar asociaciones para un servidor NIS maestro vamos a ejecutar `ypinit` con la opción `-m`. A modo de ejemplo, suponiendo que todos los pasos comentados anteriormente se han realizado con éxito, ejecute:

[source,bash]
....
ellington# ypinit -m test-domain
Server Type: MASTER Domain: test-domain
Creating an YP server will require that you answer a few questions.
Questions will all be asked at the beginning of the procedure.
Do you want this procedure to quit on non-fatal errors? [y/n: n] n
Ok, please remember to go back and redo manually whatever fails.
If you don't, something might not work.
At this point, we have to construct a list of this domains YP servers.
rod.darktech.org is already known as master server.
Please continue to add any slave servers, one per line. When you are
done with the list, type a <control D>.
master server   :  ellington
next host to add:  coltrane
next host to add:  ^D
The current list of NIS servers looks like this:
ellington
coltrane
Is this correct?  [y/n: y] y

[..salida de la generacion de mapeos..]

NIS Map update completed.
ellington has been setup as an YP master server without any errors.
....

`ypinit` debería haber creado el fichero [.filename]#/var/yp/Makefile# a partir de [.filename]#/var/yp/Makefile.dist#. Una vez creado este archivo presupone que se está usando un entorno NIS con un único servidor utilizando sólamente máquinas FreeBSD. Debido a que `test-domain` posee también un servidor NIS esclavo se debe editar el fichero [.filename]#var/yp/Makefile#:

[source,bash]
....
ellington# vi
              /var/yp/Makefile
....

Se debe comentar la línea que dice:

[.programlisting]
....
NOPUSH = "True"
....

(si es que no se encuentra ya comentada).

===== Configuración de un servidor NIS esclavo

La configuración de un servidor NIS esclavo resulta ser incluso más sencilla que la del maestro. Basta con entrar en el servidor esclavo y editar [.filename]#/etc/rc.conf# de foma semejante a como se realizó en el apartado anterior. La única diferencia consiste en que ahora debemos utilizar la opción `-s` cuando ejecutemos ejecutemos `ypinit`. A continuación del parámetro `-s` se debe especificar el nombre del servidor maestro de modo que la orden tendría que ser algo parecido a esto:

[source,bash]
....
coltrane# ypinit -s ellington test-domain

Server Type: SLAVE Domain: test-domain Master: ellington

Creating an YP server will require that you answer a few questions.
Questions will all be asked at the beginning of the procedure.

Do you want this procedure to quit on non-fatal errors? [y/n: n]  n

Ok, please remember to go back and redo manually whatever fails.
If you don't, something might not work.
There will be no further questions. The remainder of the procedure
should take a few minutes, to copy the databases from ellington.
Transferring netgroup...
ypxfr: Exiting: Map successfully transferred
Transferring netgroup.byuser...
ypxfr: Exiting: Map successfully transferred
Transferring netgroup.byhost...
ypxfr: Exiting: Map successfully transferred
Transferring master.passwd.byuid...
ypxfr: Exiting: Map successfully transferred
Transferring passwd.byuid...
ypxfr: Exiting: Map successfully transferred
Transferring passwd.byname...
ypxfr: Exiting: Map successfully transferred
Transferring group.bygid...
ypxfr: Exiting: Map successfully transferred
Transferring group.byname...
ypxfr: Exiting: Map successfully transferred
Transferring services.byname...
ypxfr: Exiting: Map successfully transferred
Transferring rpc.bynumber...
ypxfr: Exiting: Map successfully transferred
Transferring rpc.byname...
ypxfr: Exiting: Map successfully transferred
Transferring protocols.byname...
ypxfr: Exiting: Map successfully transferred
Transferring master.passwd.byname...
ypxfr: Exiting: Map successfully transferred
Transferring networks.byname...
ypxfr: Exiting: Map successfully transferred
Transferring networks.byaddr...
ypxfr: Exiting: Map successfully transferred
Transferring netid.byname...
ypxfr: Exiting: Map successfully transferred
Transferring hosts.byaddr...
ypxfr: Exiting: Map successfully transferred
Transferring protocols.bynumber...
ypxfr: Exiting: Map successfully transferred
Transferring ypservers...
ypxfr: Exiting: Map successfully transferred
Transferring hosts.byname...
ypxfr: Exiting: Map successfully transferred

coltrane has been setup as an YP slave server without any errors.
Don't forget to update map ypservers on ellington.
....

En estos momentos debemos tener un nuevo directorio llamado [.filename]#/var/yp/test-domain#. Las copias de los mapeos del servidor maestro se almacenan en dicho directorio. Es nuestra responsabilidad como administradores asegurar que dichas copias permanecen actualizadas en todo momento. La siguiente entrada en el archivo [.filename]#/etc/crontab# del servidor esclavo se encarga de realizar esta tarea:

[.programlisting]
....
20      *       *       *       *       root   /usr/libexec/ypxfr passwd.byname
21      *       *       *       *       root   /usr/libexec/ypxfr passwd.byuid
....

Estas dos líneas obligan al servidor esclavo a sincronizar los mapeos con el servidor maestro. Estas entradas no son obligatorias ya que el servidor maestro siempre intenta comunicar cualquier cambio que se produzca en sus asociaciones a todos los servidores esclavos ya que la información de, esclavos, ya que la información de, por ejemplo, contraseñas es de vital importancia para el funcionamiento de los sistemas que dependen del servidor. No obstante es una buena idea obligar a que se realicen estos cambios mediante las entradas anteriores. Esto resulta ser incluso más importante en redes de sobrecargadas donde las actualizaciones de asociaciones pueden algunas veces no llegar a realizarse de forma completa.

A continuación se ejecuta `/etc/netstart` en el servidor esclavo de igual de igual forma que se hizo con el servidor maestro; esto relanza de nuevo el servidor de NIS.

==== Clientes NIS

Un cliente de NIS establece lo que se conoce con el nombre de asociación (bind en inglés) con un servidor NIS NIS determinado utilizando el dæmon `ypbind`. `ypbind` comprueba el dominio por defecto del sistema (especificado mediante `domainname`) y comienza a enviar peticiones RPC a todos los elementos de la red local (broadcast). Estas peticiones especifican el nombre del dominio con el que se quiere establecer la asociación. Si esta petición alcanza una petición alcanza un servidor NIS configurado para servir dicho dominio el servidor responde a la petición e `ypbind` almacenará la dirección de dicho servidor. Si existen varios servidores disponibles (un maestro y varios esclavos, por ejemplo), `ypbind` utilizará la dirección del primero en responder. A partir de este punto el cliente dirigirá el resto de sus peticiones NIS directamente a la dirección IP almacenada. Ocasionalmente `ypbind` envía un "ping" sobre el servidor para comprobar que en efecto se encuentra funcionando. Si no se recibe contestación al ping dentro de un espacio de tiempo determinado `ypbind` marca el dominio como "sin asociar" y comienza de nuevo a inundar la red con la esperanza de localizar algún otro servidor NIS.

===== Configuración de un cliente NIS

La configuración de un cliente FreeBSD de NIS resulta ser una operación extremadamente sencilla.

[.procedure]
====
. Editar el fichero [.filename]#/etc/rc.conf# y añadir las siguientes líneas para establecer el nombre de dominio NIS y para que se ejecute `ypbind` al arranque de la red:
+
[.programlisting]
....
nisdomainname="test-domain"
nis_client_enable="YES"
....
+
. Para importar todas las entradas de contraseñas del servidor de NIS hay que eliminar todas las cuentas de usuario de [.filename]#/etc/master.passwd# y utilizar `vipw` para añadir la siguiente línea al final de dicho fichero:
+
[.programlisting]
....
+:::::::::
....
+
[NOTE]
======
Esta línea permite que cualquiera abra una cuenta en local, siempre que dicha cuenta se encuentre definida en las asociaciones de cuentas y contraseñas del servidor NIS. Existen varias formas de configurar los clientes NIS modificando esta línea. Consulte la <<network-netgroups,sección sobre netgroups>> que se encuentra más adelante en este mismo texto. Si quiere saber más puede consultar el libro de O'Reilly `Managing NFS and NIS`.
======
+
[NOTE]
======
Se debe mantener al menos una cuenta local (por ejemplo, una cuenta que no se importe a través de NIS) en el fichero [.filename]#/etc/master.passwd# y además dicha cuenta debería ser miembro del grupo `wheel`. Si algo va mal con el procedimiento descrito esta cuenta se puede utilizar para entrar en la máquina cliente de forma remota para posteriormente convertirse en superusuario e intentar solucionar el problema.
======
+
. Para importar todas las entradas de grupo posibles del servidor NIS se debe añadir la siguiente línea al fichero [.filename]#/etc/group#:
+
[.programlisting]
....
+:*::
....
====

Después de completar estos pasos deberímos ser capaces de ejecutar `ypcat passwd` y ver la asociación de contraseñas que se encuentra en el servidor de NIS

=== Seguridad en NIS

En general cualquier usuario remoto puede realizar peticiones de RPC a man:ypserv[8] y recuperar los contenidos de las asociaciones de NIS siempre y cuando el usuario remoto conozca el nombre de dominio de NIS. Para evitar este tipo de transacciones no autorizadas, man:ypserv[8] soporta una característica denominada "securenets" la cual se puede utilizar para limitar el acceso a un determinado conjunto de máquinas. En el arranque man:ypserv[8] intenta cargar la información de "securenets" a partir de un fichero denominado [.filename]#/var/yp/securenets#.

[NOTE]
====
Esta ruta de fichero varía dependiendo del camino especificado con la opción `-p`. Dicho fichero contiene entradas compuestas de, por un lado, un rango de red y por otro una máscara de red, separados por espacios en blanco. Las líneas que comienzan por "#" se consideran comentarios. A continuación se muestra un ejemplo de un fichero "securenet":
====

[.programlisting]
....
# admitir conexiones desde localhost -- obligatorio
127.0.0.1     255.255.255.255
# admitir conexiones desde cualquier host
# on the 192.168.128.0 network
192.168.128.0 255.255.255.0
# admitir conexiones desde cualquier host
# between 10.0.0.0 to 10.0.15.255
# esto incluye las maquinas en el 'testlab'
10.0.0.0      255.255.240.0
....

Si man:ypserv[8] recibe una petición de una dirección que coincide con alguna de las reglas especificadas en el fichero se procesa la petición. Si no existe ninguna coincidencia la petición se rechaza y se graba un mensaje de aviso. Si el archivo [.filename]#/var/yp/securnets# no existe `ypserv` acepta conexiones de cualquier máquina.

El programa `ypserv` también posee soporte para el paquete de Wietse Venema denominado tcpwrapper. Este paquete permite utilizar los ficheros de configuración de tcpwrapper para realizar control de acceso en lugar de utilizar [.filename]#var/yp/securenets#.

[NOTE]
====
Aunque ambos mecanismos de control de acceso proporcionan un grado de seguridad mayor que no utilizar nada resultan vulnerables a ataques de "falsificación de IPs". El cortafuegos de la red donde se implanta el servicio de NIS debería encargarse de bloquear el tráfico específico de dicho servicio.

Los servidores que utilizan [.filename]#/var/yp/securenets# pueden no prestar servicio a clientes NIS legítimos cuando se trabaja con implementaciones arcaicas de TCP/IP. Algunas de estas implementaciones ponen a cero todos los bits de máquina cuando se realizan broadcast y/o pueden fallar al especificar la máscara de red en el mismo caso, por citar algunos ejemplos. Mientras que algunos de estos problemas se pueden solucionar variando la configuración del cliente en otros casos podemos vernos obligados a prescindir del cliente en cuestión o a prescindir del fichero [.filename]#var/yp/securenets#.

Se desaconseja la utilización de [.filename]#var/yp/securenets# en un sistema con una implementación arcaica de TCP/IP y puede suponer una pérdida de funcionalidad para grandes zonas de la red.

La utilización del paquete tcpwrapper incrementa la latencia del servidor NIS. El retardo adicional introducido puede ser lo suficientemente grande como para causar la expiración de ciertos temporizadores de los programas clientes, especialmente en redes muy cargadas o con servidores de NIS lentos. Si se experimentan estos síntomas en varias máquinas clientes, puede ser conveniente convertir dichas máquinas en servidores NIS esclavos y obligarlas a asociarse con ellas mismas.
====

=== Prohibir el acceso a determinados usuarios

En nuestro laboratorio de ejemplo existe una máquina denominada `basie` que actúa sólo como una estación de trabajo para el profesorado. No queremos sacar a esta máquina del dominio NIS pero nos damos cuenta de que el fichero [.filename]#passwd# que se encuentra en el servidor de NIS posee cuentas tanto para profesores como para alumnos. ?Qué podemos hacer?.

Existe una forma de prohibir el acceso a determinados usuarios sobre una determinada máquina incluso aunque se encuentren dados de alta en la base de datos de NIS. Para realizar esto todo lo que debemos hacer es añadir `-nombredeusuario` al final del fichero [.filename]#/etc/master.passwd# en la máquina cliente donde _nombredeusuario_ es el nombre de usuario del usuario al que queremos prohibirle el acceso. Esto se debe realizar a poder ser mediante `vipw` ya que `vipw` realiza comprobaciones de seguridad sobre los cambios realizados y además se encarga de reconstruir la base de datos de contraseñas cuando se termina la edición. Por ejemplo, si quisiéramos prohibir el acceso al usuario `bill` a la máquina `basie` haríamos:

[source,bash]
....
basie# vipw
[añadimos -bill al final y salimos]
vipw: rebuilding the database...
vipw: done

basie# cat /etc/master.passwd

root:[password]:0:0::0:0:The super-user:/root:/bin/csh
toor:[password]:0:0::0:0:The other super-user:/root:/bin/sh
daemon:*:1:1::0:0:Owner of many system processes:/root:/sbin/nologin
operator:*:2:5::0:0:System &:/:/sbin/nologin
bin:*:3:7::0:0:Binaries Commands and Source,,,:/:/sbin/nologin
tty:*:4:65533::0:0:Tty Sandbox:/:/sbin/nologin
kmem:*:5:65533::0:0:KMem Sandbox:/:/sbin/nologin
games:*:7:13::0:0:Games pseudo-user:/usr/games:/sbin/nologin
news:*:8:8::0:0:News Subsystem:/:/sbin/nologin
man:*:9:9::0:0:Mister Man Pages:/usr/shared/man:/sbin/nologin
bind:*:53:53::0:0:Bind Sandbox:/:/sbin/nologin
uucp:*:66:66::0:0:UUCP pseudo-user:/var/spool/uucppublic:/usr/libexec/uucp/uucico
xten:*:67:67::0:0:X-10 daemon:/usr/local/xten:/sbin/nologin
pop:*:68:6::0:0:Post Office Owner:/nonexistent:/sbin/nologin
nobody:*:65534:65534::0:0:Unprivileged user:/nonexistent:/sbin/nologin
+:::::::::
-bill

basie#
....

[[network-netgroups]]
=== Uso de Netgroups

El método descrito en la sección anterior funciona razonablemente bien si las reglas especiales se definen para un conjunto pequeño de usuarios y/o máquinas. En dominios admnistrativos grandes puede que se nos _olvide_ prohibir el acceso a algún usuario en determinadas máquinas perdiendo de esta forma el principal beneficio de utilizar el servicio de páginas amarillas: _administración centralizada_.

La solución creada por los desarrolladores de NIS se denomina _netgroups_. Su propósito se asemeja al concepto de grupos utilizado por los sistemas UNIX(R). La diferencia principal es la carencia de un identificador numérico y la habilidad para definir un "netgroup" que incluye tanto a cuentas de usuario como a otros "netgroups".

Los netgroups se desarrollaron para gestionar redes grandes y y complejas con cientos de usuarios y máquinas. Por un lado esto es una Cosa Buena si nos encontramos en tal situación pero por otro lado esta complejidad añadida hace que sea casi imposible de explicar a través de ejemplos sencillos. El ejemplo que va a utilizar en lo que queda de sección ilustrará este hecho.

Vamos a suponer que la exitosa introducción del servicio de páginas amarillas en nuestro laboratorio ha llamado la atención de nuestros jefes. Nuestra siguiente tarea consiste en extender el dominio de NIS para que cubra otras máquinas del campus. Las tablas que se muestran a continuación contienen los nombres de los nuevos usuarios y máquinas junto con una breve descripción de ellas.

[.informaltable]
[cols="1,1", options="header"]
|===
| Nombre del Usuario/usuarios
| Descripción

|`alpha`, `beta`
|Empleados normales del departamento de IT

|`charlie`, `delta`
|Los nuevos aprendices del departamento de IT

|`echo`, `foxtrott`, `golf`, ...
|Empleados normales

|`able`, `baker`, ...
|Los actuales internos
|===

[.informaltable]
[cols="1,1", options="header"]
|===
| Nombre de la Máquina(s)
| Descripción

|`guerra`, `muerte`, `hambre`, `peste`
|Los servidores más importantes. Sólo los empleados de IT pueden entrar en estas máquinas

|`orgullo`, `avaricia`, `envidia`, `ira`, `lujuria`, `pereza`
|Servidores de menor importancia. Todos los miembros del departamento de IT pueden entrar en ellos.

|`uno`, `dos`, `tres`, `cuatro`, ...
|Estaciones de trabajo ordinarias. Sólo los empleados _actuales_ pueden utilizar estas máquinas.

|`trashcan`
|Una máquina muy vieja sin ningún dato dato crítico. Incluso los internos pueden utilizar esta máquina.
|===

Si se trata de implementar estas restricciones a nivel de usuario particular tendríamos que añadir una línea `-usuario` a cada fichero [.filename]#passwd# del sistema para cada usuario que tuviera prohibido el acceso a dicho sistema. Si nos olvidamos de una sola entrada podrímos tener serios problemas. Puede ser factible realizar esta configuración cuando se instala el servicio pero no obstante el riesgo que corremos al mantener este sistema de restricciones en el día día es muy alto. Después de todo Murphy era un optimista.

La gestión de esta situación mediante netgroups ofrece varias ventajas. Cada usuario no tiene que tratarse de una forma individualizada; basta con añadir un usuario a uno o más netgroups y posteriormente permitir o prohibir el acceso para todos los usuarios del netgroup. Si se añade una nueva máquina al servicio de NIS simplemente tendremos que definir restricciones de acceso para los netgroups definidos en la red. Si se añade un nuevo usuario bastará con añadir dicho usuario a un netgroup existente. Estos cambios son independientes unos de otros: se acabó la necesidad de aplicar la frase "por cada combinación de usuario y máquina haga esto y esto". Si hemos planificado nuestro servicio de NIS cuidadosamente, sólo tendremos que modificar un fichero de configuración en un determinado servidor para permitir o denegar estos accesos.

El primer paso consiste en la inicialización de la asociación o mapeo del netgroup. La orden de FreeBSD man:ypinit[8] no crea este mapeo por defecto pero una vez creado será tenido en cuenta por la implementación de NIS. Para crear una asociación vacía simplemente escriba:

[source,bash]
....
ellington# vi /var/yp/netgroup
....

y comienze a añadir contenido. En nuestro ejemplo necesitamos al menos cuatro netgroups: empleados de IT, aprendices de IT, empleados normales e internos.

[.programlisting]
....
IT_EMP  (,alpha,test-domain)    (,beta,test-domain)
IT_APP  (,charlie,test-domain)  (,delta,test-domain)
USERS   (,echo,test-domain)     (,foxtrott,test-domain) \
        (,golf,test-domain)
INTERNS (,able,test-domain)     (,baker,test-domain)
....

`IT_EMP`, `IT_APP` etc. son los nombres de los netgroups. Cada conjunto delimitado por paréntesis define una o más cuentas como pertenecientes al netgroup. Existen tres campos definidos dentro de dichos de dichos grupos:

. El nombre de las máquinas donde los siguientes items son aplicables. Si no se especifica un nombre de máquina la entrada se aplica a todas las máquinas existentes. Si se especifica una máquina determinada entraremos en un mundo lleno de horrores y confusiones así que mejor no hacerlo.
. El nombre de la cuenta que pertenece al netgroup que estamos definiendo.
. El dominio NIS donde reside la cuenta. Se pueden importar cuentas de otros dominios NIS (en caso de que usted pertenezca al extraño grupo de personas que gestionan más de un dominio NIS.

Cada uno de estos campos puede contener comodines. Consulte man:netgroup[5] para obtener más detalles.

[NOTE]
====
No se deben utilizar nombres de los netgroups superiores a ocho caracteres, especialmente si las máquinas de nuestro dominio utilizan sistemas operativos variados. Los nombres son sensibles a las mayúsculas/minúsculas: se recomienda utilizar nombres en mayúsculas para distinguirlos de los usuarios o máquinas.

Algunos clientes de NIS (distintos de los clientes FreeBSD) no pueden gestionar netgroups con un gran número de entradas. Por ejemplo algunas versiones antiguas de SunOS(TM) comienzan a presentar problemas si un netgroup contiene más de _quince entradas_. Se puede solventar este problema creando varios sub-netgroups de como mucho quince usuarios y finalmente creando el verdadero netgroup compuesto por los sub-netgroups:

[.programlisting]
....
BIGGRP1  (,joe1,domain)  (,joe2,domain)  (,joe3,domain) [...]
BIGGRP2  (,joe16,domain)  (,joe17,domain) [...]
BIGGRP3  (,joe31,domain)  (,joe32,domain)
BIGGROUP  BIGGRP1 BIGGRP2 BIGGRP3
....

Se puede repetir este proceso si se tienen que definir más de 225 usuarios dentro de un único netgroup.
====

La activación y distribución de nuestro nuevo mapeo NIS resulta sencillo:

[source,bash]
....
ellington# cd /var/yp
ellington# make
....

Esto genera las tres asociaciones NIS [.filename]#netgroup#, [.filename]#netgroup.byhost# y [.filename]#netgroup.byuser#. Utilice man:ypcat[1] para comprobar si el nuevo mapeo NIS se encuentra disponible:

[source,bash]
....
ellington% ypcat -k netgroup
ellington% ypcat -k netgroup.byhost
ellington% ypcat -k netgroup.byuser
....

La salida de la primera orden debería parecerse a los contenidos del fichero [.filename]#/var/yp/netgroup#. La segunda orden no mostrará ninguna salida salvo que se hayan especificado netgroups específicos para máquinas. La tercera orden se puede utilizar para obtener la lista de los netgroups a los que petenece un determinado usuario.

La configuración del cliente es bastante simple. Para configurar el servidor `guerra` se debe ejecutar man:vipw[8] y sustituír la línea

[.programlisting]
....
+:::::::::
....

por

[.programlisting]
....
+@IT_EMP:::::::::
....

Ahora sólo se importan los datos para los usuarios que se encuentren definidos en el netgroup `IT_EMP`; dichos datos se importan en la base de datos de contraseñas de `guerra` y sólo se permite el acceso a estos usuarios.

Por desgraciaesta información también se aplica a la función `~` del shell y a todas las rutinas que traducen nombres de usuarios con los correspondientes identificadores númericos de usuario (uid). En otras palabras, la orden `cd ~` no va a funcionar, `ls -l` muestra el identificador numérico en lugar del nombre de usuario y `find . -user joe -print` produce un error de `No such user` ("Usuario desconocido"). Para solucionar esto debemos importar todas las entradas de usuario en la máquina cliente NIS pero sin permitirles el acceso.

Esto se puede realizar añadiendo otra línea al fichero [.filename]#/etc/master.passwd#. Esta línea debería contener lo siguiente:

`+:::::::::/sbin/nologin`, lo que significa que se "importen todas las entradas pero que se reemplace el shell por [.filename]#/sbin/nologin#". Se puede sustituir cualquier campo de la entrada de contraseñas especificando un valor concreto para dicho campo en el fichero local local [.filename]#/etc/master.passwd#.

[WARNING]
====

Asegúrese de que la línea `+:::::::::/sbin/nologin` se sitúa después de `+@IT_EMP:::::::::`. Si esto no se cumple todas las cuentas de usuario importadas del servidor NIS poseerán [.filename]#/sbin/nologin# como shell de acceso.
====

Después de este cambio si se introduce un nuevo empleado en el departamento de IT basta con cambiar una asociación de NIS. Se podría aplicar una aproximación similar para los servidores menos importantes sustituyendo la cadena `+:::::::::` en las versiones locales del fichero [.filename]#/etc/master.passwd# por algo parecido a esto:

[.programlisting]
....
+@IT_EMP:::::::::
+@IT_APP:::::::::
+:::::::::/sbin/nologin
....

Las líneas correspondientes para las estaciones de trabajo normales podrían ser:

[.programlisting]
....
+@IT_EMP:::::::::
+@USERS:::::::::
+:::::::::/sbin/nologin
....

Y parece que todos nuestros problemas de gestión han desaparecido; hasta que unas semanas más tarde se produce un cambio en la política de gestión: el depatamento de IT comienza a alquilar interinos. Los interinos de IT pueden utilizar las estaciones de trabajo normales y los servidores menos importantes y los aprendices de IT pueden acceder a los servidores principales. No nos queda más remedio que añadir un nuevo netgroup denominado `IT_INTERN` y añadir los nuevos interinos de IT a este nuevo grupo y comenzar a cambiar la la configuración de cada máquina, una por una... Como dice el antiguo proverbio: "Errores en la planificación centralizada conllevan grandes quebraderos de de cabeza globales".

La habilidad que posee NIS para crear netgroups a partir de otros netgroups se puede utilizar para evitar la situación anterior. Una posibilidad consiste en la creación de netgroups basados en roles. Por ejemplo, se podría crear un netgroup denominado `BIGSRV` para definir las restricciones de acceso para los servidores importantes, otro grupo denominado `USERBOX` para las estaciones de trabajo... Cada uno de estos netgroups podría contener los netgroups que pueden acceder a dichas máquinas. Las nuevas entradas para nuestro mapeo NIS de netgroups sería algo así:

[.programlisting]
....
BIGSRV    IT_EMP  IT_APP
SMALLSRV  IT_EMP  IT_APP  ITINTERN
USERBOX   IT_EMP  ITINTERN USERS
....

Este método de definir restricciones de acceso funciona razonablemente bien si podemos definir grupos de máquinas que posean restricciones semejantes. Por desgracia lo normal es que este caso resulta ser una excepción más que una regla. En la mayor parte de las ocasiones necesitaremos definir restricciones de acceso en función de máquinas individuales.

Las definiciones de netgroups específicos para determinadas máquinas constituyen el segundo método que se puede utilizar para gestionar el cambio de política del ejemplo que estamos explicando. En nuestro caso el fichero [.filename]#/etc/master.passwd# de cada máquina contiene dos líneas que comienzan por "+". La primera de ellas añade un netgroup con las cuentas que pueden acceder a esa máquina, mientras que la segunda añade el resto de cuentas con shell el resto de cuentas con shell [.filename]#/sbin/nologin#. Es una buena idea utilizar la versión "todo en mayúsculas" del nombre de máquina como el nombre del netgroup. En otras palabras, las líneas deberían ser como la siguiente:

[.programlisting]
....
+@BOXNAME:::::::::
+:::::::::/sbin/nologin
....

Una vez que hemos completado esta tarea para todas las máquinas nunca más resultará necesario modificar las versiones locales de [.filename]#/etc/master.passwd#. Los futuros cambios se pueden gestionar simplemente modificando el mapeo o asociación de NIS. A continuación se muestra un mapeo de netgroups para el escenario que se está explicando junto con algunas buenas ideas:

[.programlisting]
....
# Definimos antes que nada los grupos de usuarios
IT_EMP    (,alpha,test-domain)    (,beta,test-domain)
IT_APP    (,charlie,test-domain)  (,delta,test-domain)
DEPT1     (,echo,test-domain)     (,foxtrott,test-domain)
DEPT2     (,golf,test-domain)     (,hotel,test-domain)
DEPT3     (,india,test-domain)    (,juliet,test-domain)
ITINTERN  (,kilo,test-domain)     (,lima,test-domain)
D_INTERNS (,able,test-domain)     (,baker,test-domain)
#
# Ahora definimos unos pocos grupos basados en roles
USERS     DEPT1   DEPT2     DEPT3
BIGSRV    IT_EMP  IT_APP
SMALLSRV  IT_EMP  IT_APP    ITINTERN
USERBOX   IT_EMP  ITINTERN  USERS
#
# Y un grupo para tareas especiales
# Permitimos a echo y golf acceso a nuestra maquina-anti-virus
SECURITY  IT_EMP  (,echo,test-domain)  (,golf,test-domain)
#
# netgroups basados en maquinas
# Nuestros servidores principales
GUERRA   BIGSRV
HAMBRE   BIGSRV
# El usuario india necesita acceso a este servidor
PESTE  BIGSRV  (,india,test-domain)
#
# Este es realmente importante y necesita mas restricciones de
# acceso
MUERTE     IT_EMP
#
# La maquina-anti-virus que mencionabamos mas arriba
ONE       SECURITY
#
# Restringimos una maquina a un solo usuario
TWO       (,hotel,test-domain)
# [...otros grupos]
....

Si estamos utilizando algun tipo de base de datos para gestionar cuentas de usuario debemos ser capaces de crear la primera parte del mapeo utilizando las herramientas proporcionadas por dicho sistema de base de datos. De este modo los nuevos usuarios tendrán automáticamente derechos de acceso sobre las máquinas. 

Una última, por precaución: puede no ser siempre aconsejable utilizar netgroups basados en máquinas. Si se están desplegando, por ejemplo, un par de docenas o incluso cientos de máquinas idénticas en laboratorios de estudiantes, es mejor utilizar netgroups basados en roles en lugar de netgroups basados en máquinas individuales para mantener el tamaño de la asociación NIS dentro de unos límites razonables.

=== Conceptos importantes a tener muy en cuenta

Todavía quedan un par de cosas que tendremos que hacer de forma distinta a lo comentado hasta ahora en caso de encontrarnos dentro de un entorno de NIS.

* Cada vez que deseemos añadir un usuario a nuestro laboratorio debemos añadirlo al servidor NIS maestro _únicamente_ y es tarea fundamental del administrador _acordarse de reconstruir los mapeos NIS_. Si nos olvidamos de esto el nuevo usuario será incapaz de acceder a ninguna máquina excepto al servidor NIS. Por ejemplo, si necesitáramos añadir el nuevo usuario `jsmith` al laboratorio tendríamos que ejecutar lo siguiente:
+

[source,bash]
....
# pw useradd jsmith
# cd /var/yp
# make test-domain
....

+ 
Se puede ejecutar también `adduser jsmith` en lugar de `pw useradd jsmith`.
* _No introduzca las cuentas de administración dentro de los mapeos de NIS_. No es buena idea propagar cuentas y contraseñas de administración a máquinas donde residen usuarios que normalmente no deberían poder acceder a dichas cuentas.
* _Mantenga el servidor maestro y esclavo de NIS seguros y minimize el tiempo de interrupción del servicio_. Si alguien fuera capaz de comprometer la integridad de estas máquinas o de apagarlas los usuarios del dominio no podrían acceder a sus cuentas en ningún sistema.
+ 
Esto es la debilidad principal de cualquier sistema de administración centralizada. Si no se protegen los servidores NIS tendremos frente a nosotros a una horda de usuarios bastante enfadados.

=== Compatibilidad con NIS v1

El servicio ypserv de FreeBSD puede servir también a clientes NIS v1. La implementación de NIS de FreeBSD sólo utiliza el protocolo NIS v2 aunque otras implementaciones incluyen soporte para el protocolo v1 por razones de compatibilidad con sistemas antiguos. Los dæmones ypbind suministrados con estos sistemas tratan de establecer una asociación con un servidor NIS versión 1 aunque puede que nunca la lleguen a utilizar (y pueden continuar realizando búsquedas mediante broadcast incluso cuando reciben una respuesta de un servidor versión 2). Tenga muy presente que mientras se soportan las llamadas normales de clientes v1, la versión de ypserv actualmente suministrada no gestiona peticiones de transferencia de mapeos a través de la versión 1; en consecuencia no se puede utilizar como maestro o esclavo junto con servidores de NIS antiguos que sólo soporten el protocolo v1. Por suerte quedan muy pocos servidores de este estilo en funcionamiento hoy en día.

[[network-nis-server-is-client]]
=== Servidores NIS que actúan también como clientes NIS

Se debe tener cuidado cuando se ejecuta ypserv en un entorno multi-servidor donde las máquinas servidoras actúan también como máquinas clientes de NIS. Normalmente es una buena idea obligar a los servidores a que se asocien con ellos mismos mejor que permitirles emitir peticiones de asociación en broadcast, lo que posiblemente terminará con los servidores asociados entre sí. Se pueden producir extraños fallos si un servidor del que dependen otros deja de funcionar. Puede darse que los contadores de todos los clientes expiren e intenten asociarse a otro servidor, pero el retardo puede ser considerable y los fallos todavía podrín persistir debido a que los servidores se asocian contínuamente los unos a los otros.

Se puede obligar a una máquina a asociarse con un servidor en particular ejecutando `ypbind` con la opción `-S`. Si no se quiere ejecutar esto manualmente cada vez que se reinice el servidor NIS se puede puede añadir la siguiente línea al fichero [.filename]#/etc/rc.conf#:

[.programlisting]
....
nis_client_enable="YES"	# ejecuta tambien el soft cliente
nis_client_flags="-S NIS domain,server"
....

Consulte man:ypbind[8] para obtener más información.

=== Formatos de contraseñas

Uno de los problemas más comunes que se encuentra la gente a la hora de implantar un servicio de NIS es la compatibilidad del formato de las contraseñas. Si nuestro servidor de NIS utiliza contraseñas cifradas mediante DES sólo podrá aceptar clientes que utilicen DES. Por ejemplo, si poseemos clientes de NIS Solaris(TM) en nuestra red casi seguro que necesitaremos utilizar contraseñas cifradas mediante DES.

Para comprobar qué formato utilizan los servidores y los clientes, se puede mirar en [.filename]#/etc/login.conf#. Si la máquina se configura para utilizar cifrado de contraseñas mediante DES, entonces la clase por defecto debe poseer una entrada como la siguiente:

[.programlisting]
....
default:\
	:passwd_format=des:\
	:copyright=/etc/COPYRIGHT:\
	[Se han omitido otras entradas]
....

Otros posibles valores para característica de `passwd_format` pueden ser `blf` y `md5` (para utilizar los algoritmos Blowfish y MD5 respectivamente).

Se debe reconstruir la base de datos de acceso siempre que se modifique el fichero [.filename]#/etc/login.conf# mediante la ejecución de la siguiente orden como `root`:

[source,bash]
....
# cap_mkdb /etc/login.conf
....

[NOTE]
====
El formato de las contraseñas que ya se encuentran definidas en [.filename]#/etc/master.passwd# no se actualizará hasta que el usuario cambie su contraseña, después de que se haya reconstruido la base de datos de tipos de acceso.
====

A continuación para asegurarse de que las contraseñas se cifran con el formato seleccionado también debemos comprobar que la variable `crypt_default` dentro del fichero [.filename]#/etc/auth.conf# da preferencia al formato de contraseña elegido. Por ejemplo cuando se utilizan contraseñas cifradas con DES la entrada debe ser:

[.programlisting]
....
crypt_default	=	des blf md5
....

Si se realizan los pasos anteriores en cada una de las máquinas clientes y servidoras de nuestro entorno NIS podemos asegurar que todas utilizan el mismo formato de contraseña dentro de nuestra red. Si se presentan problemas de validación con determinados usuarios en una determinada máquina cliente se puede empezar a investigar sobre el asunto. Recuerde: si se quiere desplegar un servicio de páginas amarillas sobre un entorno de red heterogéneo probablemente se deba utilizar DES en todos los sistemas puesto que DES es el mínimo común denominador.

[[network-dhcp]]
== DHCP

=== ?Qué es DHCP?

DHCP, el Protocolo de Configuración Dinamica de Máquinas ("Dynamic Host Configuration Protocol"), especifica un método para configurar dinámicamente los parámetros de red necesarios para que un sistema pueda comunicarse efectivamente. FreeBSD utiliza la implementación de DHCP proporcionada por el Internet Software Consortium (ISC) de tal forma que toda la información relativa a la configuración de DHCP se basa en la distribución proporcionada por el ISC.

=== Contenido de esta seccións

Esta sección describe tanto los componentes de la parte servidora como los componentes de la parte cliente del sistema DHCP del ISC. El programa cliente, denominado forma parte por defecto de los sistemas FreeBSD y el programa servidor se puede instalar a partir del "port"package:net/isc-dhcp3-server[]. Las principales fuentes de información son las páginas de manual man:dhclient[8], man:dhcp-options[5] y man:dhclient.conf[5] junto con las referencias que se muestran a continuación en esta misma sección.

=== Cómo funciona

Cuando el cliente de DHCP, `dhclient`, se ejecuta en una máquina cliente, valga la redundancia, comienza a enviar peticiones "broadcast" solicitando información de configuración. Por defecto estas peticiones se realizan contra el puerto UDP 68. El servidor responde a través del puerto UDP 67 proporcionando al cliente una dirección IP junto con otros parámetros relevantes para el correcto funcionamiento del sistema en la red, tales como la máscara de red, el " router" por defecto y los servidores de DNS. Toda esta información se "presta" y es válida sólo durante un determinado período de tiempo (configurado por el administrador del servidor de DHCP). De esta forma direcciones IP asignadas a clientes que ya no se encuentran conectados a la red pueden ser reutilizadas al pasar determinado periodo de tiempo.

Los clientes de DHCP pueden obtener una gran cantidad de información del servidor. Se puede encontrar una lista completa en man:dhcp-options[5].

=== Integración dentro de los sistemas FreeBSD

FreeBSD se integra totalmente con el cliente DHCP del ISC, `dhclient`. Este soporte se proporciona tanto en el proceso de instalación como en la instalación por defecto del sistema base obviando la necesidad de poseer un conocimiento detallado de aspectos relacionados con la configuración de redes siempre y cuando se disponga de servicio de DHCP en la red dada. `dhclient` se incluye en todas las distribuciones de FreeBSD desde la versión 3.2.

sysinstall soporta DHCP. Cuando se configura la interfaz de red la primera pregunta es: " ?Quiere intentar configurar el interfaz mediante DHCP?". Si se responde afirmativamente se ejecutará `dhclient` y si tiene éxito se procede con los siguientes pasos de configuración rellenandose automáticamente las variables de arranque necesarias para completar la configuración de la red.

Existen dos cosas que se deben realizar de tal forma que nuestro sistema utilice la configuración de red mediante DHCP al arrancar:

* Asegurarse de que el dispositivo [.filename]#bpf# se encuentra compilado en el kernel. Para ello basta añadir `device bpf` (`pseudo-device bpf` en los sistemas FreeBSD 4.X) al fichero de configuración del kernel y recompilarlo e instalarlo. Para más información sobre la construcción de núcleos consulte crossref:kernelconfig[kernelconfig,Configuración del kernel de FreeBSD].
+ 
El dispositivo [.filename]#bpf# se encuentra activado por defecto dentro del fichero de configuración del núcleo ([.filename]#GENERIC# que encontrará en su sistema FreeBSD de forma que si no se está utilizando un fichero de configuración del núcleo específico (hecho a medida y/o por usted) no es necesario crear uno nuevo y se puede utilizar directamente [.filename]#GENERIC#.
+
[NOTE]
====
Para aquellas personas especialmente preocupadas por la seguridad debemos advertir de que el dispositivo [.filename]#bpf# es el dispositivo que las aplicaciones de captura de paquetes utilizan para acceder a los mismos (aunque dichas aplicaciones deben ser ejecutadas como `root`). DHCP _requiere_ la presencia de [.filename]#bpf# pero si la seguridad del sistema es más importante que la configuración automática de la red no se recomienda instalar [.filename]#bpf# en el núcleo.
====

* Editar el fichero [.filename]#/etc/rc.conf# para para incluir lo siguiente:
+
[.programlisting]
....
ifconfig_fxp0="DHCP"
....
+
[NOTE]
====
Asegúrese de sustituir `fxp0` con el nombre de interfaz que desea que se configure dinámicamente, como se describe en crossref:config[config-network-setup,Configuración de Tarjetas de Red].
====
+ 
Si se utiliza una ubicación distinta para `dhclient` o si se desea añadir opciones adicionales a `dhclient` se puede incluir, adaptándolo a las condiciones particulares de cada usuario, lo siguiente:
+
[.programlisting]
....
dhcp_program="/sbin/dhclient"
dhcp_flags=""
....

El servidor de DHCP (dhcpd) forma parte del "port"package:net/isc-dhcp3-server[]. Este " port" también contiene la documentación de ISC DHCP.

=== Ficheros

* [.filename]#/etc/dhclient.conf#
+ 
`dhclient` necesita un fichero de configuración denominado [.filename]#/etc/dhclient.conf#. Normalmente este fichero sólo contiene comentarios de forma que las opciones que se definen por defecto son razonablemente inocuas. Este fichero de configuración se describe en la página de manual de man:dhclient.conf[5].
* [.filename]#/sbin/dhclient#
+ 
`dhclient` se encuentra enlazado de forma estática y reside en [.filename]#/sbin#. La página de manual de man:dhclient[8] proporciona más información sobre la orden `dhclient`.
* [.filename]#/sbin/dhclient-script#
+ 
`dhclient-script` es el " script" de configuración del cliente de DHCP específico de FreeBSD. Tiene todos los detalles en man:dhclient-script[8] pero no necesita hacer ninguna modificación en él para que todo funcione correctamente.
* [.filename]#/var/db/dhclient.leases#
+ 
El cliente de DHCP mantiene una base de datos de préstamos en este fichero que se escribe de forma semejante a un "log". En man:dhclient.leases[5] puede consultar una explicación ligeramente más detallada.

=== Lecturas recomendadas

El protocolo DHCP se describe completamente en http://www.freesoft.org/CIE/RFC/2131/[RFC 2131]. También tiene más información en http://www.dhcp.org/[dhcp.org].

[[network-dhcp-server]]
=== Instalación y configuración de un servidor de DHCP

==== Qué temas se tratan en esta sección

Esta sección proporciona información sobre cómo configurar un sistema FreeBSD de forma que actúe como un servidor de DHCP utilizando la implementación proporcionada por el Internet Software Consortium (ISC).

La parte servidora del paquete proporcionado por el ISC no se instala por defecto en los sistemas FreeBSD pero se puede intalar como "port" desde package:net/isc-dhcp3-server[]. Consulte crossref:ports[ports,Instalación de aplicaciones: «packages» y ports] si necesita saber más sobre la Colección de "ports".

==== Instalación del servidor DHCP

Para configurar un sistema FreeBSD como servidor de DHCP debe asegurarse de que el dispositivo man:bpf[4] está compilado dentro del núcleo. Para ello basta añadir `device bpf` (`pseudo-device bpf` en FreeBSD 4.X) al fichero de configuración del núcleo y reconstruir el mismo. Si necesita saber más sobre el proceso de compilar e instalar el núcleo consulte crossref:kernelconfig[kernelconfig,Configuración del kernel de FreeBSD].

El dispositivo [.filename]#bpf# ya se encuentra activado en el fichero de configuración [.filename]#GENERIC# del núcleo que se facilita con FreeBSD de tal forma que no resulta imprescindible crear un núcleo a medida para ejecutar DHCP.

[NOTE]
====
Para aquellas personas especialmente preocupadas por la seguridad debemos advertir de que el dispositivo [.filename]#bpf# es el dispositivo que las aplicaciones de captura de paquetes utilizan para acceder a los mismos (aunque dichas aplicaciones deben ser ejecutadas como `root`). DHCP _requiere_ la presencia de [.filename]#bpf# pero si la seguridad del sistema es más importante que la configuración automática de la red no se recomienda instalar [.filename]#bpf# en el núcleo.
====

El siguiente paso consiste en editar el fichero de ejemplo [.filename]#dhcpd.conf# que se crea al instalar el "port"package:net/isc-dhcp3-server[]. Por defecto el fichero se llama [.filename]#/usr/local/etc/dhcpd.conf.sample#, así que se debe copiar este fichero a [.filename]#/usr/local/etc/dhcpd.conf# y a continuación realizar todos los cambios sobre este último.

==== Configuración del servidor de DHCP

El fichero [.filename]#dhcpd.conf# se compone de un conjunto de declaraciones que hacen referencia a máquinas y a subredes. Esto se entenderá mejor mediante el siguiente ejemplo:

[.programlisting]
....
option domain-name "ejemplo.com";<.>
option domain-name-servers 192.168.4.100;<.>
option subnet-mask 255.255.255.0;<.>

default-lease-time 3600;<.>
max-lease-time 86400;<.>
ddns-update-style none;<.>

subnet 192.168.4.0 netmask 255.255.255.0 {
  range 192.168.4.129 192.168.4.254;<.>
  option routers 192.168.4.1;<.>
}

host mailhost {
  hardware ethernet 02:03:04:05:06:07;<.>
  fixed-address mailhost.ejemplo.com;<.>
}
....

<.> Esta opción especifica el dominio que se proporciona a los clientes y que dichos clientes utilizan como dominio de búsqueda por defecto. Consulte man:resolv.conf[5] si necesita más información sobre qué significa el dominio de búsqueda.

<.> Esta opción especifica la lista de servidores de DNS (seperados por comas) que deben utilizar los clientes.

<.> La máscara de red que se proporciona a los clientes.

<.> Un cliente puede solicitar un determinado tiempo de vida para el préstamo. En caso contrario el servidor asigna un tiempo de vida por defecto mediante este valor (expresado en segundos).

<.> Este es el máximo tiempo que el servidor puede utilizar para realizar préstamos a los clientes. Si un cliente solicita un tiempo mayor como máximo se responderá con el valor aquí configurado, ignorándose la petición de tiempo del cliente.

<.> Esta opción especifica si el servidor de DHCP debe intentar actualizar el servidor de DNS cuando se acepta o se libera un préstamo. En la implementación proporcionada por el ISC esta opción es _obligatoria_.

<.> Esto indica qué direcciones IP se pueden utilizar para ser prestadas a los clientes que las soliciten. Las direcciones IP pertenecientes a este rango, incluyendo los extremos, se pueden entregar a los clientes.

<.> Declara cúal es la pasarela por defecto que se proporcionará a los clientes.

<.> Especifica la dirección MAC de una máquina, de tal forma que el servidor de DHCP pueda identificar a la máquina cuando realice una petición.

<.> Especifica que la máquina cliente debería obtener siempre la misma dirección IP. Recuerde que se puede utilizar un nombre de máquina para esto ya que el servidor de DHCP resolverá el nombre por sí mismo antes de devolver la información al cliente.

Una vez que se ha acabado de configurar el fichero [.filename]#dhcpd.conf# se puede proceder con la ejecución del servidor mediante la siguiente orden:

[source,bash]
....
# /usr/local/etc/rc.d/isc-dhcpd.sh start
....

Si posteriormente se necesitan realizar cambios en la configuración anterior tenga en cuenta que el envío de la señál `SIGHUP` a la aplicación dhcpd _no_ provoca que se lea de nuevo la configuración como suele ocurrir en la mayoría de los dæmones. Tendrá que enviar la señal `SIGTERM` para parar el proceso y posteriormente relanzar el dæmon utilizando la orden anterior.

==== Ficheros

* [.filename]#/usr/local/sbin/dhcpd#
+ 
dhcpd se encuentra enlazado de forma estática y reside en el directorio [.filename]#/usr/local/sbin#. La página de manual man:dhcpd[8] que se instala con el "port" le proporcionará más información sobre dhcpd.
* [.filename]#/usr/local/etc/dhcpd.conf#
+ 
dhcpd necesita un fichero de configuración, [.filename]#/usr/local/etc/dhcpd.conf#. Este fichero contiene toda la información relevante que se quiere proporcionar a los clientes que la soliciten, junto con información relacionada con el funcionamiento del servidor. Este fichero de configuración se describe en la página del manual man:dhcpd.conf[5] que instala el " port".
* [.filename]#/var/db/dhcpd.leases#
+ 
El servidor de DHCP mantiene una base de datos de préstamos o alquileres dentro de este fichero, que presenta un formato de fichero de "log". La página del manual man:dhcpd.leases[5] que se instala con el "port" proporciona una descripción ligeramente más larga.
* [.filename]#/usr/local/sbin/dhcrelay#
+ 
dhcrelay se utiliza en entornos de red avanzados donde un servidor de DHCP reenvía una petición de un cliente hacia otro servidor de DHCP que se encuentra localizado en otra subred. Si se necesita esta funcionalidad se debe instalar el "port"package:net/isc-dhcp3-server[]. La página del manual man:dhcrelay[8] proporcionada por el "port" contiene más detalles sobre esto.

[[network-dns]]
== DNS

=== Resumen

FreeBSD utiliza por defecto una versión de BIND (Berkeley Internet Name Domain) que proporciona la implementación más común del protocolo de DNS. DNS es el protocolo a través del cual los nombres de máquinas se asocian con direcciones IP y viceversa. Por ejemplo una consulta preguntando por `www.FreeBSD.org` recibe una respuesta con la dirección IP del servidor web del Proyecto FreeBSD, mientras que una pregunta sobre `ftp.FreeBSD.org` recibe como respuesta la dirección IP correspondiente al servidor de FTP. El proceso inverso sucede de una forma similar. Una pregunta relativa a una determinada dirección IP se resuelve al nombre de la máquina que la posee. No se necesita ejecutar un servidor de DNS para poder realizar consultas y búsquedas de DNS.

El DNS se coordina de forma distribuida a través de Internet utilizando un sistema en cierta forma complejo de servidores de nombres raíz autorizados y mediante otros servidores de nombres de menor escala que se encargan de replicar la información de dominios individuales con el objetivo de mejorar los tiempos de respuesta de búsquedas reiteradas de la misma información.

Este documento hace referencia a la versión estable BIND 8.X. BIND 9.X se puede instalar a través del "port" package:net/bind9[].

El protocolo de DNS se encuentra definido en la RFC1034 y la RFC1035. 

El http://www.isc.org/[Internet Software Consortium (www.isc.org)] se encarga de de mantener el software de BIND. 

=== Terminología

Para comprender este documento se deben definir los siguientes términos:

[.informaltable]
[cols="1,1", frame="none", options="header"]
|===
| Término
| Definición

|DNS directo (Forward DNS)
|Asociación de nombres de máquinas con direcciones IP

|Origen
|Se refiere al dominio cubierto por un determinado fichero de zona

|named, BIND, servidor de nombres (name server)
|Nombres típicos que hacen referencia al paquete servidor de nombres de BIND de FreeBSD

| Resolver
|Un proceso del sistema que utilizan las aplicaciones para hacer preguntas al servidor de nombres.

| DNS inverso (Reverse DNS)
|Lo contrario de lo que realiza el DNS directo; asocia direcciones IP con nombres de máquinas

| Zona Raíz
|El comienzo de la jerarquía de zonas de Internet. Todas las zonas surgen a partir de una zona raíz de forma similar a como todos los directorios de un sistema de ficheros se encuentran a partir de un directorio raíz inicial.

|Zona
|Un dominio individual, subdominio o porción del DNS que se encuentra administrado por la misma autoridad.
|===

Ejemplos de zonas: 

* `.` es la zona raíz
* `org.` es una zona localizada bajo la zona raíz
* `ejemplo.org` es una zona localizada bajo la zona `org.`
* `foo.ejemplo.org.` es un subdominio o una zona ubicada bajo la zona `ejemplo.org.`
* `1.2.3.in-addr.arpa` es una zona que referencia a a todas las direcciones IP que se encuentran dentro del espacio de direcciones de 3.2.1.*.

Como se puede observar la parte más específica de una máquina aparece más a la izquierda. Por ejemplo `ejemplo.org` es más específico que `org.` y del mismo modo `org.` es más específico que la zona raíz. El formato de cada parte del nombre de la máquina es muy similar al formato de un sistema de ficheros: el directorio [.filename]#/dev# se encuentra dentro del directorio raíz, y así sucesivamente.

=== Razones para ejecutar un servidor de nombres

Los servidores de nombres normalmente son de dos tipos: autoritarios y de cache.

Se necesita un servidor de nombres autoritario cuando:

* uno quiere proporcionar información de DNS al resto del mundo respondiendo con información autoritaria a las consultas recibidas.
* un dominio, por ejemplo `ejemplo.org`, está registrado y se necesita añadir nombres de máquinas bajo dicho dominio.
* un bloque de direcciones IP necesita entradas de DNS inversas (de IP a nombre de máquina).
* un servidor de nombres de "backup", llamado esclavo, debe responder a consultas cuando el servidor primario se encuentre caído o inaccesible.

Se necesita un servidor caché cuando:

* un servidor de DNS local puede responder más rápidamente de lo que se haría si se tuviera que preguntar al servidor de nombres externo.
* se desea reducir el tráfico global de red (se ha llegado a comprobar que el tráfico de DNS supone un 5% o más del total del tráfico que circula por Internet).

Cuando se pregunta por `www.FreeBSD.org` el " resolver" normalmente pregunta al servidor de nombres del ISP de nivel superior y se encarga de recibir la respuesta. Si se utiliza un servidor de DNS caché local la pregunta sólo se dirige una única vez hacia el exterior. Dicha pregunta la realiza el servidor caché. Posteriores consultas sobre el mismo nombres son respondidas directamente por este servidor.

=== Cómo funciona

En FreeBSD el dæmon de BIND se denomina named por razones obvias.

[.informaltable]
[cols="1,1", frame="none", options="header"]
|===
| Fichero
| Descripción

|named
|El dæmon de BIND

|`ndc`
|El programa de control del dæmon

|[.filename]#/etc/namedb#
|El directorio donde se almacena la información de las zonas de BIND

|[.filename]#/etc/namedb/named.conf#
|El archivo de configuración del dæmon
|===

Los ficheros de zonas se encuentran normalmente bajo el directorio [.filename]#/etc/namedb# y contienen la información que proporciona el servidor de nombres al resto de máquinas de Internet. 

=== Ejecución de BIND

Debido a que BIND se instala por defecto la configuración resulta ser bastante sencilla. 

Para asegurarnos de que el dæmon se ejecuta al inicio del sistema se deben añadir las siguientes modificaciones en [.filename]#/etc/rc.conf#: 

[.programlisting]
....
named_enable="YES"
....

Para arrancar el dæmon de forma manual (una vez configurado)

[source,bash]
....
# ndc start
....

=== Ficheros de configuración

==== Uso de `make-localhost`

Asegúrese de hacer los siguiente 

[source,bash]
....
# cd /etc/namedb
# sh make-localhost
....

para que se cree el archivo de zona inversa [.filename]#/etc/namedb/localhost.rev# de forma apropiada. 

==== [.filename]#/etc/namedb/named.conf#

[.programlisting]
....
// $FreeBSD$
//
// Consulte la pÃ¡gina man de named(8) para mÃ¡s detalles.  tiene
// alguna vez la necesidad de configurar un servidor primario
// asegÃºree de que entiende a la perfecciÃ³n los detalles peliagudos
// del funcionamiento del DNS.  Si hay errores, incluso triviales,
// puede sufrir pÃ©rdidas de conectividad ogenerar cantidades ingentes
// de trÃ¡fico inÃºtil hacia o desde Interne

options {
        directory "/etc/namedb";

// AdemÃ¡s de con la lÃ¡usula "forwarders" puedeobligar a su servidor
// de nombres a que nunca lance bÃºsquedas por su cuenta sino que
// se las pida a sus "forwarders". Esto se hace del siguiente modo:
//
//      forward only;

// Si su proveedor de acceso tiene a su alcance un servidor DNS
// escriba aquÃ­ su direcciÃ³n IP y descomente la lÃ­neaPodrÃ¡ usar
// su cachÃ© y por lo tanto reducir el trÃ¡fico DNS de Internet.
//

/*
        forwarders {
                127.0.0.1;
        };
*/
....

Tal y como se dice en los comentarios del ejemplo para beneficiarnos de la caché se puede activar `forwarders`. En circunstancias normales un servidor de nombres busca de forma recursiva a través de Internet tratando de localizar un servidor de nombres que sea capaz de responder una determinada pregunta. Si se activa esta opción por defecto se pasa a preguntar primero al servidor de nombres especificado (servidor o servidores) pudiendo aprovecharse de la información de caché que dicho servidor tuviera disponible. Si el servidor de nivel superior al nuestro se encuentra congestionado puede merecer la pena la activación de esta característica de "redirección" ya que se puede disminuir la carga de tráfico que dicho servidor tiene que soportar.

[WARNING]
====

La dirección IP `127.0.0.1` _no_ funciona aí. Se debe cambiar esta dirección IP por un servidor de nombres válido.
====

[.programlisting]
....
        /*
	 * Si hay un cortafuegos entre usted y los servidores de
	 * nombres que quiere consultar tendrÃ¡ que descoentar la
	 * siguiente directiva, "query-source".  Las versiones
	 * anteriores de BIND siempre hacÃ­an sus consultas a travÃ©s
	 * del puerto 53 pero BIND 8.1 utiliza por defecto un puerto no
	 * privilegiado.
         */
        // query-source address * port 53;

        /*
	 * Si lo va a ejecutar en un "cajÃ³n de arena" (o "sandbox")
	 * tendrÃ¡ que declarar una uicaciÃ³n diferente para el
	 * fichero de volcado de named.
         */
        // dump-file "s/named_dump.db";
};

// Nota: lo siguiente serÃ¡ incluÃ­do en futuras versiones.
/*
host { any; } {
        topology {
                127.0.0.0/8;
        };
};
*/

// La configuraciÃ³n de secundarios se explica de modo secillo a
// partir de aquÃ­.
//
// Si activa un servidor de nombres local no olvide incluÃ­r
// 127.0.0.1 en su /etc/resolv.conf para que sea ese servidor el
// primero al que se consulte.
// AsegÃºrese tambiÃ©n de activarlo en /etc/rc.con

zone "." {
        type hint;
        file "named.root";
};

zone "0.0.127.IN-ADDR.ARPA" {
        type master;
        file "localhost.rev";
};

zone
"0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.IP6.INT" {
        type master;
        file "localhost.rev";
};

// Nota: No use las direcciones IP que se muestran aquÃ­, son falsas
// y sÃ³lo se usancomo demostraciÃ³n y para una mejor comprensiÃ³n.
//
// Ejemplo de entradas en la configuraciÃ³n de secundarios.  Puede ser
// conveniente convertirse en secundario al menos del dominio en cuya
// zona estÃ¡ su dominio.  Cnsulte con su administrador de red para
// que le facilite la direcciÃ³n IP del servidor primario.
//
// No olvide incluÃ­r la zona del bucle inverso (IN-ADDR.ARPA).  (Son
// los primeros bytes de la direcciÃ³n IP correspondiente, en orden
// inverso, con ".IN-ADDR.ARPA" al final.)
//
// Antes de configurar una zona primara asegÃºresede haber comprendido
// completamente cÃ³mo funcionan DNS y BIND.  Hay errores que no son
// visibles fÃ¡cilmente.  La configuraciÃ³n de un secundario es, por
// el contrario, muchÃ­simo mÃ¡s sencilla.
//
// Nota: No se limite a copiar los ejemplos de mÃ¡s arriba. :-)
// Utilice nombres y direcciones reales.
//
// ADVERTENCIA: FreeBSD ejecuta bind en un sandbok (observe los
// parÃ¡meros de named (named_flags) en rc.conf).  El directorio que
// contiene las zonas secundarias debe tener permisos de escritura
// para bind.  Le sugerimos la siguiente secuencia de órdenes:
//
//      mkdir /etc/namedb/s
//      chown bind:bind /etc/namedb/s
//      chmod 750 /etc/namedb/s
....

Si quiere más información sobre cómo ejecutar BIND en un "sandbox" consulte <<network-named-sandbox,Ejecución de named en un sandbox>>. 

[.programlisting]
....
/*
zone "ejemplo.com" {
        type slave;
        file "s/ejemplo.com.bak";
        masters {
                192.168.1.1;
        };
};

zone "0.168.192.in-addr.arpa" {
        type slave;
        file "s/0.168.192.in-addr.arpa.bak";
        masters {
                192.168.1.1;
        };
};
*/
....

Dentro del fichero [.filename]#named.conf# se muestran ejemplos de entradas de esclavo tanto para las zonas directas como para las inversas.

Para cada nueva zona administrada se debe crear una entrada de zona dentro del fichero [.filename]#named.conf#

Por ejemplo la entrada de zona más simple para el dominio `ejemplo.org` puede ser algo como esto:

[.programlisting]
....
zone "ejemplo.org" {
	type master;
	file "ejemplo.org";
};
....

Esta zona es una zona maestra ( observe la línea de `type`, y mantiene la información de la zona en [.filename]#/etc/namedb/ejemplo.org# tal como se indica en la línea de `file`.

[.programlisting]
....
zone "ejemplo.org" {
	type slave;
	file "ejemplo.org";
};
....

En el caso del esclavo la información de la zona se transmite desde el servidor de nombres maestro y se almacena en el fichero especificado. Cuando el servidor maestro " muere" o no puede ser alcanzado el servidor de nombres esclavo puede responder a las peticiones debido a que posee la información de la zona.

==== Ficheros de zona

A continuación se muestra un fichero de una zona maestra para el dominio `ejemplo.org`, que se encuentra ubicado en [.filename]#/etc/namedb/ejemplo.org#:

[.programlisting]
....
$TTL 3600

example.org. IN SOA ns1.ejemplo.org. admin.ejemplo.org. (
                        5               ; Serial
                        10800           ; Refresh
                        3600            ; Retry
                        604800          ; Expire
                        86400 )         ; Minimum TTL

; DNS Servers
@       IN NS           ns1.ejemplo.org.
@       IN NS           ns2.ejemplo.org.

; Machine Names
localhost       IN A    127.0.0.1
ns1             IN A    3.2.1.2
ns2             IN A    3.2.1.3
mail            IN A    3.2.1.10
@               IN A    3.2.1.30

; Aliases
www             IN CNAME        @

; MX Record
@               IN MX   10      mail.ejemplo.org.
....

Tenga muy en cuenta que todo nombre de máquina que termina en "." es tratado como si fuera un nombre de máquina completo mientras que cualquier otro nombre sin el "." final se trata como una referencia relativa al dominio de origen de la zona. Por ejemplo `www` se traduce a `www + origen`. En nuestro fichero de zona ficticio nuestro origen es `ejemplo.org` de forma que `www` se convierte en `www.ejemplo.org`

El formato de un fichero de zona es el siguiente: 

[.programlisting]
....
nombrederegistro IN tipodeentrada valor
....

Los registros de DNS que más se utilizan son: 

SOA::
Comienzo de Zona con Autoridad (Start Of zone Authority)

NS::
Un servidor de nombres con autoridad para una una determinada zona

A::
Una dirección IP de una máquina

CNAME::
El nombre canónico de una máquina para definir un alias

MX::
mail exchanger

PTR::
Un puntero a un nombre de dominio (utilizados para definir el DNS inverso) 

[.programlisting]
....

ejemplo.org. IN SOA ns1.ejemplo.org. admin.ejemplo.org. (
                        5               ; Serial
                        10800           ; Refresh after 3 hours
                        3600            ; Retry after 1 hour
                        604800          ; Expire after 1 week
                        86400 )         ; Minimum TTL of 1 day
....

`ejemplo.org.`::
el nomre de dominio, también el origen para el fichero de zona

`ns1.ejemplo.org.`::
el servidor de nombres primario/autoritario para esta zona

`admin.ejemplo.org.`::
la persona responsable de esta zona; observe que la dirección de correo electrónico aparece con la @ sustituida por un punto. (mailto:admin@ejemplo.org[admin@ejemplo.org] se escribe `admin.ejemplo.org`)

`5`::
el número de serie del fichero. Este número se debe incrementar cada vez que se modifique el fichero de zona. Muchos administradores prefieren un formato expresado del siguiente modo `aaaammddss`. 2001041002 significa (según dicho formato) que el fichero se modificó por última vez el 04/10/2001 y se indica con los dos últimos dígitos (02) que es la segunda vez en el día que se ha modificado el fichero. El número de serie es importante ya que para avisar a los servidores de nombres esclavos de que se ha actualizado la zona.

[.programlisting]
....

@       IN NS           ns1.ejemplo.org.
....

Esta es una entrada de tipo `NS`. Cada servidor de nombres que contesta de forma autoritaria a las consultas de un determinado dominio debe tener una de estas entradas. El caracter `@` se sustituye por `ejemplo.org.`, es decir, se sustituye por el origen. 

[.programlisting]
....

localhost       IN A    127.0.0.1
ns1             IN A    3.2.1.2
ns2             IN A    3.2.1.3
mail            IN A    3.2.1.10
@               IN A    3.2.1.30
....

El registro de tipo A hace referencia a nombres de máquinas . Como puede verse más arriba `ns1.ejemplo.org` se resuelve a `3.2.1.2`. Vemos que se utiliza otra vez el origen `@`, que significa que `ejemplo.org` se resuelve a `3.2.1.30`. 

[.programlisting]
....

www             IN CNAME        @
....

Los registros de nombres canónicos se utilizan normalmente como alias de máquinas. En el ejemplo `www` es un alias de `ejemplo.org` (`3.2.1.30`). ``CNAME``s se puede utilizar para proporcionar alias de nombres de máquinas, o también para proporcionar un mecanismo de vuelta cíclica ("round robin") de un nombre de máquina mapeado a un determinado conjunto de máquinas intercambiables.

[.programlisting]
....

@               IN MX   10      mail.ejemplo.org.
....

El registro `MX` indica qué servidores de correo se encargan de recibir correos para esta zona. `mail.example.org` es el nombre del servidor de correo y 10 significa la prioridad de dicho servidor. 

Se pueden especificar varios servidores de correo con prioridades de, por ejemplo,3, 2 y 1. Un servidor de correo que intenta entregar correo para el dominio `ejemplo.org` primero intentará contactar con el servidor especificado en el registro MX de mayor prioridad, después con el siguiente y así sucesivamente hasta que lo logre entregar. 

Para los ficheros de zona de in-addr.arpa (DNS inverso) se utiliza el mismo formato excepto que se especifican registros `PTR` en lugar de registros `A` o `CNAME`. 

[.programlisting]
....
$TTL 3600

1.2.3.in-addr.arpa. IN SOA ns1.ejemplo.org. admin.ejemplo.org. (
                        5               ; Serial
                        10800           ; Refresh
                        3600            ; Retry
                        604800          ; Expire
                        3600 )          ; Minimum

@       IN NS   ns1.ejemplo.org.
@       IN NS   ns2.ejemplo.org.

2       IN PTR  ns1.ejemplo.org.
3       IN PTR  ns2.ejemplo.org.
10      IN PTR  mail.ejemplo.org.
30      IN PTR  ejemplo.org.
....

Este fichero proporciona las asociaciones de direcciones IP con nombres de máquinas adecuadas para nuestro dominio ficticio. 

=== Servidor de nombres de cache

Un servidor de nombres de tipo caché es un servidor de nombres que no es autoritario para ninguna zona. Simplemente realiza consultas por sí mismo y recuerda las respuestas para futuros usos. Para configura uno de estos servidores se configura el servidor de la forma habitual pero se omite la inclusión de zonas. 

[[network-named-sandbox]]
=== Ejecución de named en una " Sandbox"

Para obtener una mayor seguridad se puede ejecutar man:named[8] como un usuario sin privilegios y configurarlo mediante man:chroot[8] dentro del directorio especificado como el directorio del "sandbox". Esto hace que cualquier cosa que se encuentre fuera de dicho directorio resulte inaccesible para el dæmon named. En caso de que se comprometiera la seguridad de named esta restricción puede ayudar a limitar el daño sufrido. FreeBSD dispone por defecto de un usuario y un grupo destinado a este uso: `bind`.

[NOTE]
====
Hay quien recomienda que en lugar de configurar named con `chroot` es mejor configurarlo dentro de man:jail[8]. En esta sección no se va a explicar esa alternativa.
====

Debido a que named no va a poder acceder a nada que se encuentre fuera del directorio " sandbox" (y esto incluye cosas tales como bibliotecas compartidas, "sockets" de "log", etc) se debe efectuar una serie de cambios para que  named pueda funcionar con normalidad. En la siguiente lista se supone que la ruta del "sandbox" es [.filename]#/etc/namedb# y que no se ha modificado anteriormente dicho directorio. Por favor, ejecute los pasos que se muestran a continuación:

* Cree todos los directorios que named espera tener a su disposición:
+
[source,bash]
....
# cd /etc/namedb
# mkdir -p bin dev etc var/tmp var/run master slave
# chown bind:bind slave var/* <.>
....
<.> named sólamente necesita escribir en estos directorios así que eso es todo lo que debemos crear.

* Reorganizar y crear los archivos de configuración de zona básicos:
+
[source,bash]
....
# cp /etc/localtime etc <.>
# mv named.conf etc && ln -sf etc/named.conf
# mv named.root master

# sh make-localhost && mv localhost.rev localhost-v6.rev master
# cat > master/named.localhost
$ORIGIN localhost.
$TTL 6h
@	IN	SOA	localhost. postmaster.localhost. (
			1	; serial
			3600	; refresh
			1800	; retry
			604800	; expiration
			3600 )	; minimum
	IN	NS	localhost.
	IN	A		127.0.0.1
^D
....
+
<.> Esto permite que named pueda escribir al archivo de log la hora correcta a través del man:syslogd[8]

* Si está usando una versión de FreeBSD anterior a 4.9-RELEASE se debe construir una copia estáticamente enlazada de named-xfer y copiarla dentro del directorio del "sandbox":
+
[source,bash]
....
# cd /usr/src/lib/libisc
# make cleandir && make cleandir && make depend && make all
# cd /usr/src/lib/libbind
# make cleandir && make cleandir && make depend && make all
# cd /usr/src/libexec/named-xfer
# make cleandir && make cleandir && make depend && make NOSHARED=yes all
# cp named-xfer /etc/namedb/bin && chmod 555 /etc/namedb/bin/named-xfer <.>
....
+ 

<.> Despueés de instalar la versión estática de `named-xfer` se deben realizar algunas tareas de limpieza para evitar dejar copias de bibliotecas o de programas en nuestros ficheros de fuentes:
+
[source,bash]
....
# cd /usr/src/lib/libisc
# make cleandir
# cd /usr/src/lib/libbind
# make cleandir
# cd /usr/src/libexec/named-xfer
# make cleandir
....
+
En algunas ocasiones este paso puede fallar. Si es su caso ejecute lo siguiente:y borre su directorio [.filename]#/usr/obj#:Esto limpia cualquier "impureza" del árbol de fuentes y si se repiten los pasos anteriores todo debería funcionar.
+ 
Si se está usando FreeBSD version 4.9-RELEASE o posterior el ejecutable de `named-xfer` del directorio [.filename]#/usr/libexec# ya se encuentra enlazado estáticamente y se puede utilizar man:cp[1] para copiarlo directamente en nuestro "sandbox".
* Cree el fichero [.filename]#dev/null# de tal forma que named pueda verlo y pueda escribir sobre él:
+
[source,bash]
....
# cd /etc/namedb/dev && mknod null c 2 2
# chmod 666 null
....

* Enlace simbólicamente [.filename]#/var/run/ndc# con [.filename]#/etc/namedb/var/run/ndc#:
+
[source,bash]
....
# ln -sf /etc/namedb/var/run/ndc /var/run/ndc
....
+
[NOTE]
====
Esto simplemente evita el tener que especificar la opción `-c` de man:ndc[8] cada vez que se ejecute. Dado que los contenidos de /var/run se borran al inicio del sistema, si se piensa que esto puede resultar útil, se puede añadir esta orden al " crontab" del usuario root utilizando la opción `@reboot`. Consulte man:crontab[5] para saber más información sobre esto.
====

* Configure man:syslogd[8] para que cree un "socket" de [.filename]#log# adicional de tal forma que named pueda escribir sobre él. Añada `-l /etc/namedb/dev/log` a la variable `syslogd_flags` dentro del fichero [.filename]#/etc/rc.conf#.
* Reorganice la ejecución de las aplicaciones named y `chroot` para que se ejecuten dentro del "sandbox" añadiendo lo siguiente al fichero [.filename]#/etc/rc.conf#:
+
[.programlisting]
....
named_enable="YES"
named_flags="-u bind -g bind -t /etc/namedb /etc/named.conf"
....
+
[NOTE]
====
Recuerde que el fichero de configuración _/etc/named.conf_ tiene una ruta completa _que comienza en el directorio del "sandbox"_; por ejemplo, en la línea superior el fichero que aparece es en realidad [.filename]#/etc/namedb/etc/named.conf#.
====

El siguiente paso consiste en editar el fichero [.filename]#/etc/namedb/etc/named.conf# de tal forma que named pueda conocer qué zonas cargar y donde encontrarlas en disco. A continuación se muestra un ejemplo comentado (cualquier cosa que no se comenta en el ejemplo es porque resulta igual que la configuración del servidor de DNS del caso normal):

[.programlisting]
....
options {
        directory "/";<.>
        named-xfer "/bin/named-xfer";<.>
        version "";		// No muestra la versiÃn de BIND
        query-source address * port 53;
};
// ndc control socket
controls {
        unix "/var/run/ndc" perm 0600 owner 0 group 0;
};
// A partir de aquÃ­van las zonas:
zone "localhost" IN {
        type master;
        file "master/named.localhost";<.>
        allow-transfer { localhost; };
        notify no;
};
zone "0.0.127.in-addr.arpa" IN {
        type master;
        file "master/localhost.rev";
        allow-transfer { localhost; };
        notify no;
};
zone "0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.int" {
	type master;
	file "master/localhost-v6.rev";
	allow-transfer { localhost; };
	notify no;
};
zone "." IN {
        type hint;
        file "master/named.root";
};
zone "private.example.net" in {
        type master;
        file "master/private.example.net.db";
	allow-transfer { 192.168.10.0/24; };
};
zone "10.168.192.in-addr.arpa" in {
        type slave;
        masters { 192.168.10.2; };
        file "slave/192.168.10.db";<.>
};
....

<.> La línea que contiene `directory` se especifica como [.filename]#/#, ya que todos los ficheros que named necesita se encuentran dentro de este directorio (recuerde que esto es equivalente al fichero [.filename]#/etc/namedb# de un usuario "normal".

<.> Especifica la ruta completa para el binario `named-xfer` binary (desde el marco de referencia de named). Esto resulta necesario ya que por defecto named se compila de tal forma que trata de localizar `named-xfer` dentro de [.filename]#/usr/libexec#.

<.> Especifica el nombre del fichero (relativo a la línea (relativo a la línea ) `directory` anterior) donde named puede encontrar el fichero de zona para esta zona.

<.> Especifica el nombre del fichero (relativo a la líena `directory` anterior) donde named debería escribir una copia del archivo de zona para esta zona después de recuperarla exitosamente desde el servidor maestro. Este es el motivo por el que en las etapas de configuración anteriores necesitábamos cambiar la propiedad del directorio [.filename]#slave# al grupo `bind`.

Después de completar los pasos anteriores reinicie el servidor o reinicie man:syslogd[8] y ejecute man:named[8] asegurándose de que se utilicen las nuevas opciones especificadas en `syslogd_flags` y `named_flags`. En estos momentos deberíamos estar ejecutando una copia de named dentro de un "sandbox".

=== Seguridad

Aunque BIND es la implementación de DNS más utilizada existe siempre el asunto relacionado con la seguridad. De vez en cuando se encuentran agujeros de seguridad y vulnerabilidades.

Es una buena idea suscribirse a http://www.cert.org/[CERT] y a crossref:eresources[eresources-mail,freebsd-security-notifications] para estar al día de los problemas de seguridad relacionados con named.

[TIP]
====

Si surge un problema nunca está de más actualizar los fuentes y recompilar los ejecutables desde dichas fuentes.
====

=== Lecturas recomendadas

Las páginas del manual de BIND/named: man:ndc[8] man:named[8] man:named.conf[8] 

* http://www.isc.org/products/BIND/[Página oficial de ISC Bind]
* http://www.nominum.com/getOpenSourceResource.php?id=6[ Preguntas más frecuentes sobre BIND]
* http://www.oreilly.com/catalog/dns4/[Libro de O'Reilly "DNS and BIND", cuarta edición]
* link:ftp://ftp.isi.edu/in-notes/rfc1034.txt[RFC1034 - Nombre de Dominio - Conceptos y Características]
* link:ftp://ftp.isi.edu/in-notes/rfc1035.txt[RFC1035 - Nombres de Domninio - Implementación y Especificación]

[[network-ntp]]
== NTP

=== Resumen

Según pasa el tiempo el reloj de un computador está expuesto a ligeros desplazamientos. NTP (Protocolo de Hora en Red, en inglés "Network Time Protocol") es un protocolo que permite asegurar la exactitud de nuestro reloj.

Existen varios servicios de internet que confían y se pueden beneficiar de relojes de computadores precisos. Por ejemplo un servidor web puede recibir peticiones de un determinado fichero si ha sido modificado posteriormente a una determinada fecha u hora. Servicios como man:cron[8] ejecutan órdenes en determinados instantes. Si el reloj no se encuentra ajustado estas órdenes pueden ejecutarse fuera de la hora prevista.

FreeBSD viene con el servidor NTP man:ntpd[8] que se puede utilizar para preguntar a otros servidores NTP, de tal forma que podemos ajustar nuestro reloj según la hora de otros servidores e incluso proporcionar servicio de hora nosotros mismos.

=== Elección de los servidores de hora adecuados

Para sincronizar nuestro reloj necesitamos comunicarnos con uno o más servidores NTP. El administrador de nuestra red o nuestro proveedor de servicios de Internet muy posiblemente hayan configurado algún servidor NTP para estos propósitos. Consulte la documentación de que disponga. Existe una http://www.eecis.udel.edu/~mills/ntp/servers.html[lista de servidores NTP públicamente accesibles] que se pueden utilizar para buscar un servidor NTP que se encuentre geográficamente próximo. Asegúrese de que conoce la política de uso de estos servidores públicos ya que en algunos casos es necesario pedir permiso al administrador antes de de poder utilizarlos, principalmente por motivos estadísticos.

Le recomendamos seleccionar servidores NTP que no se encuentren conectados entre sí por si alguno de los servidores que use sea inaccesible o su reloj se averíe. man:ntpd[8] utiliza las respuestas que recibe de otros servidores de una forma inteligente. servidores de una forma inteligente (Tiene a hacer más caso a los más fiables.

=== Configuración de la máquina

==== Configuración básica

Si sólamente deseamos sincronizar nuestro reloj cuando se arranca la máquina se puede utilizar man:ntpdate[8]. Esto puede ser adecuado en algunas máquinas de escritorio que se reinician frecuentemente y donde la sincronización no suele ser un objetivo prioritario pero normalmente la mayoría de las máquinas deberían ejecutar man:ntpd[8].

La utilización de man:ntpdate[8] en tiempo de arranque es también una buena idea incluso para las máquinas que ejecutan man:ntpd[8]. El programa man:ntpd[8] modifica el reloj de forma gradual, mientras que man:ntpdate[8] ajusta directamente el reloj sin importar que tamaño tenga la diferencia de tiempo existente entre la máquina y el servidor de tiempo de referencia.

Para activar man:ntpdate[8] en tiempo de arranque, añada `ntpdate_enable="YES"` al fichero [.filename]#/etc/rc.conf#. También es necesario especificar todos los servidores que deseamos utilizar para realizar el proceso de sincronización y cualquier parámetro que deseemos pasar a man:ntpdate[8] utilizando la variable `ntpdate_flags`.

==== Configuración general

NTP se configura mediante el archivo [.filename]#/etc/ntp.conf# utilizando el formato descrito en man:ntp.conf[5]. A continuación se muestra un sencillo ejemplo:

[.programlisting]
....
server ntplocal.ejemplo.com prefer
server timeserver.ejemplo.org
server ntp2a.ejemplo.net

driftfile /var/db/ntp.drift
....

La opción `server` especifica qué servidores se van a utilizar, especificando un servidor por línea. Si se especifica un servidor con el argumento `prefer`, como en `ntplocal.ejemplo.com` dicho servidor se prefiere sobre los demás. No obstante la respuesta de su servidor preferido se descartará si difiere sustancialmente de la respuesta recibida por parte del resto de los servidores especificados; en caso contrario sólo se tendrá en cuenta la respuesta del servidor preferido sin importar la información suministrada por el resto. El argumento `prefer` se utiliza normalmente en servidores NTP altamente precisos, como aquellos que poseen hardware de tiempo específico.

La opción `driftfile` especifica qué fichero se utiliza para almacenar el desplazamiento de la fracuencia de reloj de la máquina. El programa man:ntpd[8] utiliza este valor para automáticamente compensar el desvío que experimenta de forma natural el reloj de la máquina, permitiendo mantener una precisión acotada incluso cuando se pierde la comunicación con el resto de referencias externas.

La opción `driftfile` especifica qué fichero se utiliza para almacenar la información sobre espuestas anteriores de servidores NTP. Este fichero contiene información útil para la implementación de NTP. No debería ser modificada por ningún otro proceso.

==== Control de acceso al servidor NTP

Por defecto nuestro servidor de NTP puede ser accedido por cualquier máquina de Internet. La opción `restrict` se puede utilizar para controlar controlar qué máquinas pueden acceder al servicio.

Si queremos denegar el acceso a todas las máquinas existentes basta con añadir la siguiente línea a [.filename]#/etc/ntp.conf#:

[.programlisting]
....
restrict default ignore
....

Si sólo queremos permitir el acceso al servicio de hora a las máquinas de nuestra red y al menos tiempo nos queremos asegurar de que dichos clientes no pueden a su vez configurar la hora del servidor o utilizarse ellos mismos como nuevos servidores de hora basta con añadir lo siguiente en lugar de lo anterior:

[.programlisting]
....
restrict 192.168.1.0 mask 255.255.255.0 notrust nomodify notrap
....

donde `192.168.1.0` es la dirección IP de nuestra red y `255.255.255.0` es la máscara de red.

[.filename]#/etc/ntp.conf# puede contener varias opciones de tipo `restrict`. Para más detalles consulte la sección `Soporte de Control de Acceso` de man:ntp.conf[5].

=== Ejecución del servidor de NTP

Para asegurarnos de que el servidor de NTP se ejecuta en tiempo de arranque se debe añadir la línea `xntpd_enable="YES"` al fichero [.filename]#/etc/rc.conf#. Si deseamos pasar opciones adicionales a man:ntpd[8] se puede modificar la variable `xntpd_flags` del fichero [.filename]#/etc/rc.conf#.

Para ejecutar el servidor sin reiniciar la máquina ejecute `ntpd` junto con todos aquellos parámetros que haya especificado en la variable de arranque `xntpd_flags` del fichero [.filename]#/etc/rc.conf#. Por ejemplo:

[source,bash]
....
# ntpd -p /var/run/ntpd.pid
....

[NOTE]
====
Bajo FreeBSD 5.X se han renombrado algunas opciones del fichero [.filename]#/etc/rc.conf#. Se debe reemplazar cualquier aparición `xntpd` por por `ntpd`.
====

=== Utilización de ntpd junto con una conexión temporal a Internet

El programa man:ntpd[8] no necesita una conexión permanente a Internet para poder funcionar correctamente. No obstante si la conexión a Internet se encuentra configurada con marcación bajo demanda es una buena idea impedir que el tráfico de NTP lance una marcación automática o que mantenga la conexión viva. Si se utiliza el PPP de entorno de usuario se pueden utilizar las directivas `filter` dentro del fichero [.filename]#/etc/ppp/ppp.conf# para evitar esto. Por ejemplo:

[.programlisting]
....
 set filter dial 0 deny udp src eq 123
 # Evita que el trÃ¡fico NTP inice una llamada saliente
 set filter dial 1 permit 0 0
 set filter alive 0 deny udp src eq 123
 Evita que el trÃ¡ficoNTP entrante mantenga abierta la conexiÃ³n
 set filter alive 1 deny udp dst eq 123
 Evita que el trÃ¡fico NTP saliente mantenga abierta la conexiÃ³n
 set filter alive 2 permit 0/0 0/0
....

Para ás detalles consulte la sección `PACKET FILTERING` de man:ppp[8] y los ejemplos que se encuentran en [.filename]#/usr/shared/examples/ppp/#.

[NOTE]
====
Algunos proveedores de acceso a Internet bloquean paquetes que utilizan números de puertos bajos impidiendo que los paquetes de vuelta alcancen nuestra máquina.
====

=== Información adicional

Hay documentación sobre el servidor NTP en formato HTML en [.filename]#/usr/shared/doc/ntp/#.

[[network-natd]]
== Traducción de direcciones de red

[[network-natoverview]]
=== Overview

El dæmon de FreeBSD que se encarga de traducir direcciones de red, más conocido como man:natd[8], es un dæmon que acepta paquetes IP, modifica la dirección IP fuente de dichos paquetes y los reinyecta en el flujo de paquetes IP de salida. man:natd[8] ejecuta este proceso modificando la dirección de origen y el puerto de tal forma que cuando se reciben paquetes de contestación man:natd[8] es capaz de determinar el destino real y reenviar el paquete a dicho destino.

El uso más común de NAT es para Compartir la Conexión a Internet.

[[network-natsetup]]
=== Configuración

Debido al pequeño espacio de direccionamiento que se encuentra actualmente disponible en IPv4 y debido también al gran aumento que se está produciendo en cuanto a número de usuarios de líneas de conexión a Internet de alta velocidad como cable o DSL la gente necesita utilizar cada vez más la salida de Compartición de Conexión a Internet. La característica de poder conectar varios computadores a través de una única conexión y una única dirección IP hacen de man:natd[8] una elección razonable.

Cada vez con más frecuencia un usuario típico dispone de una máquina conectada mediante cable o DSL pero desearía utilizar dicha máquina como un " router" de acceso para el resto de los ordenadores de su red de área local.

Para poder hacerlo la máquina (FreeBSD por supuesto) debe configurarse para actuar como pasarela. Debe tener al menos dos tarjetas de red, una para conectarse a la red de área local y la otra para conectarse con el "router" de acceso a Internet. Todas las máquinas de la LAN se conectan entre sí mediante un "hub" o un " switch".

image::natd.png[Topología de la Red]

Una configuración como esta se utiliza frecuentemente para compartir el acceso a Internet. Una de las máquinas de la LAN está realmente conectada a Internet. El resto de las máquinas acceden a Internet utilizando como "pasarela" la máquina inicial.

[[network-natdkernconfiguration]]
=== Configuración

Se deben añadir las siguientes opciones al fichero de configuración del núcleo:

[.programlisting]
....
options IPFIREWALL
options IPDIVERT
....

Además, según se prefiera, se pueden añadir también las siguientes:

[.programlisting]
....
options IPFIREWALL_DEFAULT_TO_ACCEPT
options IPFIREWALL_VERBOSE
....

Lo que viene a continuación se tiene que definir en [.filename]#/etc/rc.conf#:

[.programlisting]
....
gateway_enable="YES"
firewall_enable="YES"
firewall_type="OPEN"
natd_enable="YES"
natd_interface="fxp0"
natd_flags=""
....

[.informaltable]
[cols="1,1", frame="none"]
|===

|gateway_enable="YES"
|Configura la máquina para que actúe como "router" o pasarela de red. Se puede conseguir lo mismo ejecutando `sysctl net.inet.ip.forwarding=1`.

|firewall_enable="YES"
|Activa las reglas de cortafuegos que se encuentran definidas por defecto en [.filename]#/etc/rc.firewall# y que entran en funcionamiento en el arranque del sistema.

|firewall_type="OPEN"
|Especifica un conjunto de reglas de cortafuegos que permite el acceso a todos los paquetes que se reciban. Consulte [.filename]#/etc/rc.firewall# para obtener información sobre el resto de tipos de reglas que se pueden configurar.

|natd_interface="fxp0"
|Indica qué interfaz se utiliza para reenviar paquetes (la interfaz que se conecta a Internet).

|natd_flags=""
|Define cualesquiera otras opciones que se deseen proporcionar a man:natd[8] en tiempo de arranque.
|===

Si se definen las opciones anteriores, en el arranque del sistema el fichero [.filename]#/etc/rc.conf# configurará las variables de tal forma que se ejecutaría `natd -interface fxp0`. Evidentemente esta orden también se puede ejecutar de forma manual.

[NOTE]
====
También es posible utilizar un fichero de configuración para man:natd[8] en caso de que deseemos especificar muchos parámetros de arranque. Tendremos que declarar la ubicación del fichero de configuración mediante la inclusión de lo siguiente en [.filename]#/etc/rc.conf#:

[.programlisting]
....
natd_flags="-f /etc/natd.conf"
....

El fichero [.filename]#/etc/natd.conf# debe contener una lista de opciones de configuración una opción por línea. Por ejemplo, en el caso que se comenta en la siguiente sección se utilizaría un fichero de configuración con la siguiente información:

[.programlisting]
....
redirect_port tcp 192.168.0.2:6667 6667
redirect_port tcp 192.168.0.3:80 80
....

Para obtener más información sobre el fichero de configuración se puede consultar la opción `-f` que se describe en la página del manual de man:natd[8].
====

Cada máquina (y cada interfaz) que se encuentra conectada a la LAN debe poseer una dirección IP perteneciente al espacio de direcciones IP privado tal y como se define en link:ftp://ftp.isi.edu/in-notes/rfc1918.txt[RFC 1918] y debe poseer como pasarela por defecto la dirección IP de la interfaz interna (la interfaz que se conecta a la LAN) de la máquina que ejecuta natd.

Por ejemplo los clientes `A` y `B` se encuentran en la LAN utilizando las direcciones IP `Â192.168.0.2` y `192.168.0.3`, respectivamente. La máquina que ejecuta natd posee la dirección IP `192.168.0.1` en la interfaz que se conecta a la LAN. El "router" por defecto tanto de `A` omo de `B` se establece al valor `192.168.0.1`. La interfaz externa de la máquina que ejecuta natd, la interfaz que se conecta con Internet, no necesita de ninguna especial en relación con el tema que estamos tratando en esta sección.

[[network-natdport-redirection]]
=== Redirección de puertos

El incoveniente que se presenta con la utilización de man:natd[8] es que los clientes de la LAN no son accesibles desde Internet. Dichos clientes pueden establecer conexiones con el exterior pero no pueden recibir intentos de conexión desde pueden recibir intentos de conexion desde Internet. Esto supone un gran problema cuando se quieren ejecutar servicios de acceso global en una o varias máquinas de la red LAN. Una forma sencilla de solucionar parcialmente este problemma consiste en redirigir determinados puertos en el servidor natd hacia determinadas máquinas de la LAN.

Supongamos por ejemplo que en `A` se ejecuta un servidor de IRC y que en `B` se ejecuta un servidor web. Para que funcione lo que hemos comentado anteriormente se tienen que redirigir las conexiones recibidas en los puertos 6667 (IRC) y 80 (web) a dichas máquinas, respectivamente.

Se debe pasar la opción `-redirect_port` a man:natd[8] con los valores apropiados. La sintaxis es como sigue:

[.programlisting]
....
  -redirect_port proto IPdestino:PUERTOdestino[-PUERTOdestino]
                 [aliasIP:]aliasPUERTO[-aliasPUERTO]
		 [IPremota[:PUERTOremoto[-PUERTOremoto]]]
....

Continuando con el ejemplo anterior los valores serían:

[.programlisting]
....
    -redirect_port tcp 192.168.0.2:6667 6667
    -redirect_port tcp 192.168.0.3:80 80
....

Esto redirigirá los puertos _tcp_ adecuados a las máquinas situadas en la LAN.

La opción `-redirect_port` se puede utilizar para indicar rangos de puertos en vez de puertos individuales. Por ejemplo, _tcp 192.168.0.2:2000-3000 2000-3000_ redirige todas las conexiones recibidas desde los puertos 2000 al 3000 a los puertos puertos 2000 a 3000 de la máquina `A`.

Estas opciones se pueden utilizar cuando se ejecute directamente man:natd[8] se pueden situar en la variable `natd_flags=""` en [.filename]#/etc/rc.conf# y también se pueden pasar mediante un archivo de configuración.

Para obtener más información sobre opciones de configuración por favor consulte man:natd[8]

[[network-natdaddress-redirection]]
=== Redirección de direcciones

La redirección de direcciones es una característica útil si se dispone de varias direcciones IP pero todas ellas se ubican en una única máquina. Gracias a esto man:natd[8] puede asignar a cada cliente de la red LAN su propia dirección IP externa. man:natd[8] reescribe los paquetes que salen de la red LAN con la dirección IP externa adecuada y redirige todo el tráfico recibido de vuelta al cliente en función de la dirección IP de destino: esto se conoce como NAT estático. Por ejemplo las direcciones IP `128.1.1.1`, `128.1.1.2` y `128.1.1.3` pertenecen al " router"natd. `128.1.1.1` se puede utilizar como la dirección IP externa del natd, mientras que `128.1.1.2` y `128.1.1.3` se redirigen a los clientes `A` y `B`, respectivamente.

La sintaxis de la opción `-redirect_address` es la siguiente:

[.programlisting]
....
-redirect_address IPlocal IPpública
....

[.informaltable]
[cols="1,1", frame="none"]
|===

|IPlocal
|La dirección IP interna del cliente de la LAN.

|IPpública
|La dirección IP externa que se corresponde con un determinado cliente de la LAN.
|===

En nuestro ejemplo esta opción se especificaría de la siguiente forma:

[.programlisting]
....
-redirect_address 192.168.0.2 128.1.1.2
-redirect_address 192.168.0.3 128.1.1.3
....

De forma semejante a la opción `-redirect_port` estos argumentos se pueden especificar directamente sobre la variable `natd_flags=""` del fichero [.filename]#/etc/rc.conf# o también se pueden pasar vía archivo de configuración de natd. Si se utiliza redirección de direcciones ya no es necesario utilizar redirección de puertos ya que todos los paquetes que se reciben en una determinada dirección IP son redirigidos a la máquina especificada.

Las direcciones IP externas de la máquina que ejecuta natd se deben activar y deben formar parte de un alias configurado sobre la interfaz externa que se conecta a Internet. Consulte man:rc.conf[5] para aprender a hacerlo.

[[network-inetd]]
== El "Superservidor" inetd

[[network-inetd-overview]]
=== Resumen

man:inetd[8] se conoce como el "Super Servidor de Internet" debido a que gestiona las conexiones de varios dæmones. Los dæmones son programas que proporcionan servicios de red. inetd actúa como un servidor de servidor de gestión de otros dæmones. Cuando man:inetd[8] recibe una conexión se determina qué dæmon debería responder a dicha conexión, se lanza un proceso que ejecuta dicho dæmon y se le entrega el " socket". La ejecución de una única instancia de inetd reduce la carga del sistema en comparación con lo que significaría ejecutar cada uno de los dæmones que gestiona de forma individual.

inetd se utiliza principalmente para lanzar procesos que albergan a otros dæmones pero inetd también se utiliza para gestionar determinados protocolos triviales como chargen, auth y daytime.

Esta sección trata la configuración básica de inetd a través de sus opciones de línea de órdenes y utilizando su fichero de configuración, denominado [.filename]#/etc/inetd.conf#.

[[network-inetd-settings]]
=== Configuraciones

inetd se inicializa a través del fichero [.filename]#/etc/rc.conf# en tiempo de arranque. La opción `inetd_enable` posee el valor `NO` por defecto, pero a menudo la aplicación sysinstall la activa cuando se utiliza la configuración de perfil de seguridad medio. Estableciendo

[.programlisting]
....
inetd_enable="YES"
....
o 
[.programlisting]
....
inetd_enable="NO"
....

dentro de [.filename]#/etc/rc.conf# se puede activar o desactivar la la ejecución de inetd en el arranque del sistema.

Se pueden además aplicar distintas opciones de línea de órdenes mediante la opción `inetd_flags`.

[[network-inetd-cmdline]]
=== Opciones de línea de órdenes

sinopsis de inetd:

`inetd [-d] [-l] [-w] [-W] [-c máximo] [-C tasa] [-a dirección | nombre_de_host] [-p nombre_de_fichero] [-R tasa] [fichero de configuración]`

-d::
Activa la depuración.

-l::
Activa el "logging" de las conexiones efectuadas con écute.

-w::
Activa el recubrimiento de TCP para servicios externos (activado por defecto).

-W::
Activa el recubrimiento de TCP para los servicios internos, ejecutados directamente por el dæmon inetd (activado por defecto).

-c máximo::
Especifica el máximo número de invocaciones simultáneas de cada servicio; el valor por defecto es ilimitado. Se puede sobreescribir para cada servicio utilizando la opción `max-child`.

-C tasa::
Especifica el máximo número de veces que se puede llamar a un servicio desde un dirección IP determinada por minuto; el valor por defecto es ilimitado. Se puede redefinir para cada servicio utilizando la opción `max-connections-per-ip-per-minute`.

-R tasa::
Especifica el máximo número de veces que se puede invocar un servicio en un minuto; el valor por defecto es 256. Un valor de 0 permite un número ilimitado de llamadas.

-a::
Especifica una dirección IP a la cual se asocia y sobre la cual se queda esperando recibir conexiones. Puede declararse también un nombre de máquina, en cuyo caso se utilizará la dirección (o direcciones si hay más de una) IPv4 o IPv6 que estén tras dicho nombre. Normalmente se usa un nombre de máquina cuando inetd se ejecuta dentro de un man:jail[8], en cuyo caso el nombre de máquina se corresponde con el entorno man:jail[8].
+
Cuando se desea asociarse tanto a direcciones IPv4 como a direcciones IPv6 y se utiliza un nombre de máquina se necesita una entrada para cada protocolo (IPv4 o IPv6) para cada servicio que se active a través de [.filename]#/etc/inetd.conf#. Por ejemplo un servicio basado en TCP necesitaría dos entradas, una utilizando `tcp4` para el protocolo IPv4 y otra con `tcp6` para las conexiones a través del procolo de red IPv6.

-p::
Especifica un fichero alternativo en el cual se guarda el ID del proceso.

Estas opciones se pueden declarar dentro de las variables `inetd_flags` del fichero [.filename]#/etc/rc.conf#. Por defecto `inetd_flags` tiene el valor `-wW`, lo que activa el recubrimiento de TCP para los servicios internos y externos de inetd. Los usuarios inexpertos no suelen introducir estos parámetros y por ello ni siquiera necesitan especificarse dentro de [.filename]#/etc/rc.conf#.

[NOTE]
====
Un servicio externo es un dæmon que se ejecuta fuera de inetd y que se lanza cuando se recibe un intento de conexión. Un servicio interno es un servicio que inetd puede servir directamente sin necesidad de lanzar nuevos procesos.
====

[[network-inetd-conf]]
=== [.filename]#inetd.conf#

La configuración de inetd se realiza a través del ficherode configuración [.filename]#/etc/inetd.conf#.

Cuando se realiza una modificación en el fichero [.filename]#/etc/inetd.conf# se debe obligar a inetd a releer dicho fichero de configuración, lo cual se realiza enviando una señal "HANGUP" al proceso inetd como se muestra a continuación:

[[network-inetd-hangup]]
.Envío de una señal HANGUP a inetd
[example]
====

[source,bash]
....
# kill -HUP `cat /var/run/inetd.pid`
....

====

Cada línea del fichero de configuración especifica un dæmon individual. Los comentarios se preceden por el caracter "#". El formato del fichero de configuración [.filename]##/etc/inetd.conf## es el siguiente:

[.programlisting]
....
service-name
socket-type
protocol
{wait|nowait}[/max-child[/max-connections-per-ip-per-minute]]
user[:group][/login-class]
server-program
server-program-arguments
....

A continuación se muestra una entrada de ejemplo para el dæmon ftpd para IPv4:

[.programlisting]
....
ftp     stream  tcp     nowait  root    /usr/libexec/ftpd       ftpd -l
....

service-name::
Este es el nombre del servicio que proporciona un determinado dæmon. Se debe corresponder con el nombre del nombre de servicio que se declara en el fichero [.filename]#/etc/services#. Este fichero determina sobre qué puerto debe ponerse a escuchar inetd. Si se crea un nuevo servicio se debe especificar primero en [.filename]#/etc/services#.

socket-type::
Puede ser `stream`, `dgram`, `raw` o `seqpacket`. `stream` se debe utilizar obligatoriamente para dæmones orientados a conexión (dæmones TCP) mientras que `dgram` se utiliza en dæmones basados en el protocolo de transporte UDP.

protocol::
Uno de los siguientes:
+
[.informaltable]
[cols="1,1", options="header"]
|===
| Protocolo
| Explicación

|tcp, tcp4
|TCP IPv4

|udp, udp4
|UDP IPv4

|tcp6
|TCP IPv6

|udp6
|UDP IPv6

|tcp46
|TCP IPv4 e IPv6 al mismo tiempo

|udp46
|UDP IPv4 e IPv6 al mismo tiempo
|===

{wait|nowait}[/max-child[/max-connections-per-ip-per-minute]]::
`wait|nowait` indica si el dæmon puede gestionar su propio "socket" o no. Los " sockets" de tipo `dgram` deben utilizar obigatoriamente la opción `wait` mientras que los dæmones basados en "sockets" de tipo "stream", los cuales se implementan normalmente mediante hilos, debería utilizar la opción `nowait`. La opción `wait` normalmente entrega varios " sockets" a un único dæmon, mientras que la opción `nowait` lanza un dæmon "hijo" por cada nuevo " socket".
+
El número máximo de dæmones " hijo" que puede lanzar inetd se puede especificar mediante la opción `max-child`. Si se necesita por ejemplo un límite de diez instancias para un dæmon en particular se puede especificar el valor `10` justo después de la opción `nowait`.
+
Además de `max-child` se puede activar otra opción para limitar en número máximo de conexiones que se aceptan desde un determinado lugar mediante la opción `max-connections-per-ip-per-minute`. Esta opción hace justo lo que su nombre indica. Un valor de, por ejemplo, diez en esta opción limitaría cualquier máquina remota a un máximo de diez intentos de conexión por minuto. Esto resulta útil para prevenir un consumo incontrolado de recursos y ataques de Denegación de Servicio ("Denial of Service" o DoS) sobre nuestra máquina.
+
Cuando se especifica este campo las opciones `wait` o `nowait` son obligatorias `max-child` y `max-connections-per-ip-per-minute` son opcionales.
+
Un dæmon de tipo "stream" sin la opción `max-child` y sin la opción `max-connections-per-ip-per-minute` simplemente especificaría la opción `nowait`.
+
El mismo dæmon con el límite máximo de diez dæmones "hijos" sería: `nowait/10`.
+
La misma configuración con un límite de veinte conexiones por dirección IP por minuto y un máximo total de diez dæmones "hijos" sería: `nowait/10/20`.
+
Todas estas opciones son utilizadas por el dæmon fingerd que se muestra a continuación a modo de ejemplo:
+
[.programlisting]
....
finger stream  tcp     nowait/3/10 nobody /usr/libexec/fingerd fingerd -s
....

user::
Este es el nombre de usuario con el que debería ejecutarse un determinado dæmon. Normalmente los dæmones se suelen ejectar con permisos de `root`. Por motivos de seguridad, resulta bastante común encontrarse con algunos servidores que se ejecutan bajo el usuario `daemon` o incluso por el usuario menos privilegiado de todos que es el usuario `nobody`.

server-program::
La ruta completa de la localización del dæmon que se quiere ejecutar cuando se recibe un intento de conexión. Si el dæmon es un servicio proporcionado por el propio inetd se debe utilizar la opcion `internal` en su lugar.

server-program-arguments::
Esto funciona en conjunción con `server-program`, ya que especifica los argumentos, comenzando por `argv[0]`, que se pasan al dæmon cuando se le invoca. Si la línea de órdenes es `mydaemon -d`, `midæmon -d` debería ser el valor de la opción `server-program-arguments`. Si el dæmon es un servicio interno se debe utilizar la utilizar la opción `internal` en lugar de la que estamos comentando.

[[network-inetd-security]]
=== Seguridad

Dependiendo del perfil de seguridad establecido cuando se instaló el sistema FreeBSD varios dæmones de inetd pueden estar desactivados o activados. Si no se necesita un dæmon determinado, _no lo active_. Especifique un "#" al comienzo de la línea del dæmon que quiere desactivar y envíe una señal <<network-inetd-hangup,hangup>> a inetd. No se aconseja ejecutar algunos dæmones determinados (un caso típico es fingerd) porque pueden proporcionar información valiosa para un atacante.

Algunos dæmones no presentan ninguna característica de seguridad y poseen grandes o incluso no poseen tiempos de expiración para los intentos de conexión. Esto permite que un atacante sature los recursos de nuestra máquina realizando intentos de conexión a una tasa relativamente baja contra uno de estos ingenuos dæmones. Pueder ser una buena idea protegerse de esto utilizando las opciones `max-connections-per-ip-per-minute` y `max-child` para este tipo de dæmones.

El recubrimiento de TCP está activado por defecto tal y como ya se ha comentado anteriormente. Consulte la página del manual de man:hosts_access[5] para obtener más información sobre cómo aplicar restricciones relacionadas con TCP a los dæmones invocados por inetd.

[[network-inetd-misc]]
=== Varios

daytime, time, echo, discard, chargen y auth son servicios que inetd proporciona de forma interna y propia.

El servicio auth proporciona servicios de identificación a través de la red (ident, identd) y se puede configurar hasta en cierto grado.

Consulte la página del manual de man:inetd[8] si quiere conocer todos los detalles.

[[network-plip]]
== Línea IP paralela (PLIP)

PLIP nos permite configurar TCP/IP a través del puerto paralelo. Es útil para conectar máquinas que no poseen tarjetas de red, o para instalar FreeBSD en ciertos viejos modelos de portátiles. En esta sección se explica lo siguiente:

* Construcción de un cable paralelo (laplink).
* Conexión de dos computadores utilizando PLIP.

[[network-create-parallel-cable]]
=== Construcción de un cable paralelo

Se puede comprar un cable paralelo en cualquier tienda de componentes informáticos. No obstante si no es posible comprarlo o simplemente queremos saber cómo hacerlo nosotros mismos, en la siguiente tabla mostramos como hacer un cable de impresora paralelo.

.Cableado de una conexión de cable paralelo para redes
[cols="1*l,1*l,1*l,1,1*l", options="header"]
|===
| Nombre-A
| Extremo-A
| Extremo-B
| Descr.
| Post/Bit

|

....
DATA0
-ERROR
....
|

....
2
15
....
|

....
15
2
....
|Data
|

....
0/0x01
1/0x08
....

|

....
DATA1
+SLCT
....
|

....
3
13
....
|

....
13
3
....
|Data
|

....
0/0x02
1/0x10
....

|

....
DATA2
+PE
....
|

....
4
12
....
|

....
12
4
....
|Data
|

....
0/0x04
1/0x20
....

|

....
DATA3
-ACK
....
|

....
5
10
....
|

....
10
5
....
|Strobe
|

....
0/0x08
1/0x40
....

|

....
DATA4
BUSY
....
|

....
6
11
....
|

....
11
6
....
|Data
|

....
0/0x10
1/0x80
....

|GND
|18-25
|18-25
|GND
|-
|===

[[network-plip-setup]]
=== Configuración de PLIP

En primer lugar debemos tener en nuesras manos un cable " laplink". A continuación se debe comprobar que ambos sistemas poseen núcleos con soporte para el controlador man:lpt[4]:

[source,bash]
....
# grep lp /var/run/dmesg.boot
lpt0: <Printer> on ppbus0
lpt0: Interrupt-driven port
....

El puerto paralelo debe ser un puerto controlado por alguna " irq". En FreeBSD 4.X se debería tener un línea como la siguiente en el fichero de configuración del kernel:

[.programlisting]
....
device ppc0 at isa? irq 7
....

En FreeBSD 5.X el fichero [.filename]#/boot/device.hints# debe contener las siguientes líneas:

[.programlisting]
....
hint.ppc.0.at="isa"
hint.ppc.0.irq="7"
....

A continuación se debe comprobar que el fichero de configuración del núcleo posee una línea con `device plip` o también puede comprobar si se ha cargado el módulo del núcleo [.filename]#plip.ko#. Tanto en un caso como en el otro, cuando se ejecute man:ifconfig[8] debería aparecer el interfaz de red paralelo. En FreeBSD 4.X se muestra algo parecido a lo siguiente:

[source,bash]
....
# ifconfig lp0
lp0: flags=8810<POINTOPOINT,SIMPLEX,MULTICAST> mtu 1500
....

y en FreeBSD 5.X:

[source,bash]
....
# ifconfig plip0
plip0: flags=8810<POINTOPOINT,SIMPLEX,MULTICAST> mtu 1500
....

[NOTE]
====
El nombre del dispositivo utilizado para la interfaz paralela es distinto en FreeBSD 4.X ([.filename]#lpX#) y en FreeBSD 5.X ([.filename]#plipX#).
====

Enchufe el cable "laplink" en los interfaces de ambos computadores.

Configure los parámetros de la interfaz de red en ambas máquinas como `root`. Por ejemplo, si queremos conectar la máquina `host1` ejecutando FreeBSD 4.X con la máquina `host2` que ejecuta FreeBSD 5.X:

[.programlisting]
....
                 host1 <-----> host2
Dirección IP 10.0.0.1      10.0.0.2
....

Configure la interfaz de `host1` así:

[source,bash]
....
# ifconfig lp0 10.0.0.1 10.0.0.2
....

Configure la interfaz de `host2` por medio de:

[source,bash]
....
# ifconfig plip0 10.0.0.2 10.0.0.1
....

Tras esto debería disponerse de una conexión totalmente funcional. Por favor, consulte man:lp[4] y man:lpt[4] si quiere saber más.

Además se debe añadir ambas máquinas al fichero [.filename]#/etc/hosts#:

[.programlisting]
....
127.0.0.1               localhost.mi.dominio localhost
10.0.0.1                host1.mi.dominio host1
10.0.0.2                host2.mi.dominio
....

Para comprobar que efectivamente la conexión funciona se puede probar a hacer un ping desde cada máquina. Por ejemplo en la máquina `host1`:

[source,bash]
....
# ifconfig lp0
lp0: flags=8851<UP,POINTOPOINT,RUNNING,SIMPLEX,MULTICAST> mtu 1500
        inet 10.0.0.1 --> 10.0.0.2 netmask 0xff000000
# netstat -r
Routing tables

Internet:
Destination        Gateway          Flags     Refs     Use      Netif Expire
host2              host1              UH          0       0       lp0
# ping -c 4 host2
PING host2 (10.0.0.2): 56 data bytes
64 bytes from 10.0.0.2: icmp_seq=0 ttl=255 time=2.774 ms
64 bytes from 10.0.0.2: icmp_seq=1 ttl=255 time=2.530 ms
64 bytes from 10.0.0.2: icmp_seq=2 ttl=255 time=2.556 ms
64 bytes from 10.0.0.2: icmp_seq=3 ttl=255 time=2.714 ms

--- host2 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max/stddev = 2.530/2.643/2.774/0.103 ms
....

[[network-ipv6]]
== IPv6

IPv6 (también conocido como IPng o "IP de nueva generación") es la nueva versión del conocido protocolo de red IP, tambíen llamado IPv4. Como sucede con el resto de los sistemas *BSD FreeBSD proporciona una implementación de referencia que desarrolla el proyecto japonés KAME. FreeBSD dispone de todo lo necesario para experimentar con el nuevo protocolo de red. Esta sección se centra en conseguir configurar y ejecutar correctamente el protocolo IPv6.

Al comienzo de los años 90 la gente comenzó a preocuparse por el rápido consumo del espacio de direcciones de IPv4. Dada la expansión actual de Internet existen dos preocupaciones principales:

* Agotamiento de las direcciones disponibles. Actualmente no se trata del principal problema debido al uso generalizado del del espacio de direccionamiento privado (`10.0.0.0/8`, `192.168.0.0/24`, etc.) junto con NAT.
* El número de entradas de las tablas de rutas comenzaba a ser imposible de manejar. Esto todavia es un problema prioritario.

IPv6 trata de resolver estos problemas y algunos más de la siguiente forma:

* IPv6 posee un espacio de direccionamiento de 128 bits. En otras palabras, en teoría existen 340,282,366,920,938,463,463,374,607,431,768,211,456 direcciones disponibles. Esto significa que existen aproximadamente 6.67 * 10^27 direcciones IPv6 por metro cuadrado disponibles para todo el planeta Tierra.
* Los "routers" sólo almacenan direcciones de red agregadas así que se reduce el número de entradas para cada tabla de rutas a un promedio de 8192.

Existen además muchas otras caracterísiticas interesantes que IPv6 proporciona, como:

* Autoconfiguración de direcciones (http://www.ietf.org/rfc/rfc2462.txt[RFC2462])
* Direcciones anycast ("una-de-varias")
* Soporte de direcciones multicast predefinido
* IPsec (Seguridad en IP)
* Estructura de la cabecera simplificada
* IP móvil
* Mecanismos de traducción de IPv6 a IPv4 (y viceversa)

Si quiere saber más sobre IPv6 le recomendamos que consulte:

* Resumen de IPv6 en http://playground.sun.com/pub/ipng/html/ipng-main.html[playground.sun.com]
* http://www.kame.net[KAME.net]
* http://www.6bone.net[6bone.net]

=== Conceptos básicos sobre las direcciones IPv6

Existen varios tipos distintos de direcciones IPv6: Unicast, Anycast y Multicast.

Las direcciones unicast son direcciones bien conocidas. Un paquete que se envía a una dirección unicast deberín llega a la interfaz identificada por dicha dirección.

Las direcciones anycast son sintácticamente indistinguibles de las direcciones unicast pero sirven para identificar a un _conjunto_ de interfaces. Un paquete destinado a una dirección anycast llega a la interfaz "más cercana" (en términos de métrica de "routers"). Las direcciones anycast sólo se pueden utilizar en "routers".

Las direcciones multicast identifican un grupo de interfaces. Un paquete destinado a una dirección multicast llega a todos los los interfaces que se encuentran agrupados bajo dicha dirección.

[NOTE]
====
Las direcciones IPv4 de tipo broadcast (normalmente `xxx.xxx.xxx.255`) se expresan en IPv6 mediante direcciones multicast.
====

.Direcciones IPv6 reservadas
[cols="1,1,1,1", options="header"]
|===
| Dirección IPv6
| Longitud del Prefijo (Bits)
| Descripción
| Notas

|`::`
|128 bits
|sin especificar
|como `0.0.0.0` en Pv4

|`::1`
|128 bits
|dirección de bucle local (loopback)
|como las `127.0.0.1` en IPv4

|`::00:xx:xx:xx:xx`
|96 bits
|direcciónes IPv6 compatibles con IPv4
|Los 32 bits más bajos contienen una dirección IPv4. También se denominan direcciones "empotradas."

|`::ff:xx:xx:xx:xx`
|96 bits
|direcciones IPv6 mapeadas a IPv4
|Los 32 bits más bajos contienen una dirección IPv4. Se usan para representar direcciones IPv4 mediante direcciones IPv6.

|`fe80::` - `feb::`
|10 bits
|direcciones link-local
|equivalentes a la dirección de loopback de IPv4

|`fec0::` - `fef::`
|10 bits
|direcciones site-local
|Equivalentes al direccionamiento privado de IPv4

|`ff::`
|8 bits
|multicast
|

|`001` (base 2)
|3 bits
|direcciones unicast globales
|Todas las direcciones IPv6 globales se asignan a partir de este espacio. Los primeros tres bits siempre son "001".
|===

=== Lectura de las direcciones IPv6

La forma canónica que se utiliza para representar direcciones IPv6 es: `x:x:x:x:x:x:x:x`, donde cada "x" se considera un valor hexadecimal de 16 Bit. Por ejemplo `FEBC:A574:382B:23C1:AA49:4592:4EFE:9982`

A menudo una dirección posee alguna subcadena de varios ceros consecutivos de forma que se puede abreviar dicha cadena (sólo una vez, para evitar ambigúedades) mediante "::". También se pueden omitir los ceros a la ceros a la izquierda dentro de un valor "x". Por ejemplo `fe80::1` se corresponde con la forma canónica `fe80:0000:0000:0000:0000:0000:0000:0001`.

Una tercera forma de escribir direciones IPv6 es utilizando la ya tradicional notación decimal de IPv4 pero sólamente para los 32 bits más bajos de la dirección IPv6. Por ejemplo `2002::10.0.0.1` se correspondería con la representation hexadecimal canónica `2002:0000:0000:0000:0000:0000:0a00:0001` la cual es equivalente también a escribir `2002::a00:1`.

A estas alturas el lector debería ser capaz de comprender lo siguiente:

[source,bash]
....
# ifconfig
....

[.programlisting]
....
rl0: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> mtu 1500
         inet 10.0.0.10 netmask 0xffffff00 broadcast 10.0.0.255
         inet6 fe80::200:21ff:fe03:8e1%rl0 prefixlen 64 scopeid 0x1
         ether 00:00:21:03:08:e1
         media: Ethernet autoselect (100baseTX )
         status: active
....

`fe80::200:21ff:fe03:8e1%rl0` es una dirección link-local autoconfigurada. Se construye a partir de la dirección MAC de la tarjeta de red.

Si quiere saber más sobre la estructura de las direcciones IPv6 puede consultar http://www.ietf.org/rfc/rfc3513.txt[RFC3513].

=== Establecimiento de conectividad

Actualmente existen cuatro formas distintas de conectarse con otras máquinas y redes IPv6:

* Unirse a la red experimental denominada 6bone
* Obtener una red IPv6 a través de nuestro proveedor de acceso a Internet. Consulte a su proveedor de servicios para para más información.
* Encapsulación de IPv6 sobre IPv4 (http://www.ietf.org/rfc/rfc3068.txt[RFC3068])
* Utilización del "port"package:net/freenet6[] si se dispone de una de una conexión de marcación por modem.

Vamos a explicar cómo conectarse al 6bone ya que parece ser la forma más utilizada en la actualidad.

En primer lugar se recomienda consultar el sitio web de http://www.6bone.net/[6bone] para saber cuál es la conexión del 6bone (físicamente) más próxima. Se debe escribir a la persona responsable de ese nodo y con un poco de suerte dicha persona responderá con con un conjunto de instrucciones y pasos a seguir para establecer la la conexión con ellos y a través de ellos con el resto de los nodos IPv6 que forman parte del 6bone. Normalmente esta conexión se establece usando túneles GRE (gif).

Veamos un ejemplo típico de configuración de un de un túnel man:gif[4]:

[source,bash]
....
# ifconfig gif0 create
# ifconfig gif0
gif0: flags=8010<POINTOPOINT,MULTICAST> mtu 1280
# ifconfig gif0 tunnel MI_DIRECCIÓn_IPV4  SU_DIRECCIÓn_IPV4
# ifconfig gif0 inet6 alias DIRECCIÓn_DE-SALIDA_IPv6_DEL_TÚNEL_ASIGNADO
....

Sustituya las palabras en mayúsculas por la información recibida del nodo 6bone al que nos queremos conectar.

La orden anterior establece el túnel. Compruebe que el túnel funciona correctamente mediante man:ping[8]. Haga un man:ping6[8] a `ff02::1%gif0`. Deberíamos recibir recibir "dos" respuestas.

[NOTE]
====
Para que el lector no se quede pensando en el significado significado de la dirección `ff02:1%gif0` le podemos decir que se trata de de una dirección IPv6 multicast de tipo link-local. `%gif0` no forma parte del protocolo IPv6 como tal sino que se trata de un detalle de implementación relacionado con las direcciones link-local y se añade para especificar la interfaz de salida que se debe utilizar para enviar los paquetes de man:ping6[8]. Como estamos haciendo ping a una dirección multicast a la que se unen todos los interfaces pertenecientes al mismo enlace debería responder al ping tanto nuestro propio interfaz como el interfaz remoto.
====

A continuación se configura la ruta por defecto hacia nuestro enlace 6bone; observe que es muy semejante a lo que hay que hacer en IPv4:

[source,bash]
....
# route add -inet6 default -interface gif0
# ping6 -n MI_UPLINK
....

[source,bash]
....
# traceroute6 www.jp.FreeBSD.org
(3ffe:505:2008:1:2a0:24ff:fe57:e561) from 3ffe:8060:100::40:2, 30 hops max, 12 byte packets
     1  atnet-meta6  14.147 ms  15.499 ms  24.319 ms
     2  6bone-gw2-ATNET-NT.ipv6.tilab.com  103.408 ms  95.072 ms *
     3  3ffe:1831:0:ffff::4  138.645 ms  134.437 ms  144.257 ms
     4  3ffe:1810:0:6:290:27ff:fe79:7677  282.975 ms  278.666 ms  292.811 ms
     5  3ffe:1800:0:ff00::4  400.131 ms  396.324 ms  394.769 ms
     6  3ffe:1800:0:3:290:27ff:fe14:cdee  394.712 ms  397.19 ms  394.102 ms
....

Esta captura de pantalla variará dependiendo de la localización de la máquina. Tras seguir estos pasos deberíamos poder alcanzar el sitio IPv6 de http://www.kame.net[www.kame.net] y ver la tortuga bailarina, que es una imagen animada que sólo se muestra cuando se accede al servidor web utilizando el protocolo IPv6 (para ellos se encesita utilizar un navegador web que soporte IPv6, IPv6, por ejemplo package:www/mozilla[] o Konqueror, que forma parte de package:x11/kdebase3[], o también con package:www/epiphany[].

=== DNS en el mundo IPv6

Existen dos tipos de registros de DNS para IPv6. No obstante el IETF ha declarado los registros A6 y CNAME como registros para uso experimental. Los registros de tipo AAAA son los únicos estandar a día de hoy.

La utilización de registros de tipo AAAA es muy sencilla. Se asocia el nombre de la máquina con la dirección IPv6 de la siguiente forma:

[.programlisting]
....
NOMBREDEMIMÁQUINA AAAA   MIDIRECCIÓNIPv6
....

De igual forma que en IPv4 se utilizan los registros de tipo A. En caso de no poder administrar su propia zona de DNS se puede pedir esta configuración a su proveedor de servicios. Las versiones actuales de bind (versiones 8.3 y 9) y el "port"package:dns/djbdns[] (con el parche de IPv6 correspondiente) soportan los registros de tipo AAAA.

[[network-atm]]
== ATM en FreeBSD 5.X

=== Configuración de IP clásico sobre ATM (PVCs)

IP clásico sobre ATM (CLIP) es el método más sencillo de utilizar ATM con IP. Se puede utilizar con conexiones conmutadas (SVC) y con conexiones permanentes (PVCs). En esta sección se describe cómo configurar una red basada en PVCs.

==== Configuraciones en red mallada completa

El primer método para configurar CLIP con PVCs consiste en conectar unas máquinas con otras mediante circuitos PVC dedicados. Aunque la configuración parece sencilla llega a resultar imposible de manejar cuando se posee un número grande de máquinas. El ejemplo que se muestra a continuación supone que nuestra red posee cuatro máquinas y que cada una se conecta a la red ATM mediante una tarjeta de red ATM. El primer paso consiste en planificar las direcciones IP y las conexiones ATM que se van a configurar en las máquinas.

[.informaltable]
[cols="1,1", frame="none", options="header"]
|===
| Máquina
| Dirección IP

|`hostA`
|`192.168.173.1`

|`hostB`
|`192.168.173.2`

|`hostC`
|`192.168.173.3`

|`hostD`
|`192.168.173.4`
|===

Para construir una red completamente mallada necesitamos una conexión ATM entre cada par de máquinas:

[.informaltable]
[cols="1,1", frame="none", options="header"]
|===
| Máquinas
| Pareja VPI.VCI

|`hostA` - `hostB`
|0.100

|`hostA` - `hostC`
|0.101

|`hostA` - `hostD`
|0.102

|`hostB` - `hostC`
|0.103

|`hostB` - `hostD`
|0.104

|`hostC` - `hostD`
|0.105
|===

Los valores VPI y VCI en cada extremo de la conexión pueden ser diferentes pero por simplicidad suponemos que son iguales. A continuación necesitamos configurar las interfaces ATM en cada máquina:

[source,bash]
....
hostA# ifconfig hatm0 192.168.173.1 up
hostB# ifconfig hatm0 192.168.173.2 up
hostC# ifconfig hatm0 192.168.173.3 up
hostD# ifconfig hatm0 192.168.173.4 up
....

Suponiendo que la interfaz ATM es [.filename]#hatm0# en todas las máquinas. Ahora necesitamos configurar los PVCs en las máquinas (suponemos que ya se han configurado de forma correcta en el "switch" ATM, para lo cual puede ser necesario consultar el manual del "switch").

[source,bash]
....
hostA# atmconfig natm add 192.168.173.2 hatm0 0 100 llc/snap ubr
hostA# atmconfig natm add 192.168.173.3 hatm0 0 101 llc/snap ubr
hostA# atmconfig natm add 192.168.173.4 hatm0 0 102 llc/snap ubr

hostB# atmconfig natm add 192.168.173.1 hatm0 0 100 llc/snap ubr
hostB# atmconfig natm add 192.168.173.3 hatm0 0 103 llc/snap ubr
hostB# atmconfig natm add 192.168.173.4 hatm0 0 104 llc/snap ubr

hostC# atmconfig natm add 192.168.173.1 hatm0 0 101 llc/snap ubr
hostC# atmconfig natm add 192.168.173.2 hatm0 0 103 llc/snap ubr
hostC# atmconfig natm add 192.168.173.4 hatm0 0 105 llc/snap ubr

hostD# atmconfig natm add 192.168.173.1 hatm0 0 102 llc/snap ubr
hostD# atmconfig natm add 192.168.173.2 hatm0 0 104 llc/snap ubr
hostD# atmconfig natm add 192.168.173.3 hatm0 0 105 llc/snap ubr
....

Por supuesto que se pueden utilizar otras especificaciones de tráfico siempre y cuando las tarjetas de red las soporten. En este caso la especificación del tipo de tráfico se completa con los parámetros del tráfico. Puede acceder a la ayuda de man:atmconfig[8] así:

[source,bash]
....
# atmconfig help natm add
....

y por supuesto en la página de manual de man:atmconfig[8].

Se puede crear la misma configuración utilizando el fichero [.filename]#/etc/rc.conf#. Para la máquina `hostA` sería algo así:

[.programlisting]
....
network_interfaces="lo0 hatm0"
ifconfig_hatm0="inet 192.168.173.1 up"
natm_static_routes="hostB hostC hostD"
route_hostB="192.168.173.2 hatm0 0 100 llc/snap ubr"
route_hostC="192.168.173.3 hatm0 0 101 llc/snap ubr"
route_hostD="192.168.173.4 hatm0 0 102 llc/snap ubr"
....

El estado de todas las rutas CLIP se puede obtener en todo momento con:

[source,bash]
....
hostA# atmconfig natm show
....
